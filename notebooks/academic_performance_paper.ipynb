{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "955f54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../lmmnn/\")\n",
    "\n",
    "from utils.training_functions import *\n",
    "from utils.evaluation import *\n",
    "from utils.utils import tune_xgboost,tune_lasso,evaluate_lr,evaluate_xgb\n",
    "\n",
    "from data import dataset_preprocessing\n",
    "\n",
    "from utils.evaluation import get_metrics\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6dd2461",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"academic_performance\"\n",
    "mode=\"cv\"\n",
    "RS=1\n",
    "hct=10\n",
    "test_ratio=0.2\n",
    "val_ratio=0.1\n",
    "folds=5\n",
    "target = \"continuous\"\n",
    "experiment_name = \"5CV_paper_final\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae51df",
   "metadata": {},
   "source": [
    "### Describe raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21567c48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(f\"../data/raw/{dataset_name}/{dataset_name}.xlsx\")\n",
    "df = df.drop(\"Unnamed: 9\",axis=1)\n",
    "identifiers = [\"COD_S11\", \"Cod_SPro\"]\n",
    "alternative_targets = [\"CR_PRO\", \"QR_PRO\", \"CC_PRO\", \"WC_PRO\", \"FEP_PRO\", \"ENG_PRO\", \"QUARTILE\", \"PERCENTILE\",\n",
    "                       \"2ND_DECILE\", ]\n",
    "df = df.drop(identifiers+alternative_targets,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1edfb9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_col = \"G_SC\"\n",
    "demographic_cols = ['GENDER', 'EDU_FATHER', 'EDU_MOTHER', 'OCC_FATHER', 'OCC_MOTHER',\n",
    "       'STRATUM', 'SISBEN', 'PEOPLE_HOUSE', 'INTERNET', 'TV', 'COMPUTER',\n",
    "       'WASHING_MCH', 'MIC_OVEN', 'CAR', 'DVD', 'FRESH', 'PHONE', 'MOBILE','REVENUE', 'JOB', 'SCHOOL_NAME', 'SCHOOL_NAT', 'SCHOOL_TYPE','SEL', 'SEL_IHE']\n",
    "perf_cols = ['MAT_S11','CR_S11', 'CC_S11', 'BIO_S11', 'ENG_S11']\n",
    "activity_cols = []\n",
    "other_cols = ['UNIVERSITY', 'ACADEMIC_PROGRAM']\n",
    "set(df.columns)-set([y_col]+demographic_cols+perf_cols+activity_cols+other_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "529261c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>d</th>\n",
       "      <th>% NA</th>\n",
       "      <th>Target</th>\n",
       "      <th>Performance features</th>\n",
       "      <th>Demographic features</th>\n",
       "      <th>Activity features</th>\n",
       "      <th>Other features</th>\n",
       "      <th>Categorical features</th>\n",
       "      <th>Total cardinality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cortez</th>\n",
       "      <td>12411</td>\n",
       "      <td>33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>$y \\in [$37..247]</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>3980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            N   d  % NA             Target  Performance features  \\\n",
       "cortez  12411  33   0.0  $y \\in [$37..247]                     5   \n",
       "\n",
       "        Demographic features  Activity features  Other features  \\\n",
       "cortez                    25                  0               2   \n",
       "\n",
       "        Categorical features  Total cardinality  \n",
       "cortez                    13               3980  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_df_dict = {\"N\": df.shape[0],\n",
    "           \"d\": df.shape[1],\n",
    "           \"% NA\": df.isna().sum().sum()/sum(df.shape),\n",
    "           \"Target\": f\"$y \\in [${df[y_col].min()}..{df[y_col].max()}]\",\n",
    "           \"Performance features\": len(perf_cols),\n",
    "           \"Demographic features\": len(demographic_cols),\n",
    "           \"Activity features\": len(activity_cols),\n",
    "           \"Other features\": len(other_cols),\n",
    "           \"Categorical features\": len(df.columns[list(np.logical_and(df.nunique() > 2, df.dtypes == \"object\"))]),     \n",
    "           \"Total cardinality\": df[df.columns[list(np.logical_and(df.nunique() > 2, df.dtypes == \"object\"))]].nunique().sum(),     \n",
    "#            \"High cardinality levels\":  list(df.loc[:,list(df.columns[list(np.logical_and(df.nunique() >= 10, df.dtypes == \"object\"))])].nunique().sort_values().values),\n",
    "          \n",
    "}\n",
    "desc_df = pd.DataFrame([desc_df_dict],index=[\"cortez\"])\n",
    "desc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7cc6867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAT_S11</th>\n",
       "      <td>0.643838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CR_S11</th>\n",
       "      <td>0.653572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC_S11</th>\n",
       "      <td>0.634900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BIO_S11</th>\n",
       "      <td>0.666635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENG_S11</th>\n",
       "      <td>0.662169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEL</th>\n",
       "      <td>0.271465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEL_IHE</th>\n",
       "      <td>0.374400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           target\n",
       "MAT_S11  0.643838\n",
       "CR_S11   0.653572\n",
       "CC_S11   0.634900\n",
       "BIO_S11  0.666635\n",
       "ENG_S11  0.662169\n",
       "SEL      0.271465\n",
       "SEL_IHE  0.374400\n",
       "target   1.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pd.concat([df.drop(y_col,axis=1),pd.Series(df[y_col].values,index=df.index,name=\"target\")],axis=1).corr()[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0e1463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{ll}\n",
      "\\toprule\n",
      "{} &             cortez \\\\\n",
      "\\midrule\n",
      "N                    &              12411 \\\\\n",
      "d                    &                 33 \\\\\n",
      "\\% NA                 &                0.0 \\\\\n",
      "Target               &  \\$y \\textbackslash in [\\$37..247] \\\\\n",
      "Performance features &                  5 \\\\\n",
      "Demographic features &                 25 \\\\\n",
      "Activity features    &                  0 \\\\\n",
      "Other features       &                  2 \\\\\n",
      "Categorical features &                 13 \\\\\n",
      "Total cardinality    &               3980 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(desc_df.transpose().to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6390a689",
   "metadata": {},
   "source": [
    "### Preprocessing and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5cc6a91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = f\"{mode}_RS{RS}_hct{hct}\"\n",
    "if mode == \"cv\":\n",
    "    data_path += f\"_{folds}folds\"\n",
    "elif mode == \"train_test\":\n",
    "    data_path += f\"_split{1-test_ratio*100}-{test_ratio*100}\"\n",
    "elif mode == \"train_val_test\":\n",
    "    data_path += f\"_split{round(100-(test_ratio+val_ratio)*100)}-{round(test_ratio*100)}-{round(val_ratio*100)}\"\n",
    "\n",
    "\n",
    "# If no data_dict for the configuration exists, run preprocessing, else load data_dict\n",
    "if not os.path.exists(f\"../data/prepared/{dataset_name}/\"+data_path+\"/data_dict.pickle\"):\n",
    "    dataset_preprocessing.process_dataset(dataset_name, target, mode, RS, hct, test_ratio, val_ratio, folds)\n",
    "with open(f\"../data/prepared/{dataset_name}/{data_path}/data_dict.pickle\", 'rb') as handle:\n",
    "        data_dict = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf4d87",
   "metadata": {},
   "source": [
    "## Evaluation of categorical data treatment methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d761d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\"ignore\", \"ohe\", \"target\", \"ordinal\", \"catboost\", \"glmm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fd38a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_rounds = 1\n",
    "max_evals = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f7ea8dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing results for fold 0, condition=ignore\n",
      "SCORE: 0.8410676121711731                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 16.13trial/s, best loss: 0.8410676121711731]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.020293189672666353}\n",
      "Default performance on Test: 0.8872931599617004\n",
      "SCORE: 0.8395212292671204                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:12<00:00, 12.86s/trial, best loss: 0.8395212292671204]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.009565280018344193, 'n_estimators': 206.0}\n",
      "Test Performance after first tuning round: 0.8189454078674316\n",
      "SCORE: 0.8460365533828735                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:14<00:00, 14.83s/trial, best loss: 0.8460365533828735]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.009565280018344193, 'n_estimators': 206.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 0.8262258768081665\n",
      "SCORE: 0.8447318077087402                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:13<00:00, 13.61s/trial, best loss: 0.8447318077087402]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.009565280018344193, 'n_estimators': 206.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.5003455615401939, 'subsample': 0.5183811612483311}\n",
      "Test Performance after third tuning round: 0.8267154097557068\n",
      "SCORE: 0.8678070902824402                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.33s/trial, best loss: 0.8678070902824402]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.009565280018344193, 'n_estimators': 206.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.5003455615401939, 'subsample': 0.5183811612483311, 'gamma': 4.011310822065431, 'reg_alpha': 83.0, 'reg_lambda': 0.09502922374524703}\n",
      "Test Performance after last tuning round: 0.8474481701850891\n",
      "Preparing results for fold 0, condition=ohe\n",
      "SCORE: 0.8781493959552872                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.26s/trial, best loss: 0.8781493959552872]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.15082085330112685}\n",
      "Default performance on Test: 0.6137527823448181\n",
      "SCORE: 0.6259236931800842                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:19<00:00, 19.29s/trial, best loss: 0.6259236931800842]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.07156355513781758, 'n_estimators': 333.0}\n",
      "Test Performance after first tuning round: 0.5970416069030762\n",
      "SCORE: 0.6307271718978882                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:19<00:00, 19.91s/trial, best loss: 0.6307271718978882]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.07156355513781758, 'n_estimators': 333.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 0.6067351698875427\n",
      "SCORE: 0.6259640455245972                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:12<00:00, 12.47s/trial, best loss: 0.6259640455245972]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.07156355513781758, 'n_estimators': 333.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.6450506562686471, 'subsample': 0.956354059320434}\n",
      "Test Performance after third tuning round: 0.6076377034187317\n",
      "SCORE: 0.6699044704437256                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.89s/trial, best loss: 0.6699044704437256]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.07156355513781758, 'n_estimators': 333.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.6450506562686471, 'subsample': 0.956354059320434, 'gamma': 4.336521079984528, 'reg_alpha': 34.0, 'reg_lambda': 0.9770092396142674}\n",
      "Test Performance after last tuning round: 0.6322408318519592\n",
      "Preparing results for fold 0, condition=target\n",
      "SCORE: 0.5888093964708796                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.50trial/s, best loss: 0.5888093964708796]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.10544950851843292}\n",
      "Default performance on Test: 0.6970465779304504\n",
      "SCORE: 0.5424882173538208                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.23trial/s, best loss: 0.5424882173538208]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.2132465622595689, 'n_estimators': 395.0}\n",
      "Test Performance after first tuning round: 0.7182556986808777\n",
      "SCORE: 0.6488064527511597                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.23s/trial, best loss: 0.6488064527511597]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.2132465622595689, 'n_estimators': 395.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 0.731319010257721\n",
      "SCORE: 0.6319736242294312                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.83s/trial, best loss: 0.6319736242294312]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.2132465622595689, 'n_estimators': 395.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.9639216949088762, 'subsample': 0.6447997763542792}\n",
      "Test Performance after third tuning round: 0.7487285733222961\n",
      "SCORE: 0.5608739852905273                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.23s/trial, best loss: 0.5608739852905273]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.2132465622595689, 'n_estimators': 395.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.9639216949088762, 'subsample': 0.6447997763542792, 'gamma': 6.828903777093516, 'reg_alpha': 85.0, 'reg_lambda': 0.9978674034019089}\n",
      "Test Performance after last tuning round: 0.6358975172042847\n",
      "Preparing results for fold 0, condition=ordinal\n",
      "SCORE: 0.9621235915067116                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.50trial/s, best loss: 0.9621235915067116]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.48150983926139357}\n",
      "Default performance on Test: 0.656136155128479\n",
      "SCORE: 0.6510717868804932                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.25trial/s, best loss: 0.6510717868804932]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.2966227818807729, 'n_estimators': 476.0}\n",
      "Test Performance after first tuning round: 0.7440215349197388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.687567412853241                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.09s/trial, best loss: 0.687567412853241]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.2966227818807729, 'n_estimators': 476.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 0.7019045352935791\n",
      "SCORE: 0.6877772808074951                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.27s/trial, best loss: 0.6877772808074951]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.2966227818807729, 'n_estimators': 476.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.9419670933945525, 'subsample': 0.9315637169974363}\n",
      "Test Performance after third tuning round: 0.7225984334945679\n",
      "SCORE: 0.7361633777618408                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.28s/trial, best loss: 0.7361633777618408]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.2966227818807729, 'n_estimators': 476.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.9419670933945525, 'subsample': 0.9315637169974363, 'gamma': 0.5624574905875169, 'reg_alpha': 171.0, 'reg_lambda': 0.886596349565324}\n",
      "Test Performance after last tuning round: 0.6870738863945007\n",
      "Preparing results for fold 0, condition=catboost\n",
      "SCORE: 0.9982119452120353                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.51trial/s, best loss: 0.9982119452120353]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.37190189426581843}\n",
      "Default performance on Test: 0.8546573519706726\n",
      "SCORE: 0.6484443545341492                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.95trial/s, best loss: 0.6484443545341492]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.35526937111262114, 'n_estimators': 467.0}\n",
      "Test Performance after first tuning round: 0.883952796459198\n",
      "SCORE: 0.7225003838539124                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.10s/trial, best loss: 0.7225003838539124]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.35526937111262114, 'n_estimators': 467.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 0.8608778715133667\n",
      "SCORE: 0.7610300183296204                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.08s/trial, best loss: 0.7610300183296204]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.35526937111262114, 'n_estimators': 467.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.6402236210010509, 'subsample': 0.7833599531072297}\n",
      "Test Performance after third tuning round: 0.8930456638336182\n",
      "SCORE: 0.6638661623001099                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.20trial/s, best loss: 0.6638661623001099]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.35526937111262114, 'n_estimators': 467.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.6402236210010509, 'subsample': 0.7833599531072297, 'gamma': 8.441430189050367, 'reg_alpha': 156.0, 'reg_lambda': 0.7648764709306102}\n",
      "Test Performance after last tuning round: 0.6560561060905457\n",
      "Preparing results for fold 0, condition=glmm\n",
      "SCORE: 0.998807346492077                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00, 16.13trial/s, best loss: 0.998807346492077]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.4881061469741095}\n",
      "Default performance on Test: 0.6441304087638855\n",
      "SCORE: 0.6173838376998901                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.40trial/s, best loss: 0.6173838376998901]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.2817555115775372, 'n_estimators': 213.0}\n",
      "Test Performance after first tuning round: 0.6695722937583923\n",
      "SCORE: 0.6817492246627808                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.51s/trial, best loss: 0.6817492246627808]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.2817555115775372, 'n_estimators': 213.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 8.0}\n",
      "Test Performance after second tuning round: 0.7068503499031067\n",
      "SCORE: 0.6903536319732666                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.54s/trial, best loss: 0.6903536319732666]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.2817555115775372, 'n_estimators': 213.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.7409242316433917, 'subsample': 0.9177186664999923}\n",
      "Test Performance after third tuning round: 0.6961634159088135\n",
      "SCORE: 0.6198540925979614                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.12s/trial, best loss: 0.6198540925979614]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.2817555115775372, 'n_estimators': 213.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.7409242316433917, 'subsample': 0.9177186664999923, 'gamma': 3.885054736650145, 'reg_alpha': 94.0, 'reg_lambda': 0.5879183555110212}\n",
      "Test Performance after last tuning round: 0.5884155035018921\n",
      "Preparing results for fold 1, condition=ignore\n",
      "SCORE: 0.8745632171630859                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 29.41trial/s, best loss: 0.8745632171630859]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.15673287255940202}\n",
      "Default performance on Test: 0.9169492721557617\n",
      "SCORE: 0.8312729597091675                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.20trial/s, best loss: 0.8312729597091675]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4484225524061201, 'n_estimators': 317.0}\n",
      "Test Performance after first tuning round: 1.0363969802856445\n",
      "SCORE: 0.8394018411636353                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.10trial/s, best loss: 0.8394018411636353]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4484225524061201, 'n_estimators': 317.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 2.0}\n",
      "Test Performance after second tuning round: 1.0746701955795288\n",
      "SCORE: 0.8368555307388306                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.99trial/s, best loss: 0.8368555307388306]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4484225524061201, 'n_estimators': 317.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.780879573203657, 'subsample': 0.6612196064199416}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance after third tuning round: 1.0888893604278564\n",
      "SCORE: 0.8237046003341675                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.53trial/s, best loss: 0.8237046003341675]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4484225524061201, 'n_estimators': 317.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.780879573203657, 'subsample': 0.6612196064199416, 'gamma': 0.46569849046834866, 'reg_alpha': 63.0, 'reg_lambda': 0.16854934370971153}\n",
      "Test Performance after last tuning round: 0.8213481903076172\n",
      "Preparing results for fold 1, condition=ohe\n",
      "SCORE: 0.9817577436370382                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.15trial/s, best loss: 0.9817577436370382]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.3548755007767775}\n",
      "Default performance on Test: 0.640597403049469\n",
      "SCORE: 0.6277630925178528                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.80s/trial, best loss: 0.6277630925178528]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.30636541734493994, 'n_estimators': 372.0}\n",
      "Test Performance after first tuning round: 0.6984301805496216\n",
      "SCORE: 0.6375299096107483                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.17s/trial, best loss: 0.6375299096107483]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.30636541734493994, 'n_estimators': 372.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 2.0}\n",
      "Test Performance after second tuning round: 0.7510579824447632\n",
      "SCORE: 0.6364725232124329                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.35s/trial, best loss: 0.6364725232124329]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.30636541734493994, 'n_estimators': 372.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.823103620895749, 'subsample': 0.9694989693460812}\n",
      "Test Performance after third tuning round: 0.7487941384315491\n",
      "SCORE: 0.6720647215843201                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.46s/trial, best loss: 0.6720647215843201]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.30636541734493994, 'n_estimators': 372.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.823103620895749, 'subsample': 0.9694989693460812, 'gamma': 0.1109663065935339, 'reg_alpha': 178.0, 'reg_lambda': 0.44419642028403905}\n",
      "Test Performance after last tuning round: 0.6731323003768921\n",
      "Preparing results for fold 1, condition=target\n",
      "SCORE: 0.9951144943639324                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.08trial/s, best loss: 0.9951144943639324]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.42336556255175695}\n",
      "Default performance on Test: 0.6884896159172058\n",
      "SCORE: 0.5345478057861328                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.86s/trial, best loss: 0.5345478057861328]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.14269311188634412, 'n_estimators': 57.0}\n",
      "Test Performance after first tuning round: 0.6480180025100708\n",
      "SCORE: 0.5582368969917297                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.72s/trial, best loss: 0.5582368969917297]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.14269311188634412, 'n_estimators': 57.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 0.680994987487793\n",
      "SCORE: 0.5579689145088196                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.10s/trial, best loss: 0.5579689145088196]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.14269311188634412, 'n_estimators': 57.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.8993118112423584, 'subsample': 0.7937857127538881}\n",
      "Test Performance after third tuning round: 0.6710250377655029\n",
      "SCORE: 0.5631698369979858                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.48s/trial, best loss: 0.5631698369979858]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.14269311188634412, 'n_estimators': 57.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.8993118112423584, 'subsample': 0.7937857127538881, 'gamma': 4.554350273435197, 'reg_alpha': 132.0, 'reg_lambda': 0.6078662054282565}\n",
      "Test Performance after last tuning round: 0.6475819945335388\n",
      "Preparing results for fold 1, condition=ordinal\n",
      "SCORE: 0.9008497556380253                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.64trial/s, best loss: 0.9008497556380253]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.2413778013819414}\n",
      "Default performance on Test: 0.6954842209815979\n",
      "SCORE: 0.6524350047111511                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.91s/trial, best loss: 0.6524350047111511]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.20023430621793126, 'n_estimators': 125.0}\n",
      "Test Performance after first tuning round: 0.6657370328903198\n",
      "SCORE: 0.6637014150619507                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.79s/trial, best loss: 0.6637014150619507]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.20023430621793126, 'n_estimators': 125.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 2.0}\n",
      "Test Performance after second tuning round: 0.6361340880393982\n",
      "SCORE: 0.6852473020553589                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.41s/trial, best loss: 0.6852473020553589]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.20023430621793126, 'n_estimators': 125.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.5294163948692616, 'subsample': 0.59720100408279}\n",
      "Test Performance after third tuning round: 0.6486120223999023\n",
      "SCORE: 0.7450985312461853                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.06trial/s, best loss: 0.7450985312461853]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.20023430621793126, 'n_estimators': 125.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.5294163948692616, 'subsample': 0.59720100408279, 'gamma': 0.9547268064766798, 'reg_alpha': 105.0, 'reg_lambda': 0.9331745402773025}\n",
      "Test Performance after last tuning round: 0.7327601313591003\n",
      "Preparing results for fold 1, condition=catboost\n",
      "SCORE: 0.8474593466570133                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.19trial/s, best loss: 0.8474593466570133]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.26982960952056917}\n",
      "Default performance on Test: 0.7796429991722107\n",
      "SCORE: 0.6447685360908508                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.88trial/s, best loss: 0.6447685360908508]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3941202225832249, 'n_estimators': 73.0}\n",
      "Test Performance after first tuning round: 0.8365575075149536\n",
      "SCORE: 0.7016911506652832                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.36trial/s, best loss: 0.7016911506652832]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3941202225832249, 'n_estimators': 73.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 0.8369659185409546\n",
      "SCORE: 0.736996591091156                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.74trial/s, best loss: 0.736996591091156]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3941202225832249, 'n_estimators': 73.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.7831382179638376, 'subsample': 0.5162764184221571}\n",
      "Test Performance after third tuning round: 1.0283888578414917\n",
      "SCORE: 0.6697337031364441                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.55trial/s, best loss: 0.6697337031364441]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3941202225832249, 'n_estimators': 73.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.7831382179638376, 'subsample': 0.5162764184221571, 'gamma': 7.1741937215064375, 'reg_alpha': 152.0, 'reg_lambda': 0.8684120056234695}\n",
      "Test Performance after last tuning round: 0.6866713166236877\n",
      "Preparing results for fold 1, condition=glmm\n",
      "SCORE: 0.9951144943639324                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.92trial/s, best loss: 0.9951144943639324]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.4678603444307141}\n",
      "Default performance on Test: 0.6651493906974792\n",
      "SCORE: 0.6163805723190308                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.04trial/s, best loss: 0.6163805723190308]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4719625648702583, 'n_estimators': 474.0}\n",
      "Test Performance after first tuning round: 0.7905535697937012\n",
      "SCORE: 0.6063567399978638                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.78trial/s, best loss: 0.6063567399978638]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4719625648702583, 'n_estimators': 474.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 0.7799276113510132\n",
      "SCORE: 0.6122155785560608                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.35trial/s, best loss: 0.6122155785560608]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4719625648702583, 'n_estimators': 474.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.9659511544022805, 'subsample': 0.848462044347671}\n",
      "Test Performance after third tuning round: 0.8218191266059875\n",
      "SCORE: 0.5989311337471008                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.70trial/s, best loss: 0.5989311337471008]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4719625648702583, 'n_estimators': 474.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.9659511544022805, 'subsample': 0.848462044347671, 'gamma': 2.7810419303676315, 'reg_alpha': 11.0, 'reg_lambda': 0.935304477949899}\n",
      "Test Performance after last tuning round: 0.6104108691215515\n",
      "Preparing results for fold 2, condition=ignore\n",
      "SCORE: 0.8614500164985657                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.56trial/s, best loss: 0.8614500164985657]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.12704491304513774}\n",
      "Default performance on Test: 0.8768927454948425\n",
      "SCORE: 0.8298482894897461                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.21trial/s, best loss: 0.8298482894897461]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.11111081339012187, 'n_estimators': 413.0}\n",
      "Test Performance after first tuning round: 0.8965367078781128\n",
      "SCORE: 0.8437216877937317                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.19s/trial, best loss: 0.8437216877937317]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.11111081339012187, 'n_estimators': 413.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 0.9702776074409485\n",
      "SCORE: 0.8372719883918762                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.14s/trial, best loss: 0.8372719883918762]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.11111081339012187, 'n_estimators': 413.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.6631071010266467, 'subsample': 0.6319349517972217}\n",
      "Test Performance after third tuning round: 0.9278262257575989\n",
      "SCORE: 0.8255817294120789                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.09s/trial, best loss: 0.8255817294120789]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.11111081339012187, 'n_estimators': 413.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.6631071010266467, 'subsample': 0.6319349517972217, 'gamma': 4.731785746803643, 'reg_alpha': 41.0, 'reg_lambda': 0.6726373219983329}\n",
      "Test Performance after last tuning round: 0.7982706427574158\n",
      "Preparing results for fold 2, condition=ohe\n",
      "SCORE: 0.9996644141882431                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.85trial/s, best loss: 0.9996644141882431]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.4640147142404778}\n",
      "Default performance on Test: 0.6155558228492737\n",
      "SCORE: 0.6313308477401733                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.44s/trial, best loss: 0.6313308477401733]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4059427951428407, 'n_estimators': 279.0}\n",
      "Test Performance after first tuning round: 0.6984714865684509\n",
      "SCORE: 0.6264084577560425                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.56s/trial, best loss: 0.6264084577560425]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4059427951428407, 'n_estimators': 279.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 8.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance after second tuning round: 0.6339802145957947\n",
      "SCORE: 0.6183180212974548                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.96s/trial, best loss: 0.6183180212974548]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4059427951428407, 'n_estimators': 279.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.5681869039331636, 'subsample': 0.8814156992396778}\n",
      "Test Performance after third tuning round: 0.6232075095176697\n",
      "SCORE: 0.7051634192466736                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.02s/trial, best loss: 0.7051634192466736]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4059427951428407, 'n_estimators': 279.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.5681869039331636, 'subsample': 0.8814156992396778, 'gamma': 1.711172078447993, 'reg_alpha': 91.0, 'reg_lambda': 0.7179433347199915}\n",
      "Test Performance after last tuning round: 0.6638799905776978\n",
      "Preparing results for fold 2, condition=target\n",
      "SCORE: 0.8140596538502122                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.29trial/s, best loss: 0.8140596538502122]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.29276718043027866}\n",
      "Default performance on Test: 0.6668726801872253\n",
      "SCORE: 0.5439116954803467                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.23trial/s, best loss: 0.5439116954803467]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4255368317932976, 'n_estimators': 464.0}\n",
      "Test Performance after first tuning round: 0.7816903591156006\n",
      "SCORE: 0.6188483238220215                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.71s/trial, best loss: 0.6188483238220215]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4255368317932976, 'n_estimators': 464.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.7689334154129028\n",
      "SCORE: 0.6551989912986755                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.54s/trial, best loss: 0.6551989912986755]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4255368317932976, 'n_estimators': 464.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.9211158763823036, 'subsample': 0.5388596362949329}\n",
      "Test Performance after third tuning round: 0.8989853858947754\n",
      "SCORE: 0.5769665837287903                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.10s/trial, best loss: 0.5769665837287903]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4255368317932976, 'n_estimators': 464.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.9211158763823036, 'subsample': 0.5388596362949329, 'gamma': 0.8445723426644788, 'reg_alpha': 110.0, 'reg_lambda': 0.21684876532708486}\n",
      "Test Performance after last tuning round: 0.6050686836242676\n",
      "Preparing results for fold 2, condition=ordinal\n",
      "SCORE: 0.9050103395450755                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.85trial/s, best loss: 0.9050103395450755]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.3010809471989242}\n",
      "Default performance on Test: 0.664759635925293\n",
      "SCORE: 0.6321970224380493                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.22s/trial, best loss: 0.6321970224380493]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.026269000538385778, 'n_estimators': 387.0}\n",
      "Test Performance after first tuning round: 0.6102349162101746\n",
      "SCORE: 0.6287420988082886                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:12<00:00, 12.55s/trial, best loss: 0.6287420988082886]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.026269000538385778, 'n_estimators': 387.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 0.6093935966491699\n",
      "SCORE: 0.6313959360122681                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.68s/trial, best loss: 0.6313959360122681]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.026269000538385778, 'n_estimators': 387.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.817028336320811, 'subsample': 0.5196749957687278}\n",
      "Test Performance after third tuning round: 0.6075047254562378\n",
      "SCORE: 0.6284462809562683                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:13<00:00, 13.25s/trial, best loss: 0.6284462809562683]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.026269000538385778, 'n_estimators': 387.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.817028336320811, 'subsample': 0.5196749957687278, 'gamma': 2.4173208866033606, 'reg_alpha': 3.0, 'reg_lambda': 0.09684600206694205}\n",
      "Test Performance after last tuning round: 0.5978645086288452\n",
      "Preparing results for fold 2, condition=catboost\n",
      "SCORE: 0.7975453169108142                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.45trial/s, best loss: 0.7975453169108142]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.23846623939856765}\n",
      "Default performance on Test: 0.7176701426506042\n",
      "SCORE: 0.6286700963973999                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.95s/trial, best loss: 0.6286700963973999]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.03127928220845866, 'n_estimators': 160.0}\n",
      "Test Performance after first tuning round: 0.6311818361282349\n",
      "SCORE: 0.6338955163955688                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.86s/trial, best loss: 0.6338955163955688]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.03127928220845866, 'n_estimators': 160.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 0.643996000289917\n",
      "SCORE: 0.6271528005599976                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.55s/trial, best loss: 0.6271528005599976]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.03127928220845866, 'n_estimators': 160.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.77554294357401, 'subsample': 0.7937669856083847}\n",
      "Test Performance after third tuning round: 0.6336870789527893\n",
      "SCORE: 0.6447235941886902                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:14<00:00, 14.95s/trial, best loss: 0.6447235941886902]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.03127928220845866, 'n_estimators': 160.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.77554294357401, 'subsample': 0.7937669856083847, 'gamma': 2.6168989065481196, 'reg_alpha': 144.0, 'reg_lambda': 0.9312139775514865}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance after last tuning round: 0.6406900882720947\n",
      "Preparing results for fold 2, condition=glmm\n",
      "SCORE: 0.7411980524744635                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.59trial/s, best loss: 0.7411980524744635]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.2169003995909477}\n",
      "Default performance on Test: 0.6543176770210266\n",
      "SCORE: 0.6109917163848877                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.30s/trial, best loss: 0.6109917163848877]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4054915953267071, 'n_estimators': 490.0}\n",
      "Test Performance after first tuning round: 0.7421249151229858\n",
      "SCORE: 0.6235190033912659                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.02trial/s, best loss: 0.6235190033912659]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4054915953267071, 'n_estimators': 490.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 4.0}\n",
      "Test Performance after second tuning round: 0.7464224100112915\n",
      "SCORE: 0.6287935376167297                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.51s/trial, best loss: 0.6287935376167297]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4054915953267071, 'n_estimators': 490.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.9106800780696732, 'subsample': 0.9304195063403093}\n",
      "Test Performance after third tuning round: 0.7358089685440063\n",
      "SCORE: 0.6258127093315125                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.33s/trial, best loss: 0.6258127093315125]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4054915953267071, 'n_estimators': 490.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.9106800780696732, 'subsample': 0.9304195063403093, 'gamma': 4.737934947295592, 'reg_alpha': 115.0, 'reg_lambda': 0.22105166283930522}\n",
      "Test Performance after last tuning round: 0.5868953466415405\n",
      "Preparing results for fold 3, condition=ignore\n",
      "SCORE: 0.9990728497505188                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 16.39trial/s, best loss: 0.9990728497505188]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.4167741889902464}\n",
      "Default performance on Test: 0.9251111745834351\n",
      "SCORE: 0.8299456834793091                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.26s/trial, best loss: 0.8299456834793091]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.07898173698350484, 'n_estimators': 60.0}\n",
      "Test Performance after first tuning round: 0.866542637348175\n",
      "SCORE: 0.8367444276809692                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.93s/trial, best loss: 0.8367444276809692]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.07898173698350484, 'n_estimators': 60.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.8765665292739868\n",
      "SCORE: 0.8340779542922974                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.60s/trial, best loss: 0.8340779542922974]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.07898173698350484, 'n_estimators': 60.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.833133403065661, 'subsample': 0.520182582235287}\n",
      "Test Performance after third tuning round: 0.8677257299423218\n",
      "SCORE: 0.852373480796814                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.55s/trial, best loss: 0.852373480796814]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.07898173698350484, 'n_estimators': 60.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.833133403065661, 'subsample': 0.520182582235287, 'gamma': 7.229464800210108, 'reg_alpha': 150.0, 'reg_lambda': 0.6712673922250044}\n",
      "Test Performance after last tuning round: 0.8793021440505981\n",
      "Preparing results for fold 3, condition=ohe\n",
      "SCORE: 0.9990727961667769                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.25trial/s, best loss: 0.9990727961667769]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.4757726581241234}\n",
      "Default performance on Test: 0.6535930037498474\n",
      "SCORE: 0.634069561958313                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.74s/trial, best loss: 0.634069561958313]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.31492331385051686, 'n_estimators': 82.0}\n",
      "Test Performance after first tuning round: 0.6538244485855103\n",
      "SCORE: 0.6300550699234009                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.90s/trial, best loss: 0.6300550699234009]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.31492331385051686, 'n_estimators': 82.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 0.6418527364730835\n",
      "SCORE: 0.6439654231071472                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.50s/trial, best loss: 0.6439654231071472]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.31492331385051686, 'n_estimators': 82.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.5227074477386526, 'subsample': 0.6519417796443358}\n",
      "Test Performance after third tuning round: 0.6604923009872437\n",
      "SCORE: 0.751654326915741                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.84s/trial, best loss: 0.751654326915741]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.31492331385051686, 'n_estimators': 82.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.5227074477386526, 'subsample': 0.6519417796443358, 'gamma': 8.969821907389628, 'reg_alpha': 109.0, 'reg_lambda': 0.7512969544231445}\n",
      "Test Performance after last tuning round: 0.7516882419586182\n",
      "Preparing results for fold 3, condition=target\n",
      "SCORE: 0.8245199763778034                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.18trial/s, best loss: 0.8245199763778034]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.29576902918865283}\n",
      "Default performance on Test: 0.7117905020713806\n",
      "SCORE: 0.5351533889770508                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.92s/trial, best loss: 0.5351533889770508]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.058114056490118166, 'n_estimators': 219.0}\n",
      "Test Performance after first tuning round: 0.6646080017089844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.6336272954940796                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:22<00:00, 22.71s/trial, best loss: 0.6336272954940796]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.058114056490118166, 'n_estimators': 219.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 0.7638765573501587\n",
      "SCORE: 0.5768582820892334                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:24<00:00, 24.04s/trial, best loss: 0.5768582820892334]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.058114056490118166, 'n_estimators': 219.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.9671231912066727, 'subsample': 0.6994001157207446}\n",
      "Test Performance after third tuning round: 0.713639497756958\n",
      "SCORE: 0.5583356022834778                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.37s/trial, best loss: 0.5583356022834778]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.058114056490118166, 'n_estimators': 219.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.9671231912066727, 'subsample': 0.6994001157207446, 'gamma': 7.781942775614344, 'reg_alpha': 82.0, 'reg_lambda': 0.8034068298515799}\n",
      "Test Performance after last tuning round: 0.6606830954551697\n",
      "Preparing results for fold 3, condition=ordinal\n",
      "SCORE: 0.853145123185918                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.21trial/s, best loss: 0.853145123185918]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.1767850439543258}\n",
      "Default performance on Test: 0.7084885239601135\n",
      "SCORE: 0.6653649210929871                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.07s/trial, best loss: 0.6653649210929871]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.32050837430237683, 'n_estimators': 443.0}\n",
      "Test Performance after first tuning round: 0.773636519908905\n",
      "SCORE: 0.6666967868804932                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.20s/trial, best loss: 0.6666967868804932]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.32050837430237683, 'n_estimators': 443.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 0.7744361162185669\n",
      "SCORE: 0.6553875207901001                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.78s/trial, best loss: 0.6553875207901001]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.32050837430237683, 'n_estimators': 443.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.6063502789796194, 'subsample': 0.8198842937441111}\n",
      "Test Performance after third tuning round: 0.8077911734580994\n",
      "SCORE: 0.7782154083251953                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.30s/trial, best loss: 0.7782154083251953]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.32050837430237683, 'n_estimators': 443.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.6063502789796194, 'subsample': 0.8198842937441111, 'gamma': 8.167507745225329, 'reg_alpha': 156.0, 'reg_lambda': 0.42398408202013615}\n",
      "Test Performance after last tuning round: 0.7798315286636353\n",
      "Preparing results for fold 3, condition=catboost\n",
      "SCORE: 0.7477954648800382                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.74trial/s, best loss: 0.7477954648800382]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.19319230738354817}\n",
      "Default performance on Test: 0.9435126185417175\n",
      "SCORE: 0.6338192224502563                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.34s/trial, best loss: 0.6338192224502563]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.13769601065211853, 'n_estimators': 57.0}\n",
      "Test Performance after first tuning round: 0.7675309777259827\n",
      "SCORE: 0.6884267330169678                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.71s/trial, best loss: 0.6884267330169678]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.13769601065211853, 'n_estimators': 57.0, 'seed': 0, 'max_depth': 18.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.7774284482002258\n",
      "SCORE: 0.6817354559898376                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.41s/trial, best loss: 0.6817354559898376]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.13769601065211853, 'n_estimators': 57.0, 'seed': 0, 'max_depth': 18.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.5646901111777547, 'subsample': 0.8882064971398808}\n",
      "Test Performance after third tuning round: 0.7816166877746582\n",
      "SCORE: 0.6367062330245972                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.58s/trial, best loss: 0.6367062330245972]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.13769601065211853, 'n_estimators': 57.0, 'seed': 0, 'max_depth': 18.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.5646901111777547, 'subsample': 0.8882064971398808, 'gamma': 7.26607748775942, 'reg_alpha': 47.0, 'reg_lambda': 0.2819908229414607}\n",
      "Test Performance after last tuning round: 0.6847394108772278\n",
      "Preparing results for fold 3, condition=glmm\n",
      "SCORE: 0.9990727961667769                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.82trial/s, best loss: 0.9990727961667769]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.4042915772739033}\n",
      "Default performance on Test: 0.6710900664329529\n",
      "SCORE: 0.6149713397026062                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.04s/trial, best loss: 0.6149713397026062]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4500039611050748, 'n_estimators': 97.0}\n",
      "Test Performance after first tuning round: 0.7350810766220093\n",
      "SCORE: 0.7070140242576599                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.03s/trial, best loss: 0.7070140242576599]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4500039611050748, 'n_estimators': 97.0, 'seed': 0, 'max_depth': 18.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.7778739333152771\n",
      "SCORE: 0.7363166809082031                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.30s/trial, best loss: 0.7363166809082031]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4500039611050748, 'n_estimators': 97.0, 'seed': 0, 'max_depth': 18.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.9971422887287058, 'subsample': 0.5189382484333427}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance after third tuning round: 0.991593062877655\n",
      "SCORE: 0.6329589486122131                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.02trial/s, best loss: 0.6329589486122131]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4500039611050748, 'n_estimators': 97.0, 'seed': 0, 'max_depth': 18.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.9971422887287058, 'subsample': 0.5189382484333427, 'gamma': 7.252222381258561, 'reg_alpha': 130.0, 'reg_lambda': 0.2237804154529448}\n",
      "Test Performance after last tuning round: 0.6435413360595703\n",
      "Preparing results for fold 4, condition=ignore\n",
      "SCORE: 0.8652374148368835                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.51trial/s, best loss: 0.8652374148368835]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.10964665314830896}\n",
      "Default performance on Test: 0.9157586693763733\n",
      "SCORE: 0.8326948881149292                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.35trial/s, best loss: 0.8326948881149292]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3560381120779025, 'n_estimators': 278.0}\n",
      "Test Performance after first tuning round: 0.9913507103919983\n",
      "SCORE: 0.8326948881149292                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.95trial/s, best loss: 0.8326948881149292]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3560381120779025, 'n_estimators': 278.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 0.9913507103919983\n",
      "SCORE: 0.8357952237129211                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.54trial/s, best loss: 0.8357952237129211]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3560381120779025, 'n_estimators': 278.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.7831008039320673, 'subsample': 0.5424910045106692}\n",
      "Test Performance after third tuning round: 1.026261806488037\n",
      "SCORE: 0.8470953702926636                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.80trial/s, best loss: 0.8470953702926636]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3560381120779025, 'n_estimators': 278.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.7831008039320673, 'subsample': 0.5424910045106692, 'gamma': 1.0841170500760589, 'reg_alpha': 167.0, 'reg_lambda': 0.6774504255910585}\n",
      "Test Performance after last tuning round: 0.8377745747566223\n",
      "Preparing results for fold 4, condition=ohe\n",
      "SCORE: 0.9333277654965795                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.21trial/s, best loss: 0.9333277654965795]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.2656019119522589}\n",
      "Default performance on Test: 0.6447173357009888\n",
      "SCORE: 0.6264769434928894                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.78s/trial, best loss: 0.6264769434928894]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.1518530273836201, 'n_estimators': 491.0}\n",
      "Test Performance after first tuning round: 0.6609727144241333\n",
      "SCORE: 0.6484199166297913                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:17<00:00, 17.55s/trial, best loss: 0.6484199166297913]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.1518530273836201, 'n_estimators': 491.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 2.0}\n",
      "Test Performance after second tuning round: 0.7327286601066589\n",
      "SCORE: 0.6387055516242981                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:14<00:00, 14.56s/trial, best loss: 0.6387055516242981]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.1518530273836201, 'n_estimators': 491.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.7530932416830991, 'subsample': 0.9898507088959406}\n",
      "Test Performance after third tuning round: 0.7088046669960022\n",
      "SCORE: 0.6567790508270264                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.83s/trial, best loss: 0.6567790508270264]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.1518530273836201, 'n_estimators': 491.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.7530932416830991, 'subsample': 0.9898507088959406, 'gamma': 6.882350586247343, 'reg_alpha': 39.0, 'reg_lambda': 0.6603742428462636}\n",
      "Test Performance after last tuning round: 0.663419783115387\n",
      "Preparing results for fold 4, condition=target\n",
      "SCORE: 0.6579320413763284                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.80trial/s, best loss: 0.6579320413763284]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.17148261156538877}\n",
      "Default performance on Test: 0.7165666818618774\n",
      "SCORE: 0.5362021327018738                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.69trial/s, best loss: 0.5362021327018738]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.2606373555310545, 'n_estimators': 500.0}\n",
      "Test Performance after first tuning round: 0.7728849053382874\n",
      "SCORE: 0.5362021327018738                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.42trial/s, best loss: 0.5362021327018738]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.2606373555310545, 'n_estimators': 500.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 0.0}\n",
      "Test Performance after second tuning round: 0.7728849053382874\n",
      "SCORE: 0.5389230847358704                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.67trial/s, best loss: 0.5389230847358704]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.2606373555310545, 'n_estimators': 500.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 0.0, 'colsample_bytree': 0.9404504667774474, 'subsample': 0.5663050223914157}\n",
      "Test Performance after third tuning round: 0.8453145027160645\n",
      "SCORE: 0.5447033047676086                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.12trial/s, best loss: 0.5447033047676086]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.2606373555310545, 'n_estimators': 500.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 0.0, 'colsample_bytree': 0.9404504667774474, 'subsample': 0.5663050223914157, 'gamma': 1.497096879844173, 'reg_alpha': 73.0, 'reg_lambda': 0.5597222314387882}\n",
      "Test Performance after last tuning round: 0.6637213230133057\n",
      "Preparing results for fold 4, condition=ordinal\n",
      "SCORE: 0.9471436402250036                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.28trial/s, best loss: 0.9471436402250036]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.48689663107401504}\n",
      "Default performance on Test: 0.6965401768684387\n",
      "SCORE: 0.6617347002029419                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.96trial/s, best loss: 0.6617347002029419]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.37406331130531856, 'n_estimators': 497.0}\n",
      "Test Performance after first tuning round: 0.7986522316932678\n",
      "SCORE: 0.6766167879104614                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.97trial/s, best loss: 0.6766167879104614]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.37406331130531856, 'n_estimators': 497.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 0.7719637751579285\n",
      "SCORE: 0.6581056714057922                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.96trial/s, best loss: 0.6581056714057922]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.37406331130531856, 'n_estimators': 497.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.5307988666403927, 'subsample': 0.6728625284742892}\n",
      "Test Performance after third tuning round: 0.786969780921936\n",
      "SCORE: 0.7001572847366333                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.18trial/s, best loss: 0.7001572847366333]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.37406331130531856, 'n_estimators': 497.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.5307988666403927, 'subsample': 0.6728625284742892, 'gamma': 0.36640925628864884, 'reg_alpha': 58.0, 'reg_lambda': 0.6929890363691336}\n",
      "Test Performance after last tuning round: 0.6664562225341797\n",
      "Preparing results for fold 4, condition=catboost\n",
      "SCORE: 0.9727141270906289                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.51trial/s, best loss: 0.9727141270906289]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.34658793961737644}\n",
      "Default performance on Test: 0.7697621583938599\n",
      "SCORE: 0.6580770611763                                                                                                 \n",
      "100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.47trial/s, best loss: 0.6580770611763]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4588708379769055, 'n_estimators': 372.0}\n",
      "Test Performance after first tuning round: 0.9167183637619019\n",
      "SCORE: 0.7348212003707886                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.26trial/s, best loss: 0.7348212003707886]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4588708379769055, 'n_estimators': 372.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.8735144138336182\n",
      "SCORE: 0.7691704630851746                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.22trial/s, best loss: 0.7691704630851746]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4588708379769055, 'n_estimators': 372.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.9177977150374286, 'subsample': 0.8165803263151872}\n",
      "Test Performance after third tuning round: 0.9269899725914001\n",
      "SCORE: 0.6382161378860474                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.10s/trial, best loss: 0.6382161378860474]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4588708379769055, 'n_estimators': 372.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.9177977150374286, 'subsample': 0.8165803263151872, 'gamma': 1.2079634581123284, 'reg_alpha': 63.0, 'reg_lambda': 0.3343079806026342}\n",
      "Test Performance after last tuning round: 0.6754939556121826\n",
      "Preparing results for fold 4, condition=glmm\n",
      "SCORE: 0.6853414087278527                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.66trial/s, best loss: 0.6853414087278527]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.15847552293761005}\n",
      "Default performance on Test: 0.6848561763763428\n",
      "SCORE: 0.6008161306381226                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.20s/trial, best loss: 0.6008161306381226]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.16446235649011035, 'n_estimators': 176.0}\n",
      "Test Performance after first tuning round: 0.6641440391540527\n",
      "SCORE: 0.632627010345459                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.84s/trial, best loss: 0.632627010345459]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.16446235649011035, 'n_estimators': 176.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 8.0}\n",
      "Test Performance after second tuning round: 0.6965817213058472\n",
      "SCORE: 0.6351861953735352                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.53s/trial, best loss: 0.6351861953735352]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.16446235649011035, 'n_estimators': 176.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.7294846765030926, 'subsample': 0.9138675557939224}\n",
      "Test Performance after third tuning round: 0.7020439505577087\n",
      "SCORE: 0.6039758920669556                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.02s/trial, best loss: 0.6039758920669556]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.16446235649011035, 'n_estimators': 176.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.7294846765030926, 'subsample': 0.9138675557939224, 'gamma': 3.5965996299087823, 'reg_alpha': 38.0, 'reg_lambda': 0.4639649916289539}\n",
      "Test Performance after last tuning round: 0.6160410642623901\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c7756_row17_col0, #T_c7756_row17_col1, #T_c7756_row24_col2, #T_c7756_row24_col3 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c7756\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c7756_level0_col0\" class=\"col_heading level0 col0\" >MSE Train</th>\n",
       "      <th id=\"T_c7756_level0_col1\" class=\"col_heading level0 col1\" >R2 Train</th>\n",
       "      <th id=\"T_c7756_level0_col2\" class=\"col_heading level0 col2\" >MSE Test</th>\n",
       "      <th id=\"T_c7756_level0_col3\" class=\"col_heading level0 col3\" >R2 Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row0\" class=\"row_heading level0 row0\" >LR_catboost_tuned</th>\n",
       "      <td id=\"T_c7756_row0_col0\" class=\"data row0 col0\" >0.998600</td>\n",
       "      <td id=\"T_c7756_row0_col1\" class=\"data row0 col1\" >-0.000000</td>\n",
       "      <td id=\"T_c7756_row0_col2\" class=\"data row0 col2\" >0.994600</td>\n",
       "      <td id=\"T_c7756_row0_col3\" class=\"data row0 col3\" >-0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row1\" class=\"row_heading level0 row1\" >LR_glmm_tuned</th>\n",
       "      <td id=\"T_c7756_row1_col0\" class=\"data row1 col0\" >0.998600</td>\n",
       "      <td id=\"T_c7756_row1_col1\" class=\"data row1 col1\" >-0.000000</td>\n",
       "      <td id=\"T_c7756_row1_col2\" class=\"data row1 col2\" >0.994600</td>\n",
       "      <td id=\"T_c7756_row1_col3\" class=\"data row1 col3\" >-0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row2\" class=\"row_heading level0 row2\" >Baseline</th>\n",
       "      <td id=\"T_c7756_row2_col0\" class=\"data row2 col0\" >0.998600</td>\n",
       "      <td id=\"T_c7756_row2_col1\" class=\"data row2 col1\" >-0.000000</td>\n",
       "      <td id=\"T_c7756_row2_col2\" class=\"data row2 col2\" >0.994600</td>\n",
       "      <td id=\"T_c7756_row2_col3\" class=\"data row2 col3\" >-0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row3\" class=\"row_heading level0 row3\" >LR_ordinal_tuned</th>\n",
       "      <td id=\"T_c7756_row3_col0\" class=\"data row3 col0\" >0.961200</td>\n",
       "      <td id=\"T_c7756_row3_col1\" class=\"data row3 col1\" >0.037500</td>\n",
       "      <td id=\"T_c7756_row3_col2\" class=\"data row3 col2\" >0.965400</td>\n",
       "      <td id=\"T_c7756_row3_col3\" class=\"data row3 col3\" >0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row4\" class=\"row_heading level0 row4\" >XGB_ignore</th>\n",
       "      <td id=\"T_c7756_row4_col0\" class=\"data row4 col0\" >0.686300</td>\n",
       "      <td id=\"T_c7756_row4_col1\" class=\"data row4 col1\" >0.312700</td>\n",
       "      <td id=\"T_c7756_row4_col2\" class=\"data row4 col2\" >0.887300</td>\n",
       "      <td id=\"T_c7756_row4_col3\" class=\"data row4 col3\" >0.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row5\" class=\"row_heading level0 row5\" >LR_ohe_tuned</th>\n",
       "      <td id=\"T_c7756_row5_col0\" class=\"data row5 col0\" >0.877500</td>\n",
       "      <td id=\"T_c7756_row5_col1\" class=\"data row5 col1\" >0.121300</td>\n",
       "      <td id=\"T_c7756_row5_col2\" class=\"data row5 col2\" >0.865600</td>\n",
       "      <td id=\"T_c7756_row5_col3\" class=\"data row5 col3\" >0.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row6\" class=\"row_heading level0 row6\" >XGB_catboost</th>\n",
       "      <td id=\"T_c7756_row6_col0\" class=\"data row6 col0\" >0.230200</td>\n",
       "      <td id=\"T_c7756_row6_col1\" class=\"data row6 col1\" >0.769500</td>\n",
       "      <td id=\"T_c7756_row6_col2\" class=\"data row6 col2\" >0.854700</td>\n",
       "      <td id=\"T_c7756_row6_col3\" class=\"data row6 col3\" >0.140500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row7\" class=\"row_heading level0 row7\" >XGB_ignore_tuned</th>\n",
       "      <td id=\"T_c7756_row7_col0\" class=\"data row7 col0\" >0.863300</td>\n",
       "      <td id=\"T_c7756_row7_col1\" class=\"data row7 col1\" >0.135500</td>\n",
       "      <td id=\"T_c7756_row7_col2\" class=\"data row7 col2\" >0.847300</td>\n",
       "      <td id=\"T_c7756_row7_col3\" class=\"data row7 col3\" >0.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row8\" class=\"row_heading level0 row8\" >LR_ignore_tuned</th>\n",
       "      <td id=\"T_c7756_row8_col0\" class=\"data row8 col0\" >0.840100</td>\n",
       "      <td id=\"T_c7756_row8_col1\" class=\"data row8 col1\" >0.158700</td>\n",
       "      <td id=\"T_c7756_row8_col2\" class=\"data row8 col2\" >0.822000</td>\n",
       "      <td id=\"T_c7756_row8_col3\" class=\"data row8 col3\" >0.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row9\" class=\"row_heading level0 row9\" >LR_ignore</th>\n",
       "      <td id=\"T_c7756_row9_col0\" class=\"data row9 col0\" >0.832800</td>\n",
       "      <td id=\"T_c7756_row9_col1\" class=\"data row9 col1\" >0.166000</td>\n",
       "      <td id=\"T_c7756_row9_col2\" class=\"data row9 col2\" >0.811300</td>\n",
       "      <td id=\"T_c7756_row9_col3\" class=\"data row9 col3\" >0.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row10\" class=\"row_heading level0 row10\" >LR_ordinal</th>\n",
       "      <td id=\"T_c7756_row10_col0\" class=\"data row10 col0\" >0.814600</td>\n",
       "      <td id=\"T_c7756_row10_col1\" class=\"data row10 col1\" >0.184200</td>\n",
       "      <td id=\"T_c7756_row10_col2\" class=\"data row10 col2\" >0.803500</td>\n",
       "      <td id=\"T_c7756_row10_col3\" class=\"data row10 col3\" >0.191900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row11\" class=\"row_heading level0 row11\" >XGB_target</th>\n",
       "      <td id=\"T_c7756_row11_col0\" class=\"data row11 col0\" >0.225100</td>\n",
       "      <td id=\"T_c7756_row11_col1\" class=\"data row11 col1\" >0.774500</td>\n",
       "      <td id=\"T_c7756_row11_col2\" class=\"data row11 col2\" >0.697000</td>\n",
       "      <td id=\"T_c7756_row11_col3\" class=\"data row11 col3\" >0.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row12\" class=\"row_heading level0 row12\" >XGB_ordinal_tuned</th>\n",
       "      <td id=\"T_c7756_row12_col0\" class=\"data row12 col0\" >0.696800</td>\n",
       "      <td id=\"T_c7756_row12_col1\" class=\"data row12 col1\" >0.302200</td>\n",
       "      <td id=\"T_c7756_row12_col2\" class=\"data row12 col2\" >0.687600</td>\n",
       "      <td id=\"T_c7756_row12_col3\" class=\"data row12 col3\" >0.308500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row13\" class=\"row_heading level0 row13\" >XGB_ordinal</th>\n",
       "      <td id=\"T_c7756_row13_col0\" class=\"data row13 col0\" >0.246300</td>\n",
       "      <td id=\"T_c7756_row13_col1\" class=\"data row13 col1\" >0.753400</td>\n",
       "      <td id=\"T_c7756_row13_col2\" class=\"data row13 col2\" >0.656100</td>\n",
       "      <td id=\"T_c7756_row13_col3\" class=\"data row13 col3\" >0.340100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row14\" class=\"row_heading level0 row14\" >XGB_catboost_tuned</th>\n",
       "      <td id=\"T_c7756_row14_col0\" class=\"data row14 col0\" >0.640100</td>\n",
       "      <td id=\"T_c7756_row14_col1\" class=\"data row14 col1\" >0.359000</td>\n",
       "      <td id=\"T_c7756_row14_col2\" class=\"data row14 col2\" >0.651100</td>\n",
       "      <td id=\"T_c7756_row14_col3\" class=\"data row14 col3\" >0.345200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row15\" class=\"row_heading level0 row15\" >LR_catboost</th>\n",
       "      <td id=\"T_c7756_row15_col0\" class=\"data row15 col0\" >0.627600</td>\n",
       "      <td id=\"T_c7756_row15_col1\" class=\"data row15 col1\" >0.371500</td>\n",
       "      <td id=\"T_c7756_row15_col2\" class=\"data row15 col2\" >0.648800</td>\n",
       "      <td id=\"T_c7756_row15_col3\" class=\"data row15 col3\" >0.347400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row16\" class=\"row_heading level0 row16\" >LR_target_tuned</th>\n",
       "      <td id=\"T_c7756_row16_col0\" class=\"data row16 col0\" >0.588200</td>\n",
       "      <td id=\"T_c7756_row16_col1\" class=\"data row16 col1\" >0.411000</td>\n",
       "      <td id=\"T_c7756_row16_col2\" class=\"data row16 col2\" >0.646500</td>\n",
       "      <td id=\"T_c7756_row16_col3\" class=\"data row16 col3\" >0.349800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row17\" class=\"row_heading level0 row17\" >XGB_glmm</th>\n",
       "      <td id=\"T_c7756_row17_col0\" class=\"data row17 col0\" >0.217400</td>\n",
       "      <td id=\"T_c7756_row17_col1\" class=\"data row17 col1\" >0.782200</td>\n",
       "      <td id=\"T_c7756_row17_col2\" class=\"data row17 col2\" >0.644100</td>\n",
       "      <td id=\"T_c7756_row17_col3\" class=\"data row17 col3\" >0.352200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row18\" class=\"row_heading level0 row18\" >LR_target</th>\n",
       "      <td id=\"T_c7756_row18_col0\" class=\"data row18 col0\" >0.526000</td>\n",
       "      <td id=\"T_c7756_row18_col1\" class=\"data row18 col1\" >0.473300</td>\n",
       "      <td id=\"T_c7756_row18_col2\" class=\"data row18 col2\" >0.635300</td>\n",
       "      <td id=\"T_c7756_row18_col3\" class=\"data row18 col3\" >0.361100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row19\" class=\"row_heading level0 row19\" >XGB_target_tuned</th>\n",
       "      <td id=\"T_c7756_row19_col0\" class=\"data row19 col0\" >0.541600</td>\n",
       "      <td id=\"T_c7756_row19_col1\" class=\"data row19 col1\" >0.457600</td>\n",
       "      <td id=\"T_c7756_row19_col2\" class=\"data row19 col2\" >0.632200</td>\n",
       "      <td id=\"T_c7756_row19_col3\" class=\"data row19 col3\" >0.364200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row20\" class=\"row_heading level0 row20\" >XGB_ohe_tuned</th>\n",
       "      <td id=\"T_c7756_row20_col0\" class=\"data row20 col0\" >0.639400</td>\n",
       "      <td id=\"T_c7756_row20_col1\" class=\"data row20 col1\" >0.359700</td>\n",
       "      <td id=\"T_c7756_row20_col2\" class=\"data row20 col2\" >0.629400</td>\n",
       "      <td id=\"T_c7756_row20_col3\" class=\"data row20 col3\" >0.367000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row21\" class=\"row_heading level0 row21\" >XGB_ohe</th>\n",
       "      <td id=\"T_c7756_row21_col0\" class=\"data row21 col0\" >0.367900</td>\n",
       "      <td id=\"T_c7756_row21_col1\" class=\"data row21 col1\" >0.631500</td>\n",
       "      <td id=\"T_c7756_row21_col2\" class=\"data row21 col2\" >0.613800</td>\n",
       "      <td id=\"T_c7756_row21_col3\" class=\"data row21 col3\" >0.382700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row22\" class=\"row_heading level0 row22\" >LR_ohe</th>\n",
       "      <td id=\"T_c7756_row22_col0\" class=\"data row22 col0\" >0.593800</td>\n",
       "      <td id=\"T_c7756_row22_col1\" class=\"data row22 col1\" >0.405300</td>\n",
       "      <td id=\"T_c7756_row22_col2\" class=\"data row22 col2\" >0.591500</td>\n",
       "      <td id=\"T_c7756_row22_col3\" class=\"data row22 col3\" >0.405100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row23\" class=\"row_heading level0 row23\" >XGB_glmm_tuned</th>\n",
       "      <td id=\"T_c7756_row23_col0\" class=\"data row23 col0\" >0.600700</td>\n",
       "      <td id=\"T_c7756_row23_col1\" class=\"data row23 col1\" >0.398400</td>\n",
       "      <td id=\"T_c7756_row23_col2\" class=\"data row23 col2\" >0.589400</td>\n",
       "      <td id=\"T_c7756_row23_col3\" class=\"data row23 col3\" >0.407200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c7756_level0_row24\" class=\"row_heading level0 row24\" >LR_glmm</th>\n",
       "      <td id=\"T_c7756_row24_col0\" class=\"data row24 col0\" >0.597700</td>\n",
       "      <td id=\"T_c7756_row24_col1\" class=\"data row24 col1\" >0.401400</td>\n",
       "      <td id=\"T_c7756_row24_col2\" class=\"data row24 col2\" >0.578100</td>\n",
       "      <td id=\"T_c7756_row24_col3\" class=\"data row24 col3\" >0.418600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26da4ae5460>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(f\"../results/{dataset_name}/{experiment_name}/results_encodings.pickle\"):\n",
    "\n",
    "    results_encodings = {}\n",
    "    results_encodings_feature_importances = {}\n",
    "\n",
    "    for fold in range(folds):\n",
    "        results_encodings[fold] = {}\n",
    "        results_encodings_feature_importances[fold] = {}\n",
    "        # Create baseline\n",
    "        y_train = data_dict[f\"y_train_{fold}\"]\n",
    "        y_val = data_dict[f\"y_val_{fold}\"]\n",
    "        y_test = data_dict[f\"y_test_{fold}\"]\n",
    "        y_train_val = np.concatenate([y_train,y_val])\n",
    "\n",
    "        y_train_val_pred_base = np.zeros(y_train_val.shape[0])#*np.mean(y_train_val)\n",
    "        y_test_pred_base = np.zeros(y_test.shape[0])#*np.mean(y_train_val)\n",
    "\n",
    "        results_encodings[fold][\"Baseline\"] = {}\n",
    "        eval_res_train = get_metrics(y_train_val, y_train_val_pred_base, target=target)\n",
    "        for metric in eval_res_train.keys():\n",
    "            results_encodings[fold][\"Baseline\"][metric + \" Train\"] = eval_res_train[metric]\n",
    "        eval_res_test = get_metrics(y_test, y_test_pred_base, target=target)\n",
    "        for metric in eval_res_test.keys():\n",
    "            results_encodings[fold][\"Baseline\"][metric + \" Test\"] = eval_res_test[metric]\n",
    "\n",
    "\n",
    "        for condition in conditions:\n",
    "            print(f\"Preparing results for fold {fold}, condition={condition}\")\n",
    "            # Retrieve data\n",
    "            z_cols = data_dict[\"z_cols\"]\n",
    "\n",
    "            X_train = data_dict[f\"X_train_{fold}\"]\n",
    "            y_train = data_dict[f\"y_train_{fold}\"]\n",
    "\n",
    "            X_val = data_dict[f\"X_val_{fold}\"]\n",
    "            y_val = data_dict[f\"y_val_{fold}\"]\n",
    "\n",
    "            X_test = data_dict[f\"X_test_{fold}\"]\n",
    "            y_test = data_dict[f\"y_test_{fold}\"]\n",
    "\n",
    "    ## ALL BUT PERFORMANCE:\n",
    "            # Define data subset for evaluation\n",
    "    #         X_train = X_train[[i for i in X_train.columns if all([j not in i for j in perf_cols])]]\n",
    "    #         X_val = X_val[[i for i in X_val.columns if all([j not in i for j in perf_cols])]]\n",
    "    #         X_test = X_test[[i for i in X_test.columns if all([j not in i for j in perf_cols])]]\n",
    "\n",
    "            # Define condition data subset\n",
    "    #         if condition != \"ignore\":\n",
    "    #             z_encoded_train = data_dict[f\"z_{condition}_encoded_train_{fold}\"] \n",
    "    #             z_encoded_val = data_dict[f\"z_{condition}_encoded_val_{fold}\"] \n",
    "    #             z_encoded_test = data_dict[f\"z_{condition}_encoded_test_{fold}\"] \n",
    "\n",
    "    #             X_train = pd.concat([X_train,z_encoded_train],axis=1)\n",
    "    #             X_val = pd.concat([X_val,z_encoded_val],axis=1)\n",
    "    #             X_test = pd.concat([X_test,z_encoded_test],axis=1)\n",
    "\n",
    "    # ALL BUT PERFORMANCE & ACTIVITY:\n",
    "    #         Define data subset for evaluation\n",
    "            X_train = X_train[[i for i in X_train.columns if all([j not in i for j in perf_cols+activity_cols])]]\n",
    "            X_val = X_val[[i for i in X_val.columns if all([j not in i for j in perf_cols+activity_cols])]]\n",
    "            X_test = X_test[[i for i in X_test.columns if all([j not in i for j in perf_cols+activity_cols])]]\n",
    "\n",
    "    #         Define condition data subset\n",
    "            if condition != \"ignore\":\n",
    "                z_encoded_train = data_dict[f\"z_{condition}_encoded_train_{fold}\"] \n",
    "                z_encoded_val = data_dict[f\"z_{condition}_encoded_val_{fold}\"] \n",
    "                z_encoded_test = data_dict[f\"z_{condition}_encoded_test_{fold}\"] \n",
    "\n",
    "                X_train = pd.concat([X_train,z_encoded_train],axis=1)\n",
    "                X_val = pd.concat([X_val,z_encoded_val],axis=1)\n",
    "                X_test = pd.concat([X_test,z_encoded_test],axis=1)\n",
    "\n",
    "\n",
    "    ## ONLY CATEGORICAL: --> Produces trash as almost never better than baseline\n",
    "    #         if condition != \"ignore\":        \n",
    "    #             X_train = data_dict[f\"z_{condition}_encoded_train_{fold}\"] \n",
    "    #             X_val = data_dict[f\"z_{condition}_encoded_val_{fold}\"] \n",
    "    #             X_test = data_dict[f\"z_{condition}_encoded_test_{fold}\"] \n",
    "\n",
    "    #         else:\n",
    "    #             continue\n",
    "\n",
    "            X_train_val = pd.concat([X_train,X_val])\n",
    "            y_train_val = np.concatenate([y_train,y_val])\n",
    "\n",
    "            # Train base models\n",
    "            res, feats = evaluate_lr(X_train_val, y_train_val, X_test, y_test, target=target,tune=False, seed=RS)\n",
    "            results_encodings[fold][\"LR_\"+condition] = res\n",
    "            results_encodings_feature_importances[fold][\"LR_\"+condition] = feats\n",
    "\n",
    "            res, feats = evaluate_xgb(X_train_val, y_train_val, X_test, y_test, target, tune=False, max_evals=max_evals, early_stopping_rounds=early_stopping_rounds, seed=RS)\n",
    "            results_encodings[fold][\"XGB_\"+condition] = res\n",
    "            results_encodings_feature_importances[fold][\"XGB_\"+condition] = feats\n",
    "\n",
    "            # Train tuned models\n",
    "            res, feats = evaluate_lr(X_train_val, y_train_val, X_test, y_test, target=target, max_evals=max_evals, tune=True, seed=RS)\n",
    "            results_encodings[fold][\"LR_\"+condition+\"_tuned\"] = res\n",
    "            results_encodings_feature_importances[fold][\"LR_\"+condition+\"_tuned\"] = feats\n",
    "\n",
    "            res, feats = evaluate_xgb(X_train_val, y_train_val, X_test, y_test, target, tune=True, max_evals=max_evals, early_stopping_rounds=early_stopping_rounds, seed=RS)\n",
    "            results_encodings[fold][\"XGB_\"+condition+\"_tuned\"] = res\n",
    "            results_encodings_feature_importances[fold][\"XGB_\"+condition+\"_tuned\"] = feats\n",
    "    \n",
    "    if not os.path.exists(f\"../results/{dataset_name}/{experiment_name}\"):\n",
    "        os.makedirs(f\"../results/{dataset_name}/{experiment_name}\")\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_encodings.pickle\", 'wb') as handle:\n",
    "        pickle.dump(results_encodings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_encodings_feature_importances.pickle\", 'wb') as handle:\n",
    "        pickle.dump(results_encodings_feature_importances, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "else:\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_encodings.pickle\", 'rb') as handle:\n",
    "        results_encodings = pickle.load(handle)\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_encodings_feature_importances.pickle\", 'rb') as handle:\n",
    "        results_encodings_feature_importances = pickle.load(handle)\n",
    "        \n",
    "        \n",
    "results_encodings_df = pd.DataFrame(results_encodings[0]).transpose().sort_values(\"MSE Test\",ascending=False).round(4)\n",
    "results_encodings_df[[\"MSE Train\", \"R2 Train\", \"MSE Test\", \"R2 Test\"]].style.highlight_min(subset=[\"MSE Train\", \"MSE Test\"], color = 'lightgreen', axis = 0).highlight_max(subset=[\"R2 Train\", \"R2 Test\"], color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b1592e",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb07e361",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# models = list(results_feature_importances[fold].keys())\n",
    "# model = models[0]\n",
    "\n",
    "# importances_df = pd.DataFrame(index=results_feature_importances[fold][model].index)\n",
    "# for model in results_feature_importances[fold].keys():\n",
    "#     importances_df.loc[:,model] = results_feature_importances[fold][model]\n",
    "# importances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d18855",
   "metadata": {},
   "source": [
    "### Effectiveness of Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccbe70b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_26f8a_row0_col0, #T_26f8a_row0_col1, #T_26f8a_row1_col0, #T_26f8a_row2_col0, #T_26f8a_row3_col0, #T_26f8a_row4_col0, #T_26f8a_row5_col0, #T_26f8a_row6_col0, #T_26f8a_row7_col1, #T_26f8a_row8_col1, #T_26f8a_row9_col1, #T_26f8a_row10_col0, #T_26f8a_row11_col0, #T_26f8a_row12_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_26f8a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_26f8a_level0_col0\" class=\"col_heading level0 col0\" >Untuned</th>\n",
       "      <th id=\"T_26f8a_level0_col1\" class=\"col_heading level0 col1\" >Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_26f8a_level0_row0\" class=\"row_heading level0 row0\" >Baseline</th>\n",
       "      <td id=\"T_26f8a_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_26f8a_row0_col1\" class=\"data row0 col1\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26f8a_level0_row1\" class=\"row_heading level0 row1\" >LR_catboost</th>\n",
       "      <td id=\"T_26f8a_row1_col0\" class=\"data row1 col0\" >0.660000</td>\n",
       "      <td id=\"T_26f8a_row1_col1\" class=\"data row1 col1\" >0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26f8a_level0_row2\" class=\"row_heading level0 row2\" >LR_glmm</th>\n",
       "      <td id=\"T_26f8a_row2_col0\" class=\"data row2 col0\" >0.600000</td>\n",
       "      <td id=\"T_26f8a_row2_col1\" class=\"data row2 col1\" >0.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26f8a_level0_row3\" class=\"row_heading level0 row3\" >LR_ignore</th>\n",
       "      <td id=\"T_26f8a_row3_col0\" class=\"data row3 col0\" >0.830000</td>\n",
       "      <td id=\"T_26f8a_row3_col1\" class=\"data row3 col1\" >0.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26f8a_level0_row4\" class=\"row_heading level0 row4\" >LR_ohe</th>\n",
       "      <td id=\"T_26f8a_row4_col0\" class=\"data row4 col0\" >0.610000</td>\n",
       "      <td id=\"T_26f8a_row4_col1\" class=\"data row4 col1\" >0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26f8a_level0_row5\" class=\"row_heading level0 row5\" >LR_ordinal</th>\n",
       "      <td id=\"T_26f8a_row5_col0\" class=\"data row5 col0\" >0.800000</td>\n",
       "      <td id=\"T_26f8a_row5_col1\" class=\"data row5 col1\" >0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26f8a_level0_row6\" class=\"row_heading level0 row6\" >LR_target</th>\n",
       "      <td id=\"T_26f8a_row6_col0\" class=\"data row6 col0\" >0.640000</td>\n",
       "      <td id=\"T_26f8a_row6_col1\" class=\"data row6 col1\" >0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26f8a_level0_row7\" class=\"row_heading level0 row7\" >XGB_catboost</th>\n",
       "      <td id=\"T_26f8a_row7_col0\" class=\"data row7 col0\" >0.810000</td>\n",
       "      <td id=\"T_26f8a_row7_col1\" class=\"data row7 col1\" >0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26f8a_level0_row8\" class=\"row_heading level0 row8\" >XGB_glmm</th>\n",
       "      <td id=\"T_26f8a_row8_col0\" class=\"data row8 col0\" >0.660000</td>\n",
       "      <td id=\"T_26f8a_row8_col1\" class=\"data row8 col1\" >0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26f8a_level0_row9\" class=\"row_heading level0 row9\" >XGB_ignore</th>\n",
       "      <td id=\"T_26f8a_row9_col0\" class=\"data row9 col0\" >0.900000</td>\n",
       "      <td id=\"T_26f8a_row9_col1\" class=\"data row9 col1\" >0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26f8a_level0_row10\" class=\"row_heading level0 row10\" >XGB_ohe</th>\n",
       "      <td id=\"T_26f8a_row10_col0\" class=\"data row10 col0\" >0.630000</td>\n",
       "      <td id=\"T_26f8a_row10_col1\" class=\"data row10 col1\" >0.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26f8a_level0_row11\" class=\"row_heading level0 row11\" >XGB_ordinal</th>\n",
       "      <td id=\"T_26f8a_row11_col0\" class=\"data row11 col0\" >0.680000</td>\n",
       "      <td id=\"T_26f8a_row11_col1\" class=\"data row11 col1\" >0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_26f8a_level0_row12\" class=\"row_heading level0 row12\" >XGB_target</th>\n",
       "      <td id=\"T_26f8a_row12_col0\" class=\"data row12 col0\" >0.700000</td>\n",
       "      <td id=\"T_26f8a_row12_col1\" class=\"data row12 col1\" >0.640000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26da4258be0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = results_encodings[0].keys()\n",
    "metric = \"MSE Test\"\n",
    "\n",
    "encodings_folds_df = pd.DataFrame([pd.DataFrame(results_encodings[fold_num]).loc[metric,models] for fold_num in results_encodings.keys()],index=results_encodings.keys())\n",
    "encodings_mean_df = encodings_folds_df.mean(axis=0)\n",
    "encodings_std_df = encodings_folds_df.std(axis=0)\n",
    "\n",
    "methods = sorted(list(encodings_mean_df.index))[1:]\n",
    "not_tuned = [\"Baseline\"]+methods[::2]\n",
    "tuned = [\"Baseline\"]+methods[1::2]\n",
    "\n",
    "res_df_tune_comp_mean = pd.DataFrame([encodings_mean_df.loc[not_tuned].values,encodings_mean_df.loc[tuned]],index=[\"Untuned\",\"Tuned\"],columns=not_tuned).transpose()\n",
    "res_df_tune_comp_std = pd.DataFrame([encodings_std_df.loc[not_tuned].values,encodings_std_df.loc[tuned]],index=[\"Untuned\",\"Tuned\"],columns=not_tuned).transpose()\n",
    "\n",
    "res_df_tune_comp_mean.round(2).style.highlight_min(color = 'lightgreen', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7d9f8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Untuned</th>\n",
       "      <th>Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>1.0 (0.033)</td>\n",
       "      <td>1.0 (0.033)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_catboost</th>\n",
       "      <td>0.66 (0.023)</td>\n",
       "      <td>0.88 (0.101)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_glmm</th>\n",
       "      <td>0.6 (0.02)</td>\n",
       "      <td>0.89 (0.172)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_ignore</th>\n",
       "      <td>0.83 (0.023)</td>\n",
       "      <td>0.89 (0.087)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_ohe</th>\n",
       "      <td>0.61 (0.018)</td>\n",
       "      <td>0.96 (0.064)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_ordinal</th>\n",
       "      <td>0.8 (0.023)</td>\n",
       "      <td>0.92 (0.043)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_target</th>\n",
       "      <td>0.64 (0.022)</td>\n",
       "      <td>0.8 (0.141)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_catboost</th>\n",
       "      <td>0.81 (0.088)</td>\n",
       "      <td>0.66 (0.021)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_glmm</th>\n",
       "      <td>0.66 (0.016)</td>\n",
       "      <td>0.61 (0.025)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_ignore</th>\n",
       "      <td>0.9 (0.021)</td>\n",
       "      <td>0.84 (0.031)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_ohe</th>\n",
       "      <td>0.63 (0.018)</td>\n",
       "      <td>0.67 (0.045)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_ordinal</th>\n",
       "      <td>0.68 (0.023)</td>\n",
       "      <td>0.69 (0.068)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_target</th>\n",
       "      <td>0.7 (0.02)</td>\n",
       "      <td>0.64 (0.022)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Untuned         Tuned\n",
       "Baseline       1.0 (0.033)   1.0 (0.033)\n",
       "LR_catboost   0.66 (0.023)  0.88 (0.101)\n",
       "LR_glmm         0.6 (0.02)  0.89 (0.172)\n",
       "LR_ignore     0.83 (0.023)  0.89 (0.087)\n",
       "LR_ohe        0.61 (0.018)  0.96 (0.064)\n",
       "LR_ordinal     0.8 (0.023)  0.92 (0.043)\n",
       "LR_target     0.64 (0.022)   0.8 (0.141)\n",
       "XGB_catboost  0.81 (0.088)  0.66 (0.021)\n",
       "XGB_glmm      0.66 (0.016)  0.61 (0.025)\n",
       "XGB_ignore     0.9 (0.021)  0.84 (0.031)\n",
       "XGB_ohe       0.63 (0.018)  0.67 (0.045)\n",
       "XGB_ordinal   0.68 (0.023)  0.69 (0.068)\n",
       "XGB_target      0.7 (0.02)  0.64 (0.022)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_df = res_df_tune_comp_mean.round(2).astype(str) + \" (\" +  res_df_tune_comp_std.round(3).astype(str) + \")\"\n",
    "latex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3144e076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lll}\n",
      "\\toprule\n",
      "{} &       Untuned &         Tuned \\\\\n",
      "\\midrule\n",
      "Baseline     &   1.0 (0.033) &   1.0 (0.033) \\\\\n",
      "LR\\_catboost  &  0.66 (0.023) &  0.88 (0.101) \\\\\n",
      "LR\\_glmm      &    0.6 (0.02) &  0.89 (0.172) \\\\\n",
      "LR\\_ignore    &  0.83 (0.023) &  0.89 (0.087) \\\\\n",
      "LR\\_ohe       &  0.61 (0.018) &  0.96 (0.064) \\\\\n",
      "LR\\_ordinal   &   0.8 (0.023) &  0.92 (0.043) \\\\\n",
      "LR\\_target    &  0.64 (0.022) &   0.8 (0.141) \\\\\n",
      "XGB\\_catboost &  0.81 (0.088) &  0.66 (0.021) \\\\\n",
      "XGB\\_glmm     &  0.66 (0.016) &  0.61 (0.025) \\\\\n",
      "XGB\\_ignore   &   0.9 (0.021) &  0.84 (0.031) \\\\\n",
      "XGB\\_ohe      &  0.63 (0.018) &  0.67 (0.045) \\\\\n",
      "XGB\\_ordinal  &  0.68 (0.023) &  0.69 (0.068) \\\\\n",
      "XGB\\_target   &    0.7 (0.02) &  0.64 (0.022) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_df.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4888d3a",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "641714ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_98a80_row0_col1, #T_98a80_row0_col3, #T_98a80_row0_col4, #T_98a80_row0_col5, #T_98a80_row0_col6 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_98a80\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_98a80_level0_col0\" class=\"col_heading level0 col0\" >Baseline</th>\n",
       "      <th id=\"T_98a80_level0_col1\" class=\"col_heading level0 col1\" >LR_ignore_tuned</th>\n",
       "      <th id=\"T_98a80_level0_col2\" class=\"col_heading level0 col2\" >LR_ohe_tuned</th>\n",
       "      <th id=\"T_98a80_level0_col3\" class=\"col_heading level0 col3\" >LR_target_tuned</th>\n",
       "      <th id=\"T_98a80_level0_col4\" class=\"col_heading level0 col4\" >LR_ordinal_tuned</th>\n",
       "      <th id=\"T_98a80_level0_col5\" class=\"col_heading level0 col5\" >LR_catboost_tuned</th>\n",
       "      <th id=\"T_98a80_level0_col6\" class=\"col_heading level0 col6\" >LR_glmm_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_98a80_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_98a80_row0_col0\" class=\"data row0 col0\" >0.999 (0.033)</td>\n",
       "      <td id=\"T_98a80_row0_col1\" class=\"data row0 col1\" >0.888 (0.087)</td>\n",
       "      <td id=\"T_98a80_row0_col2\" class=\"data row0 col2\" >0.956 (0.064)</td>\n",
       "      <td id=\"T_98a80_row0_col3\" class=\"data row0 col3\" >0.798 (0.141)</td>\n",
       "      <td id=\"T_98a80_row0_col4\" class=\"data row0 col4\" >0.921 (0.043)</td>\n",
       "      <td id=\"T_98a80_row0_col5\" class=\"data row0 col5\" >0.883 (0.101)</td>\n",
       "      <td id=\"T_98a80_row0_col6\" class=\"data row0 col6\" >0.888 (0.172)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26da520d2b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For LR\n",
    "models = [\"Baseline\"]+[i for i in results_encodings[0].keys() if (\"tuned\" in i and \"LR\" in i)]\n",
    "metric = \"MSE Test\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "use_df = pd.DataFrame([pd.DataFrame(results_encodings[fold_num]).loc[metric,models] for fold_num in results_encodings.keys()],index=results_encodings.keys())*-1\n",
    "\n",
    "df_mean = pd.DataFrame((-1*use_df).mean(axis=0).round(3).astype(str) + \" (\" + use_df.std(axis=0).round(3).astype(str) + \")\").transpose()\n",
    "model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "\n",
    "best_model = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "t_test_res = np.array([stats.ttest_rel(use_df[best_model].values, use_df[model].values)[1] for model in models]).round(3)\n",
    "t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    \n",
    "res_df_lr = pd.DataFrame([model_dict])\n",
    "\n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_res[i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "res_df_lr.style.apply(negative_bold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c51bbf97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1b62b_row0_col6 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1b62b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1b62b_level0_col0\" class=\"col_heading level0 col0\" >Baseline</th>\n",
       "      <th id=\"T_1b62b_level0_col1\" class=\"col_heading level0 col1\" >XGB_ignore_tuned</th>\n",
       "      <th id=\"T_1b62b_level0_col2\" class=\"col_heading level0 col2\" >XGB_ohe_tuned</th>\n",
       "      <th id=\"T_1b62b_level0_col3\" class=\"col_heading level0 col3\" >XGB_target_tuned</th>\n",
       "      <th id=\"T_1b62b_level0_col4\" class=\"col_heading level0 col4\" >XGB_ordinal_tuned</th>\n",
       "      <th id=\"T_1b62b_level0_col5\" class=\"col_heading level0 col5\" >XGB_catboost_tuned</th>\n",
       "      <th id=\"T_1b62b_level0_col6\" class=\"col_heading level0 col6\" >XGB_glmm_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1b62b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_1b62b_row0_col0\" class=\"data row0 col0\" >0.999 (0.033)</td>\n",
       "      <td id=\"T_1b62b_row0_col1\" class=\"data row0 col1\" >0.837 (0.031)</td>\n",
       "      <td id=\"T_1b62b_row0_col2\" class=\"data row0 col2\" >0.675 (0.045)</td>\n",
       "      <td id=\"T_1b62b_row0_col3\" class=\"data row0 col3\" >0.642 (0.022)</td>\n",
       "      <td id=\"T_1b62b_row0_col4\" class=\"data row0 col4\" >0.693 (0.068)</td>\n",
       "      <td id=\"T_1b62b_row0_col5\" class=\"data row0 col5\" >0.665 (0.021)</td>\n",
       "      <td id=\"T_1b62b_row0_col6\" class=\"data row0 col6\" >0.612 (0.025)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26da42582b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For LR\n",
    "models = [\"Baseline\"]+[i for i in results_encodings[0].keys() if (\"tuned\" in i and \"XGB\" in i)]\n",
    "metric = \"MSE Test\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "use_df = pd.DataFrame([pd.DataFrame(results_encodings[fold_num]).loc[metric,models] for fold_num in results_encodings.keys()],index=results_encodings.keys())*-1\n",
    "\n",
    "df_mean = pd.DataFrame((-1*use_df).mean(axis=0).round(3).astype(str) + \" (\" + use_df.std(axis=0).round(3).astype(str) + \")\").transpose()\n",
    "model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "\n",
    "best_model = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "t_test_res = np.array([stats.ttest_rel(use_df[best_model].values, use_df[model].values)[1] for model in models]).round(3)\n",
    "t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    \n",
    "res_df_xgb = pd.DataFrame([model_dict])\n",
    "    \n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_res[i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "res_df_xgb.style.apply(negative_bold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c4fedca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline</th>\n",
       "      <th>ignore</th>\n",
       "      <th>ohe</th>\n",
       "      <th>target</th>\n",
       "      <th>ordinal</th>\n",
       "      <th>catboost</th>\n",
       "      <th>glmm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.999 (0.033)</td>\n",
       "      <td>0.888 (0.087)</td>\n",
       "      <td>0.956 (0.064)</td>\n",
       "      <td>0.798 (0.141)</td>\n",
       "      <td>0.921 (0.043)</td>\n",
       "      <td>0.883 (0.101)</td>\n",
       "      <td>0.888 (0.172)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>0.999 (0.033)</td>\n",
       "      <td>0.837 (0.031)</td>\n",
       "      <td>0.675 (0.045)</td>\n",
       "      <td>0.642 (0.022)</td>\n",
       "      <td>0.693 (0.068)</td>\n",
       "      <td>0.665 (0.021)</td>\n",
       "      <td>0.612 (0.025)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Baseline         ignore            ohe         target  \\\n",
       "LR   0.999 (0.033)  0.888 (0.087)  0.956 (0.064)  0.798 (0.141)   \n",
       "XGB  0.999 (0.033)  0.837 (0.031)  0.675 (0.045)  0.642 (0.022)   \n",
       "\n",
       "           ordinal       catboost           glmm  \n",
       "LR   0.921 (0.043)  0.883 (0.101)  0.888 (0.172)  \n",
       "XGB  0.693 (0.068)  0.665 (0.021)  0.612 (0.025)  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df_lr.columns = [i.split(\"_\")[1] if i != \"Baseline\" else \"Baseline\" for i in res_df_lr.columns]    \n",
    "res_df_xgb.columns = [i.split(\"_\")[1] if i != \"Baseline\" else \"Baseline\" for i in res_df_xgb.columns]    \n",
    "\n",
    "latex_df_encodings = pd.concat([res_df_lr,res_df_xgb],axis=0)\n",
    "latex_df_encodings.index = [\"LR\", \"XGB\"]\n",
    "latex_df_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dc09516",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "{} &       Baseline &         ignore &            ohe &         target &        ordinal &       catboost &           glmm \\\\\n",
      "\\midrule\n",
      "LR  &  0.999 (0.033) &  0.888 (0.087) &  0.956 (0.064) &  0.798 (0.141) &  0.921 (0.043) &  0.883 (0.101) &  0.888 (0.172) \\\\\n",
      "XGB &  0.999 (0.033) &  0.837 (0.031) &  0.675 (0.045) &  0.642 (0.022) &  0.693 (0.068) &  0.665 (0.021) &  0.612 (0.025) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_df_encodings.round(2).to_latex())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc8295b",
   "metadata": {},
   "source": [
    "### Data subset comparisons\n",
    "\n",
    "As it does not matter which encoding method is used we use 5CV-GLMM encoding for LR and Ordinal encoding for XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45afff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = {\"demo_only\": demographic_cols,\n",
    "           \"performance_only\": perf_cols,\n",
    "#            \"activity_only\": activity_cols,\n",
    "#            \"activity_and_demo\": activity_cols+demographic_cols,\n",
    "           \"performance_and_demo\": perf_cols+demographic_cols,\n",
    "           \"all\": list(df.columns)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea911f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddeafda6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing results for fold 0, subset=demo_only\n",
      "SCORE: 0.998807346492077                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.92trial/s, best loss: 0.998807346492077]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.4691514476879839}\n",
      "Default performance on Test: 0.9057999849319458\n",
      "SCORE: 0.8181661367416382                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.59trial/s, best loss: 0.8181661367416382]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3561870396702645, 'n_estimators': 471.0}\n",
      "Test Performance after first tuning round: 1.0241843461990356\n",
      "SCORE: 0.8329249620437622                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.64trial/s, best loss: 0.8329249620437622]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3561870396702645, 'n_estimators': 471.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 1.0140677690505981\n",
      "SCORE: 0.8359103202819824                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.46trial/s, best loss: 0.8359103202819824]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3561870396702645, 'n_estimators': 471.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.7792388343695756, 'subsample': 0.5856623942649538}\n",
      "Test Performance after third tuning round: 1.1847591400146484\n",
      "SCORE: 0.8328110575675964                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.80trial/s, best loss: 0.8328110575675964]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3561870396702645, 'n_estimators': 471.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.7792388343695756, 'subsample': 0.5856623942649538, 'gamma': 5.628353663118048, 'reg_alpha': 118.0, 'reg_lambda': 0.7537645811464503}\n",
      "Test Performance after last tuning round: 0.798032283782959\n",
      "Preparing results for fold 0, subset=performance_only\n",
      "SCORE: 0.49743399024009705                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00, 23.80trial/s, best loss: 0.49743399024009705]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.2709244161952679}\n",
      "Default performance on Test: 0.41533058881759644\n",
      "SCORE: 0.40297359228134155                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.55trial/s, best loss: 0.40297359228134155]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.284438260853831, 'n_estimators': 201.0}\n",
      "Test Performance after first tuning round: 0.4450613856315613\n",
      "SCORE: 0.48872193694114685                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.02s/trial, best loss: 0.48872193694114685]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.284438260853831, 'n_estimators': 201.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 0.5010019540786743\n",
      "SCORE: 0.46938449144363403                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.13trial/s, best loss: 0.46938449144363403]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.284438260853831, 'n_estimators': 201.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.7633792997277529, 'subsample': 0.7464646703987908}\n",
      "Test Performance after third tuning round: 0.5086840391159058\n",
      "SCORE: 0.4234393537044525                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.80trial/s, best loss: 0.4234393537044525]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.284438260853831, 'n_estimators': 201.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.7633792997277529, 'subsample': 0.7464646703987908, 'gamma': 2.2452512049824196, 'reg_alpha': 136.0, 'reg_lambda': 0.18837946460032873}\n",
      "Test Performance after last tuning round: 0.4076242744922638\n",
      "Preparing results for fold 0, subset=activity_and_demo\n",
      "SCORE: 0.9336158763023796                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.28trial/s, best loss: 0.9336158763023796]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.26692034923065283}\n",
      "Default performance on Test: 0.9057999849319458\n",
      "SCORE: 0.8192008137702942                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.75trial/s, best loss: 0.8192008137702942]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3658160027645912, 'n_estimators': 358.0}\n",
      "Test Performance after first tuning round: 1.0097362995147705\n",
      "SCORE: 0.90924072265625                                                                                                \n",
      "100%|████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.52trial/s, best loss: 0.90924072265625]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3658160027645912, 'n_estimators': 358.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 0.9951981902122498\n",
      "SCORE: 0.9243966937065125                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.95trial/s, best loss: 0.9243966937065125]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3658160027645912, 'n_estimators': 358.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.9578951298714187, 'subsample': 0.5135535814787496}\n",
      "Test Performance after third tuning round: 1.1794230937957764\n",
      "SCORE: 0.8175896406173706                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.77trial/s, best loss: 0.8175896406173706]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3658160027645912, 'n_estimators': 358.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.9578951298714187, 'subsample': 0.5135535814787496, 'gamma': 0.9372931135929687, 'reg_alpha': 82.0, 'reg_lambda': 0.22019566204206564}\n",
      "Test Performance after last tuning round: 0.785005509853363\n",
      "Preparing results for fold 0, subset=performance_and_demo\n",
      "SCORE: 0.4111410411585322                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.95trial/s, best loss: 0.4111410411585322]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.11580972753564585}\n",
      "Default performance on Test: 0.43167415261268616\n",
      "SCORE: 0.3946511149406433                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.37s/trial, best loss: 0.3946511149406433]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.09764482857524383, 'n_estimators': 93.0}\n",
      "Test Performance after first tuning round: 0.38258132338523865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.4049307703971863                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.85s/trial, best loss: 0.4049307703971863]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.09764482857524383, 'n_estimators': 93.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 0.39281654357910156\n",
      "SCORE: 0.40269598364830017                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.74s/trial, best loss: 0.40269598364830017]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.09764482857524383, 'n_estimators': 93.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.6926690866822035, 'subsample': 0.9416083533004669}\n",
      "Test Performance after third tuning round: 0.3900200128555298\n",
      "SCORE: 0.40615326166152954                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.56s/trial, best loss: 0.40615326166152954]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.09764482857524383, 'n_estimators': 93.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.6926690866822035, 'subsample': 0.9416083533004669, 'gamma': 8.26822528260803, 'reg_alpha': 83.0, 'reg_lambda': 0.06673741438762848}\n",
      "Test Performance after last tuning round: 0.3889710307121277\n",
      "Preparing results for fold 0, subset=all\n",
      "SCORE: 0.5667943191968345                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.43trial/s, best loss: 0.5667943191968345]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.34888317952681547}\n",
      "Default performance on Test: 0.3999135196208954\n",
      "SCORE: 0.3915049135684967                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.45trial/s, best loss: 0.3915049135684967]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.22623861840373038, 'n_estimators': 254.0}\n",
      "Test Performance after first tuning round: 0.40502020716667175\n",
      "SCORE: 0.436987966299057                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.95s/trial, best loss: 0.436987966299057]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.22623861840373038, 'n_estimators': 254.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 0.42343664169311523\n",
      "SCORE: 0.44451451301574707                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.19s/trial, best loss: 0.44451451301574707]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.22623861840373038, 'n_estimators': 254.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.6483171217698059, 'subsample': 0.546583064682874}\n",
      "Test Performance after third tuning round: 0.44705116748809814\n",
      "SCORE: 0.4102875292301178                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.26trial/s, best loss: 0.4102875292301178]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.22623861840373038, 'n_estimators': 254.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.6483171217698059, 'subsample': 0.546583064682874, 'gamma': 7.955547519850237, 'reg_alpha': 49.0, 'reg_lambda': 0.009839051431896229}\n",
      "Test Performance after last tuning round: 0.38932985067367554\n",
      "Preparing results for fold 1, subset=demo_only\n",
      "SCORE: 0.9951144943639324                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 16.66trial/s, best loss: 0.9951144943639324]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.4375213588975074}\n",
      "Default performance on Test: 0.875645637512207\n",
      "SCORE: 0.8115955591201782                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.94trial/s, best loss: 0.8115955591201782]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3950328439573807, 'n_estimators': 306.0}\n",
      "Test Performance after first tuning round: 1.0146280527114868\n",
      "SCORE: 0.8739754557609558                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.97trial/s, best loss: 0.8739754557609558]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3950328439573807, 'n_estimators': 306.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 2.0}\n",
      "Test Performance after second tuning round: 0.9904207587242126\n",
      "SCORE: 0.8757190704345703                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.07trial/s, best loss: 0.8757190704345703]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3950328439573807, 'n_estimators': 306.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.9917971504993695, 'subsample': 0.7182128064418132}\n",
      "Test Performance after third tuning round: 1.0501041412353516\n",
      "SCORE: 0.8129207491874695                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.98trial/s, best loss: 0.8129207491874695]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3950328439573807, 'n_estimators': 306.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.9917971504993695, 'subsample': 0.7182128064418132, 'gamma': 5.970562664194611, 'reg_alpha': 53.0, 'reg_lambda': 0.33650175774887703}\n",
      "Test Performance after last tuning round: 0.8051162958145142\n",
      "Preparing results for fold 1, subset=performance_only\n",
      "SCORE: 0.506902813911438                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00, 23.25trial/s, best loss: 0.506902813911438]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.28322961718220824}\n",
      "Default performance on Test: 0.42201852798461914\n",
      "SCORE: 0.39796844124794006                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.15s/trial, best loss: 0.39796844124794006]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.007795516096758986, 'n_estimators': 483.0}\n",
      "Test Performance after first tuning round: 0.38513803482055664\n",
      "SCORE: 0.4367024302482605                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:14<00:00, 14.41s/trial, best loss: 0.4367024302482605]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.007795516096758986, 'n_estimators': 483.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 9.0}\n",
      "Test Performance after second tuning round: 0.4297959506511688\n",
      "SCORE: 0.41885581612586975                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:15<00:00, 15.74s/trial, best loss: 0.41885581612586975]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.007795516096758986, 'n_estimators': 483.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.9136610437730839, 'subsample': 0.9771211031946839}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance after third tuning round: 0.4127160310745239\n",
      "SCORE: 0.4113096296787262                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.80s/trial, best loss: 0.4113096296787262]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.007795516096758986, 'n_estimators': 483.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.9136610437730839, 'subsample': 0.9771211031946839, 'gamma': 0.7289457476182496, 'reg_alpha': 104.0, 'reg_lambda': 0.16188162048807186}\n",
      "Test Performance after last tuning round: 0.3995228111743927\n",
      "Preparing results for fold 1, subset=activity_and_demo\n",
      "SCORE: 0.9951144943639324                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.15trial/s, best loss: 0.9951144943639324]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.4682563876475205}\n",
      "Default performance on Test: 0.875645637512207\n",
      "SCORE: 0.8025026321411133                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.83trial/s, best loss: 0.8025026321411133]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.2404098709014535, 'n_estimators': 467.0}\n",
      "Test Performance after first tuning round: 0.9317979216575623\n",
      "SCORE: 0.8221657872200012                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.54trial/s, best loss: 0.8221657872200012]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.2404098709014535, 'n_estimators': 467.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 8.0}\n",
      "Test Performance after second tuning round: 0.9689628481864929\n",
      "SCORE: 0.8256006240844727                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.69trial/s, best loss: 0.8256006240844727]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.2404098709014535, 'n_estimators': 467.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.9708517800415689, 'subsample': 0.5813820444572632}\n",
      "Test Performance after third tuning round: 1.0712810754776\n",
      "SCORE: 0.8258913159370422                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.69trial/s, best loss: 0.8258913159370422]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.2404098709014535, 'n_estimators': 467.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.9708517800415689, 'subsample': 0.5813820444572632, 'gamma': 4.138397153281374, 'reg_alpha': 117.0, 'reg_lambda': 0.581757492075633}\n",
      "Test Performance after last tuning round: 0.8156946897506714\n",
      "Preparing results for fold 1, subset=performance_and_demo\n",
      "SCORE: 0.3880707357757489                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.88trial/s, best loss: 0.3880707357757489]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.024057233171532886}\n",
      "Default performance on Test: 0.4342070519924164\n",
      "SCORE: 0.3921091854572296                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.55s/trial, best loss: 0.3921091854572296]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.07602234650169262, 'n_estimators': 415.0}\n",
      "Test Performance after first tuning round: 0.40893688797950745\n",
      "SCORE: 0.3887389898300171                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.83s/trial, best loss: 0.3887389898300171]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.07602234650169262, 'n_estimators': 415.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 9.0}\n",
      "Test Performance after second tuning round: 0.4061533212661743\n",
      "SCORE: 0.3875439763069153                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.67s/trial, best loss: 0.3875439763069153]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.07602234650169262, 'n_estimators': 415.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.8011650697143378, 'subsample': 0.7973952546964643}\n",
      "Test Performance after third tuning round: 0.40742072463035583\n",
      "SCORE: 0.40338602662086487                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.10s/trial, best loss: 0.40338602662086487]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.07602234650169262, 'n_estimators': 415.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.8011650697143378, 'subsample': 0.7973952546964643, 'gamma': 2.892139815123431, 'reg_alpha': 95.0, 'reg_lambda': 0.9399385540809512}\n",
      "Test Performance after last tuning round: 0.3900825083255768\n",
      "Preparing results for fold 1, subset=all\n",
      "SCORE: 0.5972139513448675                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.40trial/s, best loss: 0.5972139513448675]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.37906773103678043}\n",
      "Default performance on Test: 0.41950058937072754\n",
      "SCORE: 0.397858202457428                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.28trial/s, best loss: 0.397858202457428]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3971711325125021, 'n_estimators': 411.0}\n",
      "Test Performance after first tuning round: 0.48791664838790894\n",
      "SCORE: 0.42750391364097595                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.81trial/s, best loss: 0.42750391364097595]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3971711325125021, 'n_estimators': 411.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 4.0}\n",
      "Test Performance after second tuning round: 0.464641809463501\n",
      "SCORE: 0.43387722969055176                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.72trial/s, best loss: 0.43387722969055176]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3971711325125021, 'n_estimators': 411.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.982920927618301, 'subsample': 0.6966393512102063}\n",
      "Test Performance after third tuning round: 0.5195894837379456\n",
      "SCORE: 0.4159491956233978                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.63trial/s, best loss: 0.4159491956233978]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3971711325125021, 'n_estimators': 411.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.982920927618301, 'subsample': 0.6966393512102063, 'gamma': 3.4391443971974036, 'reg_alpha': 87.0, 'reg_lambda': 0.5548855308983957}\n",
      "Test Performance after last tuning round: 0.3909997045993805\n",
      "Preparing results for fold 2, subset=demo_only\n",
      "SCORE: 0.8240658392362729                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.23trial/s, best loss: 0.8240658392362729]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.03026215556413114}\n",
      "Default performance on Test: 0.8449751138687134\n",
      "SCORE: 0.8110359311103821                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.74trial/s, best loss: 0.8110359311103821]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.35380015198822284, 'n_estimators': 165.0}\n",
      "Test Performance after first tuning round: 0.9009509086608887\n",
      "SCORE: 0.8461192846298218                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.09trial/s, best loss: 0.8461192846298218]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.35380015198822284, 'n_estimators': 165.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 0.993706226348877\n",
      "SCORE: 0.8642370104789734                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.33trial/s, best loss: 0.8642370104789734]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.35380015198822284, 'n_estimators': 165.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.9736946740337844, 'subsample': 0.6184699536615652}\n",
      "Test Performance after third tuning round: 1.0944591760635376\n",
      "SCORE: 0.8236715197563171                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.19trial/s, best loss: 0.8236715197563171]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.35380015198822284, 'n_estimators': 165.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.9736946740337844, 'subsample': 0.6184699536615652, 'gamma': 3.638381744156706, 'reg_alpha': 135.0, 'reg_lambda': 0.35670505274893327}\n",
      "Test Performance after last tuning round: 0.7844898104667664\n",
      "Preparing results for fold 2, subset=performance_only\n",
      "SCORE: 0.395071417093277                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00, 23.80trial/s, best loss: 0.395071417093277]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.03049443536100283}\n",
      "Default performance on Test: 0.4051690697669983\n",
      "SCORE: 0.40315109491348267                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.46trial/s, best loss: 0.40315109491348267]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3938701123979678, 'n_estimators': 394.0}\n",
      "Test Performance after first tuning round: 0.5088541507720947\n",
      "SCORE: 0.4454387128353119                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.06trial/s, best loss: 0.4454387128353119]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3938701123979678, 'n_estimators': 394.0, 'seed': 0, 'max_depth': 12.0, 'min_child_weight': 8.0}\n",
      "Test Performance after second tuning round: 0.544660747051239\n",
      "SCORE: 0.437549889087677                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.96trial/s, best loss: 0.437549889087677]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3938701123979678, 'n_estimators': 394.0, 'seed': 0, 'max_depth': 12.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.9200200742145686, 'subsample': 0.7959113032228717}\n",
      "Test Performance after third tuning round: 0.6016430854797363\n",
      "SCORE: 0.4417564868927002                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.91trial/s, best loss: 0.4417564868927002]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3938701123979678, 'n_estimators': 394.0, 'seed': 0, 'max_depth': 12.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.9200200742145686, 'subsample': 0.7959113032228717, 'gamma': 8.317678961400928, 'reg_alpha': 176.0, 'reg_lambda': 0.9730056812020778}\n",
      "Test Performance after last tuning round: 0.39490801095962524\n",
      "Preparing results for fold 2, subset=activity_and_demo\n",
      "SCORE: 0.9996644141882431                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 16.13trial/s, best loss: 0.9996644141882431]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.4444176196678208}\n",
      "Default performance on Test: 0.8449751138687134\n",
      "SCORE: 0.8069509267807007                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.16s/trial, best loss: 0.8069509267807007]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.03654266155248804, 'n_estimators': 60.0}\n",
      "Test Performance after first tuning round: 0.7762789726257324\n",
      "SCORE: 0.8408063054084778                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.75s/trial, best loss: 0.8408063054084778]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.03654266155248804, 'n_estimators': 60.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 9.0}\n",
      "Test Performance after second tuning round: 0.8090768456459045\n",
      "SCORE: 0.8168390393257141                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.61s/trial, best loss: 0.8168390393257141]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.03654266155248804, 'n_estimators': 60.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.9094707331869261, 'subsample': 0.9329676364672115}\n",
      "Test Performance after third tuning round: 0.792609691619873\n",
      "SCORE: 0.8147835731506348                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.14s/trial, best loss: 0.8147835731506348]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.03654266155248804, 'n_estimators': 60.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.9094707331869261, 'subsample': 0.9329676364672115, 'gamma': 8.87833743909912, 'reg_alpha': 13.0, 'reg_lambda': 0.0736660743686326}\n",
      "Test Performance after last tuning round: 0.7843995094299316\n",
      "Preparing results for fold 2, subset=performance_and_demo\n",
      "SCORE: 0.5983668541269334                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.77trial/s, best loss: 0.5983668541269334]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.37836690643172977}\n",
      "Default performance on Test: 0.4161408543586731\n",
      "SCORE: 0.3929498791694641                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.51s/trial, best loss: 0.3929498791694641]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.0451199625549261, 'n_estimators': 156.0}\n",
      "Test Performance after first tuning round: 0.3659683167934418\n",
      "SCORE: 0.40185075998306274                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.65s/trial, best loss: 0.40185075998306274]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.0451199625549261, 'n_estimators': 156.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 0.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance after second tuning round: 0.3768017590045929\n",
      "SCORE: 0.3967636823654175                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.23s/trial, best loss: 0.3967636823654175]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.0451199625549261, 'n_estimators': 156.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 0.0, 'colsample_bytree': 0.7194552326314301, 'subsample': 0.6803516654625412}\n",
      "Test Performance after third tuning round: 0.36664021015167236\n",
      "SCORE: 0.3949905037879944                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.32s/trial, best loss: 0.3949905037879944]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.0451199625549261, 'n_estimators': 156.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 0.0, 'colsample_bytree': 0.7194552326314301, 'subsample': 0.6803516654625412, 'gamma': 0.400001823568256, 'reg_alpha': 55.0, 'reg_lambda': 0.8281927773260546}\n",
      "Test Performance after last tuning round: 0.36223018169403076\n",
      "Preparing results for fold 2, subset=all\n",
      "SCORE: 0.42283229749968376                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.89trial/s, best loss: 0.42283229749968376]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.14275362380990547}\n",
      "Default performance on Test: 0.40400731563568115\n",
      "SCORE: 0.38679200410842896                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.60s/trial, best loss: 0.38679200410842896]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.027314541469884764, 'n_estimators': 276.0}\n",
      "Test Performance after first tuning round: 0.35723403096199036\n",
      "SCORE: 0.4158594012260437                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.96s/trial, best loss: 0.4158594012260437]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.027314541469884764, 'n_estimators': 276.0, 'seed': 0, 'max_depth': 12.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.38232657313346863\n",
      "SCORE: 0.3954654932022095                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.28s/trial, best loss: 0.3954654932022095]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.027314541469884764, 'n_estimators': 276.0, 'seed': 0, 'max_depth': 12.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.8339347496815741, 'subsample': 0.9096104698395673}\n",
      "Test Performance after third tuning round: 0.3656335771083832\n",
      "SCORE: 0.3963465094566345                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.61s/trial, best loss: 0.3963465094566345]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.027314541469884764, 'n_estimators': 276.0, 'seed': 0, 'max_depth': 12.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.8339347496815741, 'subsample': 0.9096104698395673, 'gamma': 8.379052446018807, 'reg_alpha': 47.0, 'reg_lambda': 0.5913319383191417}\n",
      "Test Performance after last tuning round: 0.36010751128196716\n",
      "Preparing results for fold 3, subset=demo_only\n",
      "SCORE: 0.8711721763807617                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.49trial/s, best loss: 0.8711721763807617]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.13824128671341102}\n",
      "Default performance on Test: 0.921075701713562\n",
      "SCORE: 0.8069814443588257                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.14s/trial, best loss: 0.8069814443588257]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.08508547663363615, 'n_estimators': 268.0}\n",
      "Test Performance after first tuning round: 0.8512009382247925\n",
      "SCORE: 0.8022724390029907                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.20trial/s, best loss: 0.8022724390029907]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.08508547663363615, 'n_estimators': 268.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 0.8284243941307068\n",
      "SCORE: 0.8024422526359558                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.20trial/s, best loss: 0.8024422526359558]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.08508547663363615, 'n_estimators': 268.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.688278725584933, 'subsample': 0.5112663741373684}\n",
      "Test Performance after third tuning round: 0.8327266573905945\n",
      "SCORE: 0.8375334739685059                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.41trial/s, best loss: 0.8375334739685059]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.08508547663363615, 'n_estimators': 268.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.688278725584933, 'subsample': 0.5112663741373684, 'gamma': 7.813661781525029, 'reg_alpha': 135.0, 'reg_lambda': 0.07334536809824344}\n",
      "Test Performance after last tuning round: 0.861402690410614\n",
      "Preparing results for fold 3, subset=performance_only\n",
      "SCORE: 0.4339444041252136                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 22.72trial/s, best loss: 0.4339444041252136]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.18136590809669098}\n",
      "Default performance on Test: 0.45961031317710876\n",
      "SCORE: 0.3950461447238922                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.04trial/s, best loss: 0.3950461447238922]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.33318710009006136, 'n_estimators': 269.0}\n",
      "Test Performance after first tuning round: 0.5123313069343567\n",
      "SCORE: 0.4378989636898041                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.03trial/s, best loss: 0.4378989636898041]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.33318710009006136, 'n_estimators': 269.0, 'seed': 0, 'max_depth': 18.0, 'min_child_weight': 10.0}\n",
      "Test Performance after second tuning round: 0.5652094483375549\n",
      "SCORE: 0.4142208993434906                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.46trial/s, best loss: 0.4142208993434906]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.33318710009006136, 'n_estimators': 269.0, 'seed': 0, 'max_depth': 18.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.5992700714315105, 'subsample': 0.6860007496441094}\n",
      "Test Performance after third tuning round: 0.7141551971435547\n",
      "SCORE: 0.4068916440010071                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.80trial/s, best loss: 0.4068916440010071]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.33318710009006136, 'n_estimators': 269.0, 'seed': 0, 'max_depth': 18.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.5992700714315105, 'subsample': 0.6860007496441094, 'gamma': 8.75961484696728, 'reg_alpha': 59.0, 'reg_lambda': 0.9038835610260592}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance after last tuning round: 0.4383499324321747\n",
      "Preparing results for fold 3, subset=activity_and_demo\n",
      "SCORE: 0.824343255604569                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.26trial/s, best loss: 0.824343255604569]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.030320786532977193}\n",
      "Default performance on Test: 0.921075701713562\n",
      "SCORE: 0.8147565126419067                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.01trial/s, best loss: 0.8147565126419067]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.2478222147240913, 'n_estimators': 408.0}\n",
      "Test Performance after first tuning round: 0.9651429057121277\n",
      "SCORE: 0.8177701830863953                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.01trial/s, best loss: 0.8177701830863953]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.2478222147240913, 'n_estimators': 408.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 1.010103464126587\n",
      "SCORE: 0.8205517530441284                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.22trial/s, best loss: 0.8205517530441284]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.2478222147240913, 'n_estimators': 408.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.651457046990447, 'subsample': 0.6581652109006029}\n",
      "Test Performance after third tuning round: 1.0776656866073608\n",
      "SCORE: 0.8231558799743652                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.20trial/s, best loss: 0.8231558799743652]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.2478222147240913, 'n_estimators': 408.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.651457046990447, 'subsample': 0.6581652109006029, 'gamma': 6.24929218918469, 'reg_alpha': 93.0, 'reg_lambda': 0.7800167839763487}\n",
      "Test Performance after last tuning round: 0.8476651310920715\n",
      "Preparing results for fold 3, subset=performance_and_demo\n",
      "SCORE: 0.4264852084763458                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.52trial/s, best loss: 0.4264852084763458]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.1664281354503603}\n",
      "Default performance on Test: 0.46780717372894287\n",
      "SCORE: 0.38561469316482544                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.30s/trial, best loss: 0.38561469316482544]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.10365438466927646, 'n_estimators': 209.0}\n",
      "Test Performance after first tuning round: 0.44019657373428345\n",
      "SCORE: 0.4106474816799164                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.47s/trial, best loss: 0.4106474816799164]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.10365438466927646, 'n_estimators': 209.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 0.4548083245754242\n",
      "SCORE: 0.40091902017593384                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.14s/trial, best loss: 0.40091902017593384]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.10365438466927646, 'n_estimators': 209.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.5046719048740225, 'subsample': 0.9014176627150392}\n",
      "Test Performance after third tuning round: 0.4568706154823303\n",
      "SCORE: 0.3851640820503235                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.73s/trial, best loss: 0.3851640820503235]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.10365438466927646, 'n_estimators': 209.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.5046719048740225, 'subsample': 0.9014176627150392, 'gamma': 3.592342746211543, 'reg_alpha': 38.0, 'reg_lambda': 0.6955880124857722}\n",
      "Test Performance after last tuning round: 0.42763030529022217\n",
      "Preparing results for fold 3, subset=all\n",
      "SCORE: 0.37313408206973514                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.40trial/s, best loss: 0.37313408206973514]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.026327071254921265}\n",
      "Default performance on Test: 0.45340967178344727\n",
      "SCORE: 0.3991062045097351                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.36trial/s, best loss: 0.3991062045097351]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4588307195086095, 'n_estimators': 236.0}\n",
      "Test Performance after first tuning round: 0.5166515707969666\n",
      "SCORE: 0.41101542115211487                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.34trial/s, best loss: 0.41101542115211487]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4588307195086095, 'n_estimators': 236.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 0.5147383213043213\n",
      "SCORE: 0.4024689793586731                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.52trial/s, best loss: 0.4024689793586731]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4588307195086095, 'n_estimators': 236.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.7467046271597242, 'subsample': 0.995334242582422}\n",
      "Test Performance after third tuning round: 0.5060125589370728\n",
      "SCORE: 0.4104503095149994                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.15trial/s, best loss: 0.4104503095149994]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4588307195086095, 'n_estimators': 236.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.7467046271597242, 'subsample': 0.995334242582422, 'gamma': 6.769213856442162, 'reg_alpha': 96.0, 'reg_lambda': 0.03060637391975185}\n",
      "Test Performance after last tuning round: 0.45093315839767456\n",
      "Preparing results for fold 4, subset=demo_only\n",
      "SCORE: 1.0008845485131568                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.24trial/s, best loss: 1.0008845485131568]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.41482367007022036}\n",
      "Default performance on Test: 0.9213951826095581\n",
      "SCORE: 0.8143004179000854                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.05trial/s, best loss: 0.8143004179000854]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.45318118216106684, 'n_estimators': 217.0}\n",
      "Test Performance after first tuning round: 1.0690839290618896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.9355239868164062                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.62trial/s, best loss: 0.9355239868164062]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.45318118216106684, 'n_estimators': 217.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 0.0}\n",
      "Test Performance after second tuning round: 1.0167723894119263\n",
      "SCORE: 0.9734603762626648                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.41trial/s, best loss: 0.9734603762626648]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.45318118216106684, 'n_estimators': 217.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 0.0, 'colsample_bytree': 0.5363102060560359, 'subsample': 0.5107018285899158}\n",
      "Test Performance after third tuning round: 1.2879382371902466\n",
      "SCORE: 0.8335188031196594                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.14trial/s, best loss: 0.8335188031196594]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.45318118216106684, 'n_estimators': 217.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 0.0, 'colsample_bytree': 0.5363102060560359, 'subsample': 0.5107018285899158, 'gamma': 2.8166491997020544, 'reg_alpha': 137.0, 'reg_lambda': 0.6176500704946594}\n",
      "Test Performance after last tuning round: 0.8322664499282837\n",
      "Preparing results for fold 4, subset=performance_only\n",
      "SCORE: 0.42154908180236816                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00, 23.80trial/s, best loss: 0.42154908180236816]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.14622975836607452}\n",
      "Default performance on Test: 0.43798020482063293\n",
      "SCORE: 0.4034607410430908                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.33trial/s, best loss: 0.4034607410430908]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4095858012740486, 'n_estimators': 342.0}\n",
      "Test Performance after first tuning round: 0.5339770317077637\n",
      "SCORE: 0.43820005655288696                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.06trial/s, best loss: 0.43820005655288696]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4095858012740486, 'n_estimators': 342.0, 'seed': 0, 'max_depth': 12.0, 'min_child_weight': 10.0}\n",
      "Test Performance after second tuning round: 0.5720316767692566\n",
      "SCORE: 0.4354518949985504                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.19trial/s, best loss: 0.4354518949985504]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4095858012740486, 'n_estimators': 342.0, 'seed': 0, 'max_depth': 12.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.9310279667987087, 'subsample': 0.5722367423008595}\n",
      "Test Performance after third tuning round: 0.6671243906021118\n",
      "SCORE: 0.42472440004348755                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.13trial/s, best loss: 0.42472440004348755]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4095858012740486, 'n_estimators': 342.0, 'seed': 0, 'max_depth': 12.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.9310279667987087, 'subsample': 0.5722367423008595, 'gamma': 7.273559933885485, 'reg_alpha': 76.0, 'reg_lambda': 0.10312151621928611}\n",
      "Test Performance after last tuning round: 0.42137569189071655\n",
      "Preparing results for fold 4, subset=activity_and_demo\n",
      "SCORE: 0.8549440457763972                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.28trial/s, best loss: 0.8549440457763972]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.06874238172632209}\n",
      "Default performance on Test: 0.9213951826095581\n",
      "SCORE: 0.8182356953620911                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.23trial/s, best loss: 0.8182356953620911]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.480900108046144, 'n_estimators': 124.0}\n",
      "Test Performance after first tuning round: 1.0463241338729858\n",
      "SCORE: 0.9324100613594055                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.82trial/s, best loss: 0.9324100613594055]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.480900108046144, 'n_estimators': 124.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 1.0624806880950928\n",
      "SCORE: 0.9482237696647644                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.68trial/s, best loss: 0.9482237696647644]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.480900108046144, 'n_estimators': 124.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.5369729427339112, 'subsample': 0.6590983956097567}\n",
      "Test Performance after third tuning round: 1.2253811359405518\n",
      "SCORE: 0.8199542760848999                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.14trial/s, best loss: 0.8199542760848999]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.480900108046144, 'n_estimators': 124.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.5369729427339112, 'subsample': 0.6590983956097567, 'gamma': 4.431752480043562, 'reg_alpha': 77.0, 'reg_lambda': 0.12411082848368993}\n",
      "Test Performance after last tuning round: 0.8276273012161255\n",
      "Preparing results for fold 4, subset=performance_and_demo\n",
      "SCORE: 0.39182523586627155                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.40trial/s, best loss: 0.39182523586627155]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.05785313708667772}\n",
      "Default performance on Test: 0.4483131170272827\n",
      "SCORE: 0.3909595310688019                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.63s/trial, best loss: 0.3909595310688019]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.020127869067286176, 'n_estimators': 449.0}\n",
      "Test Performance after first tuning round: 0.39880767464637756\n",
      "SCORE: 0.38551822304725647                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.25s/trial, best loss: 0.38551822304725647]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.020127869067286176, 'n_estimators': 449.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 0.39407509565353394\n",
      "SCORE: 0.3852306604385376                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.20s/trial, best loss: 0.3852306604385376]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.020127869067286176, 'n_estimators': 449.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.960854704850493, 'subsample': 0.7564439206411836}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance after third tuning round: 0.3932492434978485\n",
      "SCORE: 0.3941243588924408                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.27s/trial, best loss: 0.3941243588924408]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.020127869067286176, 'n_estimators': 449.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.960854704850493, 'subsample': 0.7564439206411836, 'gamma': 6.002051184760806, 'reg_alpha': 37.0, 'reg_lambda': 0.22708026466074716}\n",
      "Test Performance after last tuning round: 0.3963690996170044\n",
      "Preparing results for fold 4, subset=all\n",
      "SCORE: 0.374882599585508                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.55trial/s, best loss: 0.374882599585508]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'alpha': 0.005080564897487548}\n",
      "Default performance on Test: 0.4422115683555603\n",
      "SCORE: 0.40331339836120605                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.40trial/s, best loss: 0.40331339836120605]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4215999328760708, 'n_estimators': 379.0}\n",
      "Test Performance after first tuning round: 0.5172502994537354\n",
      "SCORE: 0.4005814492702484                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.60trial/s, best loss: 0.4005814492702484]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4215999328760708, 'n_estimators': 379.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 9.0}\n",
      "Test Performance after second tuning round: 0.5118474364280701\n",
      "SCORE: 0.4063478410243988                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.60trial/s, best loss: 0.4063478410243988]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4215999328760708, 'n_estimators': 379.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.9196417125557588, 'subsample': 0.5181878236407911}\n",
      "Test Performance after third tuning round: 0.6374777555465698\n",
      "SCORE: 0.39703160524368286                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.87trial/s, best loss: 0.39703160524368286]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4215999328760708, 'n_estimators': 379.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.9196417125557588, 'subsample': 0.5181878236407911, 'gamma': 3.478019946450231, 'reg_alpha': 7.0, 'reg_lambda': 0.12580042550561066}\n",
      "Test Performance after last tuning round: 0.40174421668052673\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_11262_row15_col0, #T_11262_row15_col1, #T_11262_row20_col2, #T_11262_row20_col3 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_11262\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_11262_level0_col0\" class=\"col_heading level0 col0\" >MSE Train</th>\n",
       "      <th id=\"T_11262_level0_col1\" class=\"col_heading level0 col1\" >R2 Train</th>\n",
       "      <th id=\"T_11262_level0_col2\" class=\"col_heading level0 col2\" >MSE Test</th>\n",
       "      <th id=\"T_11262_level0_col3\" class=\"col_heading level0 col3\" >R2 Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row0\" class=\"row_heading level0 row0\" >LR_demo_only_tuned</th>\n",
       "      <td id=\"T_11262_row0_col0\" class=\"data row0 col0\" >0.998600</td>\n",
       "      <td id=\"T_11262_row0_col1\" class=\"data row0 col1\" >-0.000000</td>\n",
       "      <td id=\"T_11262_row0_col2\" class=\"data row0 col2\" >0.994600</td>\n",
       "      <td id=\"T_11262_row0_col3\" class=\"data row0 col3\" >-0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row1\" class=\"row_heading level0 row1\" >Baseline</th>\n",
       "      <td id=\"T_11262_row1_col0\" class=\"data row1 col0\" >0.998600</td>\n",
       "      <td id=\"T_11262_row1_col1\" class=\"data row1 col1\" >-0.000000</td>\n",
       "      <td id=\"T_11262_row1_col2\" class=\"data row1 col2\" >0.994600</td>\n",
       "      <td id=\"T_11262_row1_col3\" class=\"data row1 col3\" >-0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row2\" class=\"row_heading level0 row2\" >LR_activity_and_demo_tuned</th>\n",
       "      <td id=\"T_11262_row2_col0\" class=\"data row2 col0\" >0.933100</td>\n",
       "      <td id=\"T_11262_row2_col1\" class=\"data row2 col1\" >0.065600</td>\n",
       "      <td id=\"T_11262_row2_col2\" class=\"data row2 col2\" >0.925100</td>\n",
       "      <td id=\"T_11262_row2_col3\" class=\"data row2 col3\" >0.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row3\" class=\"row_heading level0 row3\" >XGB_activity_and_demo</th>\n",
       "      <td id=\"T_11262_row3_col0\" class=\"data row3 col0\" >0.346900</td>\n",
       "      <td id=\"T_11262_row3_col1\" class=\"data row3 col1\" >0.652600</td>\n",
       "      <td id=\"T_11262_row3_col2\" class=\"data row3 col2\" >0.905800</td>\n",
       "      <td id=\"T_11262_row3_col3\" class=\"data row3 col3\" >0.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row4\" class=\"row_heading level0 row4\" >XGB_demo_only</th>\n",
       "      <td id=\"T_11262_row4_col0\" class=\"data row4 col0\" >0.346900</td>\n",
       "      <td id=\"T_11262_row4_col1\" class=\"data row4 col1\" >0.652600</td>\n",
       "      <td id=\"T_11262_row4_col2\" class=\"data row4 col2\" >0.905800</td>\n",
       "      <td id=\"T_11262_row4_col3\" class=\"data row4 col3\" >0.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row5\" class=\"row_heading level0 row5\" >XGB_demo_only_tuned</th>\n",
       "      <td id=\"T_11262_row5_col0\" class=\"data row5 col0\" >0.815900</td>\n",
       "      <td id=\"T_11262_row5_col1\" class=\"data row5 col1\" >0.182900</td>\n",
       "      <td id=\"T_11262_row5_col2\" class=\"data row5 col2\" >0.798200</td>\n",
       "      <td id=\"T_11262_row5_col3\" class=\"data row5 col3\" >0.197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row6\" class=\"row_heading level0 row6\" >LR_activity_and_demo</th>\n",
       "      <td id=\"T_11262_row6_col0\" class=\"data row6 col0\" >0.789800</td>\n",
       "      <td id=\"T_11262_row6_col1\" class=\"data row6 col1\" >0.209100</td>\n",
       "      <td id=\"T_11262_row6_col2\" class=\"data row6 col2\" >0.784100</td>\n",
       "      <td id=\"T_11262_row6_col3\" class=\"data row6 col3\" >0.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row7\" class=\"row_heading level0 row7\" >LR_demo_only</th>\n",
       "      <td id=\"T_11262_row7_col0\" class=\"data row7 col0\" >0.789800</td>\n",
       "      <td id=\"T_11262_row7_col1\" class=\"data row7 col1\" >0.209100</td>\n",
       "      <td id=\"T_11262_row7_col2\" class=\"data row7 col2\" >0.784100</td>\n",
       "      <td id=\"T_11262_row7_col3\" class=\"data row7 col3\" >0.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row8\" class=\"row_heading level0 row8\" >XGB_activity_and_demo_tuned</th>\n",
       "      <td id=\"T_11262_row8_col0\" class=\"data row8 col0\" >0.786200</td>\n",
       "      <td id=\"T_11262_row8_col1\" class=\"data row8 col1\" >0.212700</td>\n",
       "      <td id=\"T_11262_row8_col2\" class=\"data row8 col2\" >0.783700</td>\n",
       "      <td id=\"T_11262_row8_col3\" class=\"data row8 col3\" >0.211800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row9\" class=\"row_heading level0 row9\" >LR_all_tuned</th>\n",
       "      <td id=\"T_11262_row9_col0\" class=\"data row9 col0\" >0.566000</td>\n",
       "      <td id=\"T_11262_row9_col1\" class=\"data row9 col1\" >0.433100</td>\n",
       "      <td id=\"T_11262_row9_col2\" class=\"data row9 col2\" >0.561000</td>\n",
       "      <td id=\"T_11262_row9_col3\" class=\"data row9 col3\" >0.435800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row10\" class=\"row_heading level0 row10\" >LR_performance_only_tuned</th>\n",
       "      <td id=\"T_11262_row10_col0\" class=\"data row10 col0\" >0.496800</td>\n",
       "      <td id=\"T_11262_row10_col1\" class=\"data row10 col1\" >0.502500</td>\n",
       "      <td id=\"T_11262_row10_col2\" class=\"data row10 col2\" >0.491400</td>\n",
       "      <td id=\"T_11262_row10_col3\" class=\"data row10 col3\" >0.505800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row11\" class=\"row_heading level0 row11\" >XGB_performance_and_demo</th>\n",
       "      <td id=\"T_11262_row11_col0\" class=\"data row11 col0\" >0.140500</td>\n",
       "      <td id=\"T_11262_row11_col1\" class=\"data row11 col1\" >0.859300</td>\n",
       "      <td id=\"T_11262_row11_col2\" class=\"data row11 col2\" >0.431700</td>\n",
       "      <td id=\"T_11262_row11_col3\" class=\"data row11 col3\" >0.565900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row12\" class=\"row_heading level0 row12\" >XGB_performance_only</th>\n",
       "      <td id=\"T_11262_row12_col0\" class=\"data row12 col0\" >0.229900</td>\n",
       "      <td id=\"T_11262_row12_col1\" class=\"data row12 col1\" >0.769700</td>\n",
       "      <td id=\"T_11262_row12_col2\" class=\"data row12 col2\" >0.415300</td>\n",
       "      <td id=\"T_11262_row12_col3\" class=\"data row12 col3\" >0.582300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row13\" class=\"row_heading level0 row13\" >LR_performance_and_demo_tuned</th>\n",
       "      <td id=\"T_11262_row13_col0\" class=\"data row13 col0\" >0.410800</td>\n",
       "      <td id=\"T_11262_row13_col1\" class=\"data row13 col1\" >0.588700</td>\n",
       "      <td id=\"T_11262_row13_col2\" class=\"data row13 col2\" >0.405400</td>\n",
       "      <td id=\"T_11262_row13_col3\" class=\"data row13 col3\" >0.592300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row14\" class=\"row_heading level0 row14\" >XGB_performance_only_tuned</th>\n",
       "      <td id=\"T_11262_row14_col0\" class=\"data row14 col0\" >0.406700</td>\n",
       "      <td id=\"T_11262_row14_col1\" class=\"data row14 col1\" >0.592800</td>\n",
       "      <td id=\"T_11262_row14_col2\" class=\"data row14 col2\" >0.404900</td>\n",
       "      <td id=\"T_11262_row14_col3\" class=\"data row14 col3\" >0.592800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row15\" class=\"row_heading level0 row15\" >XGB_all</th>\n",
       "      <td id=\"T_11262_row15_col0\" class=\"data row15 col0\" >0.128700</td>\n",
       "      <td id=\"T_11262_row15_col1\" class=\"data row15 col1\" >0.871100</td>\n",
       "      <td id=\"T_11262_row15_col2\" class=\"data row15 col2\" >0.399900</td>\n",
       "      <td id=\"T_11262_row15_col3\" class=\"data row15 col3\" >0.597800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row16\" class=\"row_heading level0 row16\" >XGB_all_tuned</th>\n",
       "      <td id=\"T_11262_row16_col0\" class=\"data row16 col0\" >0.390800</td>\n",
       "      <td id=\"T_11262_row16_col1\" class=\"data row16 col1\" >0.608600</td>\n",
       "      <td id=\"T_11262_row16_col2\" class=\"data row16 col2\" >0.387700</td>\n",
       "      <td id=\"T_11262_row16_col3\" class=\"data row16 col3\" >0.610100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row17\" class=\"row_heading level0 row17\" >LR_performance_only</th>\n",
       "      <td id=\"T_11262_row17_col0\" class=\"data row17 col0\" >0.391600</td>\n",
       "      <td id=\"T_11262_row17_col1\" class=\"data row17 col1\" >0.607800</td>\n",
       "      <td id=\"T_11262_row17_col2\" class=\"data row17 col2\" >0.386900</td>\n",
       "      <td id=\"T_11262_row17_col3\" class=\"data row17 col3\" >0.610900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row18\" class=\"row_heading level0 row18\" >XGB_performance_and_demo_tuned</th>\n",
       "      <td id=\"T_11262_row18_col0\" class=\"data row18 col0\" >0.389100</td>\n",
       "      <td id=\"T_11262_row18_col1\" class=\"data row18 col1\" >0.610300</td>\n",
       "      <td id=\"T_11262_row18_col2\" class=\"data row18 col2\" >0.386900</td>\n",
       "      <td id=\"T_11262_row18_col3\" class=\"data row18 col3\" >0.610900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row19\" class=\"row_heading level0 row19\" >LR_performance_and_demo</th>\n",
       "      <td id=\"T_11262_row19_col0\" class=\"data row19 col0\" >0.384800</td>\n",
       "      <td id=\"T_11262_row19_col1\" class=\"data row19 col1\" >0.614600</td>\n",
       "      <td id=\"T_11262_row19_col2\" class=\"data row19 col2\" >0.378100</td>\n",
       "      <td id=\"T_11262_row19_col3\" class=\"data row19 col3\" >0.619700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_11262_level0_row20\" class=\"row_heading level0 row20\" >LR_all</th>\n",
       "      <td id=\"T_11262_row20_col0\" class=\"data row20 col0\" >0.373300</td>\n",
       "      <td id=\"T_11262_row20_col1\" class=\"data row20 col1\" >0.626200</td>\n",
       "      <td id=\"T_11262_row20_col2\" class=\"data row20 col2\" >0.363600</td>\n",
       "      <td id=\"T_11262_row20_col3\" class=\"data row20 col3\" >0.634300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26dc491be50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(f\"../results/{dataset_name}/{experiment_name}/results_subsets.pickle\"):\n",
    "\n",
    "    results_subsets = {}\n",
    "    results_subsets_feature_importances = {}\n",
    "\n",
    "    for fold in range(folds):\n",
    "        results_subsets[fold] = {}\n",
    "        results_subsets_feature_importances[fold] = {}\n",
    "        # Create baseline\n",
    "        y_train = data_dict[f\"y_train_{fold}\"]\n",
    "        y_val = data_dict[f\"y_val_{fold}\"]\n",
    "        y_test = data_dict[f\"y_test_{fold}\"]\n",
    "        y_train_val = np.concatenate([y_train,y_val])\n",
    "\n",
    "        y_train_val_pred_base = np.zeros(y_train_val.shape[0])#*np.mean(y_train_val)\n",
    "        y_test_pred_base = np.zeros(y_test.shape[0])#*np.mean(y_train_val)\n",
    "\n",
    "        results_subsets[fold][\"Baseline\"] = {}\n",
    "        eval_res_train = get_metrics(y_train_val, y_train_val_pred_base, target=target)\n",
    "        for metric in eval_res_train.keys():\n",
    "            results_subsets[fold][\"Baseline\"][metric + \" Train\"] = eval_res_train[metric]\n",
    "        eval_res_test = get_metrics(y_test, y_test_pred_base, target=target)\n",
    "        for metric in eval_res_test.keys():\n",
    "            results_subsets[fold][\"Baseline\"][metric + \" Test\"] = eval_res_test[metric]\n",
    "\n",
    "\n",
    "        for subset_key in subsets:\n",
    "            if len(subsets[subset_key])>0:\n",
    "                print(f\"Preparing results for fold {fold}, subset={subset_key}\")\n",
    "                # Retrieve data\n",
    "                z_cols = data_dict[\"z_cols\"]\n",
    "\n",
    "                X_train = data_dict[f\"X_train_{fold}\"]\n",
    "                y_train = data_dict[f\"y_train_{fold}\"]\n",
    "\n",
    "                X_val = data_dict[f\"X_val_{fold}\"]\n",
    "                y_val = data_dict[f\"y_val_{fold}\"]\n",
    "\n",
    "                X_test = data_dict[f\"X_test_{fold}\"]\n",
    "                y_test = data_dict[f\"y_test_{fold}\"]\n",
    "\n",
    "                y_train_val = np.concatenate([y_train,y_val])\n",
    "\n",
    "                # Define data subset for LR\n",
    "                z_glmm_encoded_train = data_dict[f\"z_glmm_encoded_train_{fold}\"] \n",
    "                z_glmm_encoded_val = data_dict[f\"z_glmm_encoded_val_{fold}\"] \n",
    "                z_glmm_encoded_test = data_dict[f\"z_glmm_encoded_test_{fold}\"] \n",
    "                X_train_lr = pd.concat([X_train,z_glmm_encoded_train],axis=1)\n",
    "                X_val_lr = pd.concat([X_val,z_glmm_encoded_val],axis=1)\n",
    "                X_test_lr = pd.concat([X_test,z_glmm_encoded_test],axis=1)      \n",
    "                X_train_val_lr = pd.concat([X_train_lr,X_val_lr])\n",
    "\n",
    "                # Define data subset for XGB\n",
    "                z_ordinal_encoded_train = data_dict[f\"z_ordinal_encoded_train_{fold}\"] \n",
    "                z_ordinal_encoded_val = data_dict[f\"z_ordinal_encoded_val_{fold}\"] \n",
    "                z_ordinal_encoded_test = data_dict[f\"z_ordinal_encoded_test_{fold}\"] \n",
    "                X_train_xgb = pd.concat([X_train,z_ordinal_encoded_train],axis=1)\n",
    "                X_val_xgb = pd.concat([X_val,z_ordinal_encoded_val],axis=1)\n",
    "                X_test_xgb = pd.concat([X_test,z_ordinal_encoded_test],axis=1)\n",
    "                X_train_val_xgb = pd.concat([X_train_xgb,X_val_xgb])\n",
    "\n",
    "\n",
    "                # Define data subset for evaluation\n",
    "                X_train_val_lr = X_train_val_lr[[i for i in X_train_val_lr.columns if i in subsets[subset_key]]]\n",
    "                X_test_lr = X_test_lr[[i for i in X_test_lr.columns if i in subsets[subset_key]]]\n",
    "                X_train_val_xgb = X_train_val_xgb[[i for i in X_train_val_xgb.columns if i in subsets[subset_key]]]\n",
    "                X_test_xgb = X_test_xgb[[i for i in X_test_xgb.columns if i in subsets[subset_key]]]\n",
    "\n",
    "\n",
    "                # Train base models\n",
    "                res, feats = evaluate_lr(X_train_val_lr, y_train_val, X_test_lr, y_test, target=target,tune=False, seed=RS)\n",
    "                results_subsets[fold][\"LR_\"+subset_key] = res\n",
    "                results_subsets_feature_importances[fold][\"LR_\"+subset_key] = feats\n",
    "\n",
    "                res, feats = evaluate_xgb(X_train_val_xgb, y_train_val, X_test_xgb, y_test, target, tune=False, max_evals=max_evals, early_stopping_rounds=early_stopping_rounds, seed=RS)\n",
    "                results_subsets[fold][\"XGB_\"+subset_key] = res\n",
    "                results_subsets_feature_importances[fold][\"XGB_\"+subset_key] = feats\n",
    "\n",
    "                # Train tuned models\n",
    "                res, feats = evaluate_lr(X_train_val_lr, y_train_val, X_test_lr, y_test, target=target, max_evals=max_evals, tune=True, seed=RS)\n",
    "                results_subsets[fold][\"LR_\"+subset_key+\"_tuned\"] = res\n",
    "                results_subsets_feature_importances[fold][\"LR_\"+subset_key+\"_tuned\"] = feats\n",
    "\n",
    "                res, feats = evaluate_xgb(X_train_val_xgb, y_train_val, X_test_xgb, y_test, target, tune=True, max_evals=max_evals, early_stopping_rounds=early_stopping_rounds, seed=RS)\n",
    "                results_subsets[fold][\"XGB_\"+subset_key+\"_tuned\"] = res\n",
    "                results_subsets_feature_importances[fold][\"XGB_\"+subset_key+\"_tuned\"] = feats\n",
    "            else:\n",
    "                continue                \n",
    "                \n",
    "    if not os.path.exists(f\"../results/{dataset_name}/{experiment_name}\"):\n",
    "        os.makedirs(f\"../results/{dataset_name}/{experiment_name}\")\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_subsets.pickle\", 'wb') as handle:\n",
    "        pickle.dump(results_subsets, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_subsets_feature_importances.pickle\", 'wb') as handle:\n",
    "        pickle.dump(results_subsets_feature_importances, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "else:\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_subsets.pickle\", 'rb') as handle:\n",
    "        results_subsets = pickle.load(handle)\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_subsets_feature_importances.pickle\", 'rb') as handle:\n",
    "        results_subsets_feature_importances = pickle.load(handle)\n",
    "        \n",
    "        \n",
    "results_subsets_df = pd.DataFrame(results_subsets[0]).transpose().sort_values(\"MSE Test\",ascending=False).round(4)\n",
    "results_subsets_df[[\"MSE Train\", \"R2 Train\", \"MSE Test\", \"R2 Test\"]].style.highlight_min(subset=[\"MSE Train\", \"MSE Test\"], color = 'lightgreen', axis = 0).highlight_max(subset=[\"R2 Train\", \"R2 Test\"], color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16a4e175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f4c92_row15_col0, #T_f4c92_row15_col1, #T_f4c92_row20_col2, #T_f4c92_row20_col3 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f4c92\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f4c92_level0_col0\" class=\"col_heading level0 col0\" >MSE Train</th>\n",
       "      <th id=\"T_f4c92_level0_col1\" class=\"col_heading level0 col1\" >R2 Train</th>\n",
       "      <th id=\"T_f4c92_level0_col2\" class=\"col_heading level0 col2\" >MSE Test</th>\n",
       "      <th id=\"T_f4c92_level0_col3\" class=\"col_heading level0 col3\" >R2 Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row0\" class=\"row_heading level0 row0\" >LR_demo_only_tuned</th>\n",
       "      <td id=\"T_f4c92_row0_col0\" class=\"data row0 col0\" >0.998600</td>\n",
       "      <td id=\"T_f4c92_row0_col1\" class=\"data row0 col1\" >-0.000000</td>\n",
       "      <td id=\"T_f4c92_row0_col2\" class=\"data row0 col2\" >0.994600</td>\n",
       "      <td id=\"T_f4c92_row0_col3\" class=\"data row0 col3\" >-0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row1\" class=\"row_heading level0 row1\" >Baseline</th>\n",
       "      <td id=\"T_f4c92_row1_col0\" class=\"data row1 col0\" >0.998600</td>\n",
       "      <td id=\"T_f4c92_row1_col1\" class=\"data row1 col1\" >-0.000000</td>\n",
       "      <td id=\"T_f4c92_row1_col2\" class=\"data row1 col2\" >0.994600</td>\n",
       "      <td id=\"T_f4c92_row1_col3\" class=\"data row1 col3\" >-0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row2\" class=\"row_heading level0 row2\" >LR_activity_and_demo_tuned</th>\n",
       "      <td id=\"T_f4c92_row2_col0\" class=\"data row2 col0\" >0.933100</td>\n",
       "      <td id=\"T_f4c92_row2_col1\" class=\"data row2 col1\" >0.065600</td>\n",
       "      <td id=\"T_f4c92_row2_col2\" class=\"data row2 col2\" >0.925100</td>\n",
       "      <td id=\"T_f4c92_row2_col3\" class=\"data row2 col3\" >0.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row3\" class=\"row_heading level0 row3\" >XGB_activity_and_demo</th>\n",
       "      <td id=\"T_f4c92_row3_col0\" class=\"data row3 col0\" >0.346900</td>\n",
       "      <td id=\"T_f4c92_row3_col1\" class=\"data row3 col1\" >0.652600</td>\n",
       "      <td id=\"T_f4c92_row3_col2\" class=\"data row3 col2\" >0.905800</td>\n",
       "      <td id=\"T_f4c92_row3_col3\" class=\"data row3 col3\" >0.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row4\" class=\"row_heading level0 row4\" >XGB_demo_only</th>\n",
       "      <td id=\"T_f4c92_row4_col0\" class=\"data row4 col0\" >0.346900</td>\n",
       "      <td id=\"T_f4c92_row4_col1\" class=\"data row4 col1\" >0.652600</td>\n",
       "      <td id=\"T_f4c92_row4_col2\" class=\"data row4 col2\" >0.905800</td>\n",
       "      <td id=\"T_f4c92_row4_col3\" class=\"data row4 col3\" >0.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row5\" class=\"row_heading level0 row5\" >XGB_demo_only_tuned</th>\n",
       "      <td id=\"T_f4c92_row5_col0\" class=\"data row5 col0\" >0.815900</td>\n",
       "      <td id=\"T_f4c92_row5_col1\" class=\"data row5 col1\" >0.182900</td>\n",
       "      <td id=\"T_f4c92_row5_col2\" class=\"data row5 col2\" >0.798200</td>\n",
       "      <td id=\"T_f4c92_row5_col3\" class=\"data row5 col3\" >0.197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row6\" class=\"row_heading level0 row6\" >LR_activity_and_demo</th>\n",
       "      <td id=\"T_f4c92_row6_col0\" class=\"data row6 col0\" >0.789800</td>\n",
       "      <td id=\"T_f4c92_row6_col1\" class=\"data row6 col1\" >0.209100</td>\n",
       "      <td id=\"T_f4c92_row6_col2\" class=\"data row6 col2\" >0.784100</td>\n",
       "      <td id=\"T_f4c92_row6_col3\" class=\"data row6 col3\" >0.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row7\" class=\"row_heading level0 row7\" >LR_demo_only</th>\n",
       "      <td id=\"T_f4c92_row7_col0\" class=\"data row7 col0\" >0.789800</td>\n",
       "      <td id=\"T_f4c92_row7_col1\" class=\"data row7 col1\" >0.209100</td>\n",
       "      <td id=\"T_f4c92_row7_col2\" class=\"data row7 col2\" >0.784100</td>\n",
       "      <td id=\"T_f4c92_row7_col3\" class=\"data row7 col3\" >0.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row8\" class=\"row_heading level0 row8\" >XGB_activity_and_demo_tuned</th>\n",
       "      <td id=\"T_f4c92_row8_col0\" class=\"data row8 col0\" >0.786200</td>\n",
       "      <td id=\"T_f4c92_row8_col1\" class=\"data row8 col1\" >0.212700</td>\n",
       "      <td id=\"T_f4c92_row8_col2\" class=\"data row8 col2\" >0.783700</td>\n",
       "      <td id=\"T_f4c92_row8_col3\" class=\"data row8 col3\" >0.211800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row9\" class=\"row_heading level0 row9\" >LR_all_tuned</th>\n",
       "      <td id=\"T_f4c92_row9_col0\" class=\"data row9 col0\" >0.566000</td>\n",
       "      <td id=\"T_f4c92_row9_col1\" class=\"data row9 col1\" >0.433100</td>\n",
       "      <td id=\"T_f4c92_row9_col2\" class=\"data row9 col2\" >0.561000</td>\n",
       "      <td id=\"T_f4c92_row9_col3\" class=\"data row9 col3\" >0.435800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row10\" class=\"row_heading level0 row10\" >LR_performance_only_tuned</th>\n",
       "      <td id=\"T_f4c92_row10_col0\" class=\"data row10 col0\" >0.496800</td>\n",
       "      <td id=\"T_f4c92_row10_col1\" class=\"data row10 col1\" >0.502500</td>\n",
       "      <td id=\"T_f4c92_row10_col2\" class=\"data row10 col2\" >0.491400</td>\n",
       "      <td id=\"T_f4c92_row10_col3\" class=\"data row10 col3\" >0.505800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row11\" class=\"row_heading level0 row11\" >XGB_performance_and_demo</th>\n",
       "      <td id=\"T_f4c92_row11_col0\" class=\"data row11 col0\" >0.140500</td>\n",
       "      <td id=\"T_f4c92_row11_col1\" class=\"data row11 col1\" >0.859300</td>\n",
       "      <td id=\"T_f4c92_row11_col2\" class=\"data row11 col2\" >0.431700</td>\n",
       "      <td id=\"T_f4c92_row11_col3\" class=\"data row11 col3\" >0.565900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row12\" class=\"row_heading level0 row12\" >XGB_performance_only</th>\n",
       "      <td id=\"T_f4c92_row12_col0\" class=\"data row12 col0\" >0.229900</td>\n",
       "      <td id=\"T_f4c92_row12_col1\" class=\"data row12 col1\" >0.769700</td>\n",
       "      <td id=\"T_f4c92_row12_col2\" class=\"data row12 col2\" >0.415300</td>\n",
       "      <td id=\"T_f4c92_row12_col3\" class=\"data row12 col3\" >0.582300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row13\" class=\"row_heading level0 row13\" >LR_performance_and_demo_tuned</th>\n",
       "      <td id=\"T_f4c92_row13_col0\" class=\"data row13 col0\" >0.410800</td>\n",
       "      <td id=\"T_f4c92_row13_col1\" class=\"data row13 col1\" >0.588700</td>\n",
       "      <td id=\"T_f4c92_row13_col2\" class=\"data row13 col2\" >0.405400</td>\n",
       "      <td id=\"T_f4c92_row13_col3\" class=\"data row13 col3\" >0.592300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row14\" class=\"row_heading level0 row14\" >XGB_performance_only_tuned</th>\n",
       "      <td id=\"T_f4c92_row14_col0\" class=\"data row14 col0\" >0.406700</td>\n",
       "      <td id=\"T_f4c92_row14_col1\" class=\"data row14 col1\" >0.592800</td>\n",
       "      <td id=\"T_f4c92_row14_col2\" class=\"data row14 col2\" >0.404900</td>\n",
       "      <td id=\"T_f4c92_row14_col3\" class=\"data row14 col3\" >0.592800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row15\" class=\"row_heading level0 row15\" >XGB_all</th>\n",
       "      <td id=\"T_f4c92_row15_col0\" class=\"data row15 col0\" >0.128700</td>\n",
       "      <td id=\"T_f4c92_row15_col1\" class=\"data row15 col1\" >0.871100</td>\n",
       "      <td id=\"T_f4c92_row15_col2\" class=\"data row15 col2\" >0.399900</td>\n",
       "      <td id=\"T_f4c92_row15_col3\" class=\"data row15 col3\" >0.597800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row16\" class=\"row_heading level0 row16\" >XGB_all_tuned</th>\n",
       "      <td id=\"T_f4c92_row16_col0\" class=\"data row16 col0\" >0.390800</td>\n",
       "      <td id=\"T_f4c92_row16_col1\" class=\"data row16 col1\" >0.608600</td>\n",
       "      <td id=\"T_f4c92_row16_col2\" class=\"data row16 col2\" >0.387700</td>\n",
       "      <td id=\"T_f4c92_row16_col3\" class=\"data row16 col3\" >0.610100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row17\" class=\"row_heading level0 row17\" >LR_performance_only</th>\n",
       "      <td id=\"T_f4c92_row17_col0\" class=\"data row17 col0\" >0.391600</td>\n",
       "      <td id=\"T_f4c92_row17_col1\" class=\"data row17 col1\" >0.607800</td>\n",
       "      <td id=\"T_f4c92_row17_col2\" class=\"data row17 col2\" >0.386900</td>\n",
       "      <td id=\"T_f4c92_row17_col3\" class=\"data row17 col3\" >0.610900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row18\" class=\"row_heading level0 row18\" >XGB_performance_and_demo_tuned</th>\n",
       "      <td id=\"T_f4c92_row18_col0\" class=\"data row18 col0\" >0.389100</td>\n",
       "      <td id=\"T_f4c92_row18_col1\" class=\"data row18 col1\" >0.610300</td>\n",
       "      <td id=\"T_f4c92_row18_col2\" class=\"data row18 col2\" >0.386900</td>\n",
       "      <td id=\"T_f4c92_row18_col3\" class=\"data row18 col3\" >0.610900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row19\" class=\"row_heading level0 row19\" >LR_performance_and_demo</th>\n",
       "      <td id=\"T_f4c92_row19_col0\" class=\"data row19 col0\" >0.384800</td>\n",
       "      <td id=\"T_f4c92_row19_col1\" class=\"data row19 col1\" >0.614600</td>\n",
       "      <td id=\"T_f4c92_row19_col2\" class=\"data row19 col2\" >0.378100</td>\n",
       "      <td id=\"T_f4c92_row19_col3\" class=\"data row19 col3\" >0.619700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f4c92_level0_row20\" class=\"row_heading level0 row20\" >LR_all</th>\n",
       "      <td id=\"T_f4c92_row20_col0\" class=\"data row20 col0\" >0.373300</td>\n",
       "      <td id=\"T_f4c92_row20_col1\" class=\"data row20 col1\" >0.626200</td>\n",
       "      <td id=\"T_f4c92_row20_col2\" class=\"data row20 col2\" >0.363600</td>\n",
       "      <td id=\"T_f4c92_row20_col3\" class=\"data row20 col3\" >0.634300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26dc48fddc0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_subsets_df = pd.DataFrame(results_subsets[0]).transpose().sort_values(\"MSE Test\",ascending=False).round(4)\n",
    "results_subsets_df[[\"MSE Train\", \"R2 Train\", \"MSE Test\", \"R2 Test\"]].style.highlight_min(subset=[\"MSE Train\", \"MSE Test\"], color = 'lightgreen', axis = 0).highlight_max(subset=[\"R2 Train\", \"R2 Test\"], color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e907788",
   "metadata": {},
   "source": [
    "### Effectiveness of Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "859a262f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_93fe0_row0_col0, #T_93fe0_row0_col1, #T_93fe0_row1_col0, #T_93fe0_row2_col0, #T_93fe0_row3_col0, #T_93fe0_row4_col0, #T_93fe0_row5_col0, #T_93fe0_row6_col1, #T_93fe0_row7_col1, #T_93fe0_row8_col1, #T_93fe0_row9_col1, #T_93fe0_row10_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_93fe0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_93fe0_level0_col0\" class=\"col_heading level0 col0\" >Untuned</th>\n",
       "      <th id=\"T_93fe0_level0_col1\" class=\"col_heading level0 col1\" >Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_93fe0_level0_row0\" class=\"row_heading level0 row0\" >Baseline</th>\n",
       "      <td id=\"T_93fe0_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_93fe0_row0_col1\" class=\"data row0 col1\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_93fe0_level0_row1\" class=\"row_heading level0 row1\" >LR_activity_and_demo</th>\n",
       "      <td id=\"T_93fe0_row1_col0\" class=\"data row1 col0\" >0.790000</td>\n",
       "      <td id=\"T_93fe0_row1_col1\" class=\"data row1 col1\" >0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_93fe0_level0_row2\" class=\"row_heading level0 row2\" >LR_all</th>\n",
       "      <td id=\"T_93fe0_row2_col0\" class=\"data row2 col0\" >0.370000</td>\n",
       "      <td id=\"T_93fe0_row2_col1\" class=\"data row2 col1\" >0.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_93fe0_level0_row3\" class=\"row_heading level0 row3\" >LR_demo_only</th>\n",
       "      <td id=\"T_93fe0_row3_col0\" class=\"data row3 col0\" >0.790000</td>\n",
       "      <td id=\"T_93fe0_row3_col1\" class=\"data row3 col1\" >0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_93fe0_level0_row4\" class=\"row_heading level0 row4\" >LR_performance_and_demo</th>\n",
       "      <td id=\"T_93fe0_row4_col0\" class=\"data row4 col0\" >0.390000</td>\n",
       "      <td id=\"T_93fe0_row4_col1\" class=\"data row4 col1\" >0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_93fe0_level0_row5\" class=\"row_heading level0 row5\" >LR_performance_only</th>\n",
       "      <td id=\"T_93fe0_row5_col0\" class=\"data row5 col0\" >0.390000</td>\n",
       "      <td id=\"T_93fe0_row5_col1\" class=\"data row5 col1\" >0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_93fe0_level0_row6\" class=\"row_heading level0 row6\" >XGB_activity_and_demo</th>\n",
       "      <td id=\"T_93fe0_row6_col0\" class=\"data row6 col0\" >0.890000</td>\n",
       "      <td id=\"T_93fe0_row6_col1\" class=\"data row6 col1\" >0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_93fe0_level0_row7\" class=\"row_heading level0 row7\" >XGB_all</th>\n",
       "      <td id=\"T_93fe0_row7_col0\" class=\"data row7 col0\" >0.420000</td>\n",
       "      <td id=\"T_93fe0_row7_col1\" class=\"data row7 col1\" >0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_93fe0_level0_row8\" class=\"row_heading level0 row8\" >XGB_demo_only</th>\n",
       "      <td id=\"T_93fe0_row8_col0\" class=\"data row8 col0\" >0.890000</td>\n",
       "      <td id=\"T_93fe0_row8_col1\" class=\"data row8 col1\" >0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_93fe0_level0_row9\" class=\"row_heading level0 row9\" >XGB_performance_and_demo</th>\n",
       "      <td id=\"T_93fe0_row9_col0\" class=\"data row9 col0\" >0.440000</td>\n",
       "      <td id=\"T_93fe0_row9_col1\" class=\"data row9 col1\" >0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_93fe0_level0_row10\" class=\"row_heading level0 row10\" >XGB_performance_only</th>\n",
       "      <td id=\"T_93fe0_row10_col0\" class=\"data row10 col0\" >0.430000</td>\n",
       "      <td id=\"T_93fe0_row10_col1\" class=\"data row10 col1\" >0.410000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26da4d002b0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = results_subsets[0].keys()\n",
    "metric = \"MSE Test\"\n",
    "\n",
    "subsets_folds_df = pd.DataFrame([pd.DataFrame(results_subsets[fold_num]).loc[metric,models] for fold_num in results_subsets.keys()],index=results_subsets.keys())\n",
    "subsets_mean_df = subsets_folds_df.mean(axis=0)\n",
    "subsets_std_df = subsets_folds_df.std(axis=0)\n",
    "\n",
    "methods = sorted(list(subsets_mean_df.index))[1:]\n",
    "not_tuned = [\"Baseline\"]+methods[::2]\n",
    "tuned = [\"Baseline\"]+methods[1::2]\n",
    "\n",
    "res_df_tune_comp_mean = pd.DataFrame([subsets_mean_df.loc[not_tuned].values,subsets_mean_df.loc[tuned]],index=[\"Untuned\",\"Tuned\"],columns=not_tuned).transpose()\n",
    "res_df_tune_comp_std = pd.DataFrame([subsets_std_df.loc[not_tuned].values,subsets_std_df.loc[tuned]],index=[\"Untuned\",\"Tuned\"],columns=not_tuned).transpose()\n",
    "\n",
    "res_df_tune_comp_mean.round(2).style.highlight_min(color = 'lightgreen', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "094bfac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Untuned</th>\n",
       "      <th>Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>1.0 (0.033)</td>\n",
       "      <td>1.0 (0.033)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_activity_and_demo</th>\n",
       "      <td>0.79 (0.026)</td>\n",
       "      <td>0.92 (0.066)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_all</th>\n",
       "      <td>0.37 (0.022)</td>\n",
       "      <td>0.47 (0.104)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_demo_only</th>\n",
       "      <td>0.79 (0.026)</td>\n",
       "      <td>0.94 (0.092)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_performance_and_demo</th>\n",
       "      <td>0.39 (0.022)</td>\n",
       "      <td>0.44 (0.071)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_performance_only</th>\n",
       "      <td>0.39 (0.024)</td>\n",
       "      <td>0.45 (0.059)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_activity_and_demo</th>\n",
       "      <td>0.89 (0.033)</td>\n",
       "      <td>0.81 (0.028)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_all</th>\n",
       "      <td>0.42 (0.023)</td>\n",
       "      <td>0.4 (0.031)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_demo_only</th>\n",
       "      <td>0.89 (0.033)</td>\n",
       "      <td>0.82 (0.031)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_performance_and_demo</th>\n",
       "      <td>0.44 (0.019)</td>\n",
       "      <td>0.39 (0.022)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_performance_only</th>\n",
       "      <td>0.43 (0.021)</td>\n",
       "      <td>0.41 (0.021)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Untuned         Tuned\n",
       "Baseline                   1.0 (0.033)   1.0 (0.033)\n",
       "LR_activity_and_demo      0.79 (0.026)  0.92 (0.066)\n",
       "LR_all                    0.37 (0.022)  0.47 (0.104)\n",
       "LR_demo_only              0.79 (0.026)  0.94 (0.092)\n",
       "LR_performance_and_demo   0.39 (0.022)  0.44 (0.071)\n",
       "LR_performance_only       0.39 (0.024)  0.45 (0.059)\n",
       "XGB_activity_and_demo     0.89 (0.033)  0.81 (0.028)\n",
       "XGB_all                   0.42 (0.023)   0.4 (0.031)\n",
       "XGB_demo_only             0.89 (0.033)  0.82 (0.031)\n",
       "XGB_performance_and_demo  0.44 (0.019)  0.39 (0.022)\n",
       "XGB_performance_only      0.43 (0.021)  0.41 (0.021)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_df = res_df_tune_comp_mean.round(2).astype(str) + \" (\" +  res_df_tune_comp_std.round(3).astype(str) + \")\"\n",
    "latex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dce79333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lll}\n",
      "\\toprule\n",
      "{} &       Untuned &         Tuned \\\\\n",
      "\\midrule\n",
      "Baseline                 &   1.0 (0.033) &   1.0 (0.033) \\\\\n",
      "LR\\_activity\\_and\\_demo     &  0.79 (0.026) &  0.92 (0.066) \\\\\n",
      "LR\\_all                   &  0.37 (0.022) &  0.47 (0.104) \\\\\n",
      "LR\\_demo\\_only             &  0.79 (0.026) &  0.94 (0.092) \\\\\n",
      "LR\\_performance\\_and\\_demo  &  0.39 (0.022) &  0.44 (0.071) \\\\\n",
      "LR\\_performance\\_only      &  0.39 (0.024) &  0.45 (0.059) \\\\\n",
      "XGB\\_activity\\_and\\_demo    &  0.89 (0.033) &  0.81 (0.028) \\\\\n",
      "XGB\\_all                  &  0.42 (0.023) &   0.4 (0.031) \\\\\n",
      "XGB\\_demo\\_only            &  0.89 (0.033) &  0.82 (0.031) \\\\\n",
      "XGB\\_performance\\_and\\_demo &  0.44 (0.019) &  0.39 (0.022) \\\\\n",
      "XGB\\_performance\\_only     &  0.43 (0.021) &  0.41 (0.021) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_df.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f28d6ab",
   "metadata": {},
   "source": [
    "### Performance Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6cfeb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_feac4_row0_col2, #T_feac4_row0_col4, #T_feac4_row0_col5 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_feac4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_feac4_level0_col0\" class=\"col_heading level0 col0\" >Baseline</th>\n",
       "      <th id=\"T_feac4_level0_col1\" class=\"col_heading level0 col1\" >LR_demo_only_tuned</th>\n",
       "      <th id=\"T_feac4_level0_col2\" class=\"col_heading level0 col2\" >LR_performance_only_tuned</th>\n",
       "      <th id=\"T_feac4_level0_col3\" class=\"col_heading level0 col3\" >LR_activity_and_demo_tuned</th>\n",
       "      <th id=\"T_feac4_level0_col4\" class=\"col_heading level0 col4\" >LR_performance_and_demo_tuned</th>\n",
       "      <th id=\"T_feac4_level0_col5\" class=\"col_heading level0 col5\" >LR_all_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_feac4_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_feac4_row0_col0\" class=\"data row0 col0\" >0.999 (0.033)</td>\n",
       "      <td id=\"T_feac4_row0_col1\" class=\"data row0 col1\" >0.942 (0.092)</td>\n",
       "      <td id=\"T_feac4_row0_col2\" class=\"data row0 col2\" >0.453 (0.059)</td>\n",
       "      <td id=\"T_feac4_row0_col3\" class=\"data row0 col3\" >0.916 (0.066)</td>\n",
       "      <td id=\"T_feac4_row0_col4\" class=\"data row0 col4\" >0.441 (0.071)</td>\n",
       "      <td id=\"T_feac4_row0_col5\" class=\"data row0 col5\" >0.469 (0.104)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26da4ceed90>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For LR\n",
    "models = [\"Baseline\"]+[i for i in results_subsets[0].keys() if (\"tuned\" in i and \"LR\" in i)]\n",
    "metric = \"MSE Test\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "use_df = pd.DataFrame([pd.DataFrame(results_subsets[fold_num]).loc[metric,models] for fold_num in results_subsets.keys()],index=results_subsets.keys())*-1\n",
    "\n",
    "df_mean = pd.DataFrame((-1*use_df).mean(axis=0).round(3).astype(str) + \" (\" + use_df.std(axis=0).round(3).astype(str) + \")\").transpose()\n",
    "model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "\n",
    "best_model = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "t_test_res = np.array([stats.ttest_rel(use_df[best_model].values, use_df[model].values)[1] for model in models]).round(3)\n",
    "t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    \n",
    "res_df_lr = pd.DataFrame([model_dict])\n",
    "\n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_res[i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "res_df_lr.style.apply(negative_bold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07d6d766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_9b75a_row0_col4, #T_9b75a_row0_col5 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_9b75a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_9b75a_level0_col0\" class=\"col_heading level0 col0\" >Baseline</th>\n",
       "      <th id=\"T_9b75a_level0_col1\" class=\"col_heading level0 col1\" >XGB_demo_only_tuned</th>\n",
       "      <th id=\"T_9b75a_level0_col2\" class=\"col_heading level0 col2\" >XGB_performance_only_tuned</th>\n",
       "      <th id=\"T_9b75a_level0_col3\" class=\"col_heading level0 col3\" >XGB_activity_and_demo_tuned</th>\n",
       "      <th id=\"T_9b75a_level0_col4\" class=\"col_heading level0 col4\" >XGB_performance_and_demo_tuned</th>\n",
       "      <th id=\"T_9b75a_level0_col5\" class=\"col_heading level0 col5\" >XGB_all_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9b75a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_9b75a_row0_col0\" class=\"data row0 col0\" >0.999 (0.033)</td>\n",
       "      <td id=\"T_9b75a_row0_col1\" class=\"data row0 col1\" >0.818 (0.031)</td>\n",
       "      <td id=\"T_9b75a_row0_col2\" class=\"data row0 col2\" >0.413 (0.021)</td>\n",
       "      <td id=\"T_9b75a_row0_col3\" class=\"data row0 col3\" >0.812 (0.028)</td>\n",
       "      <td id=\"T_9b75a_row0_col4\" class=\"data row0 col4\" >0.392 (0.022)</td>\n",
       "      <td id=\"T_9b75a_row0_col5\" class=\"data row0 col5\" >0.398 (0.031)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26dc48dda90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For XGB\n",
    "models = [\"Baseline\"]+[i for i in results_subsets[0].keys() if (\"tuned\" in i and \"XGB\" in i)]\n",
    "metric = \"MSE Test\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "use_df = pd.DataFrame([pd.DataFrame(results_subsets[fold_num]).loc[metric,models] for fold_num in results_subsets.keys()],index=results_subsets.keys())*-1\n",
    "\n",
    "df_mean = pd.DataFrame((-1*use_df).mean(axis=0).round(3).astype(str) + \" (\" + use_df.std(axis=0).round(3).astype(str) + \")\").transpose()\n",
    "model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "\n",
    "best_model = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "t_test_res = np.array([stats.ttest_rel(use_df[best_model].values, use_df[model].values)[1] for model in models]).round(3)\n",
    "t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    \n",
    "res_df_xgb = pd.DataFrame([model_dict])\n",
    "\n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_res[i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "res_df_xgb.style.apply(negative_bold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35190c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline</th>\n",
       "      <th>demo_only</th>\n",
       "      <th>performance_only</th>\n",
       "      <th>activity_and_demo</th>\n",
       "      <th>performance_and_demo</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.999 (0.033)</td>\n",
       "      <td>0.942 (0.092)</td>\n",
       "      <td>0.453 (0.059)</td>\n",
       "      <td>0.916 (0.066)</td>\n",
       "      <td>0.441 (0.071)</td>\n",
       "      <td>0.469 (0.104)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>0.999 (0.033)</td>\n",
       "      <td>0.818 (0.031)</td>\n",
       "      <td>0.413 (0.021)</td>\n",
       "      <td>0.812 (0.028)</td>\n",
       "      <td>0.392 (0.022)</td>\n",
       "      <td>0.398 (0.031)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Baseline      demo_only performance_only activity_and_demo  \\\n",
       "LR   0.999 (0.033)  0.942 (0.092)    0.453 (0.059)     0.916 (0.066)   \n",
       "XGB  0.999 (0.033)  0.818 (0.031)    0.413 (0.021)     0.812 (0.028)   \n",
       "\n",
       "    performance_and_demo            all  \n",
       "LR         0.441 (0.071)  0.469 (0.104)  \n",
       "XGB        0.392 (0.022)  0.398 (0.031)  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df_lr.columns = [i[3:-6] if i != \"Baseline\" else \"Baseline\" for i in res_df_lr.columns]    \n",
    "res_df_xgb.columns = [i[4:-6] if i != \"Baseline\" else \"Baseline\" for i in res_df_xgb.columns]    \n",
    "\n",
    "latex_df_subsets = pd.concat([res_df_lr,res_df_xgb],axis=0)\n",
    "latex_df_subsets.index = [\"LR\", \"XGB\"]\n",
    "latex_df_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7d9a0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllll}\n",
      "\\toprule\n",
      "{} &       Baseline &      demo\\_only & performance\\_only & activity\\_and\\_demo & performance\\_and\\_demo &            all \\\\\n",
      "\\midrule\n",
      "LR  &  0.999 (0.033) &  0.942 (0.092) &    0.453 (0.059) &     0.916 (0.066) &        0.441 (0.071) &  0.469 (0.104) \\\\\n",
      "XGB &  0.999 (0.033) &  0.818 (0.031) &    0.413 (0.021) &     0.812 (0.028) &        0.392 (0.022) &  0.398 (0.031) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_df_subsets.round(2).to_latex())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824865af",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2629395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_10_importances = {}\n",
    "\n",
    "# for model in list(results_subsets_feature_importances[fold].keys()):\n",
    "#     imp_df = pd.concat([results_subsets_feature_importances[fold][model] for fold in range(folds)],axis=1)\n",
    "\n",
    "#     if \"LR\" in model:\n",
    "#         direction = imp_df.apply(lambda x: np.sign(x))\n",
    "#         imp_df = imp_df.abs()\n",
    "\n",
    "#     imp_df = imp_df/imp_df.sum(axis=0)\n",
    "\n",
    "#     mean_imp_df = imp_df.mean(axis=1)\n",
    "#     std_imp_df = imp_df.std(axis=1)\n",
    "\n",
    "#     mean_imp_df = mean_imp_df.sort_values(ascending=False)\n",
    "#     std_imp_df = std_imp_df.loc[mean_imp_df.index]\n",
    "#     final_imps = mean_imp_df[:10]\n",
    "#     final_imps[\"Rest\"] = sum(mean_imp_df[10:])\n",
    "#     top_5_importances[model] = np.array([final_imps.index.values, final_imps.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "853ff79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_importances = {}\n",
    "demo_importances_stds = {}\n",
    "\n",
    "for model in list(results_subsets_feature_importances[fold].keys()):\n",
    "    if \"demo\" in model or \"all\" in model:\n",
    "        imp_df_all = pd.concat([results_subsets_feature_importances[fold][model] for fold in range(folds)],axis=1)\n",
    "        \n",
    "        if \"LR\" in model:\n",
    "            direction = imp_df_all.apply(lambda x: np.sign(x))\n",
    "            imp_df_all = imp_df_all.abs()\n",
    "        if imp_df_all.sum().sum()!=0:\n",
    "            imp_df = imp_df_all/imp_df_all.sum(axis=0)\n",
    "        imp_df = imp_df.fillna(1/imp_df.shape[0])\n",
    "#         imp_df = imp_df.loc[demographic_cols]\n",
    "\n",
    "#         mean_imp_df = imp_df.mean(axis=1)\n",
    "#         std_imp_df = imp_df.std(axis=1)\n",
    "\n",
    "#         mean_imp_df = mean_imp_df.sort_values(ascending=False)\n",
    "#         std_imp_df = std_imp_df.loc[mean_imp_df.index]\n",
    "#         final_imps = mean_imp_df#[:10]\n",
    "#         final_imps[\"Rest\"] = sum(mean_imp_df[10:])\n",
    "#         final_imps[\"Total\"] = sum(mean_imp_df)\n",
    "        demo_importances[model] = np.round(np.mean(imp_df.loc[demographic_cols].sum(axis=0)),2)#final_imps.values\n",
    "        demo_importances_stds[model] = np.round(np.std(imp_df.loc[demographic_cols].sum(axis=0)),2)#final_imps.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58eb09bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demo_only</th>\n",
       "      <th>activity_and_demo</th>\n",
       "      <th>performance_and_demo</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>1.0 (0.0)</td>\n",
       "      <td>1.0 (0.0)</td>\n",
       "      <td>0.02 (0.03)</td>\n",
       "      <td>0.02 (0.03)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>1.0 (0.0)</td>\n",
       "      <td>1.0 (0.0)</td>\n",
       "      <td>0.1 (0.09)</td>\n",
       "      <td>0.06 (0.07)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     demo_only activity_and_demo performance_and_demo          all\n",
       "LR   1.0 (0.0)         1.0 (0.0)          0.02 (0.03)  0.02 (0.03)\n",
       "XGB  1.0 (0.0)         1.0 (0.0)           0.1 (0.09)  0.06 (0.07)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_demo_imp = pd.Series({i: demo_importances[i] for i in demo_importances if \"LR\" in i and \"tuned\" in i})\n",
    "xgb_demo_imp = pd.Series({i: demo_importances[i] for i in demo_importances if \"XGB\" in i and \"tuned\" in i})\n",
    "lr_demo_imp.index = [i[3:-6] for i in lr_demo_imp.index]    \n",
    "xgb_demo_imp.index = [i[4:-6] for i in xgb_demo_imp.index]    \n",
    "\n",
    "lr_demo_imp_stds = pd.Series({i: demo_importances_stds[i] for i in demo_importances_stds if \"LR\" in i and \"tuned\" in i})\n",
    "xgb_demo_imp_stds = pd.Series({i: demo_importances_stds[i] for i in demo_importances_stds if \"XGB\" in i and \"tuned\" in i})\n",
    "lr_demo_imp_stds.index = [i[3:-6] for i in lr_demo_imp_stds.index]    \n",
    "xgb_demo_imp_stds.index = [i[4:-6] for i in xgb_demo_imp_stds.index]    \n",
    "\n",
    "\n",
    "latex_df_imp = pd.DataFrame([lr_demo_imp.astype(str) + \" (\" + lr_demo_imp_stds.astype(str) + \")\",\n",
    "                             xgb_demo_imp.astype(str) + \" (\" + xgb_demo_imp_stds.astype(str) + \")\"])\n",
    "latex_df_imp.index = [\"LR\", \"XGB\"]\n",
    "latex_df_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e58a8558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "demo_only                 1.0 (0.0)\n",
       "activity_and_demo         1.0 (0.0)\n",
       "performance_and_demo    0.02 (0.03)\n",
       "all                     0.02 (0.03)\n",
       "dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_demo_imp.astype(str) + \" (\" + lr_demo_imp_stds.astype(str) + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39acdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7996afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} &  demo\\_only &  activity\\_and\\_demo &  performance\\_and\\_demo &   all \\\\\n",
      "\\midrule\n",
      "LR  &        1.0 &                1.0 &                  0.02 &  0.02 \\\\\n",
      "XGB &        1.0 &                1.0 &                  0.10 &  0.06 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_df_imp.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbc5267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3051e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdba567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7749d553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e565d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f3401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc07112e7ae1e8e28a0232207620ff002934c05692de8df42430404c766a0a8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
