{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955f54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils.evaluation import *\n",
    "from utils.utils import *\n",
    "\n",
    "from data import dataset_preprocessing\n",
    "\n",
    "from utils.evaluation import get_metrics\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6dd2461",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"xAPI-Edu-Data\"\n",
    "mode=\"cv\"\n",
    "RS=1\n",
    "hct=10\n",
    "test_ratio=0.2\n",
    "val_ratio=0.1\n",
    "folds=5\n",
    "target = \"categorical\"\n",
    "experiment_name = \"5CV_paper_final\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae51df",
   "metadata": {},
   "source": [
    "### Describe raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21567c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"../data/raw/{dataset_name}/{dataset_name}.csv\",sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baeaba73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_col = \"Class\"\n",
    "demographic_cols = [\"gender\", \"NationalITy\", \"PlaceofBirth\", \"Relation\"]\n",
    "perf_cols = []\n",
    "activity_cols = [\"raisedhands\", \"VisITedResources\", \"AnnouncementsView\", \"Discussion\", 'StudentAbsenceDays']\n",
    "other_cols = ['GradeID', 'ParentAnsweringSurvey', 'ParentschoolSatisfaction', 'SectionID', 'Semester', 'StageID', \"Topic\"]\n",
    "set(df.columns)-set([y_col]+demographic_cols+perf_cols+activity_cols+other_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "529261c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No. of samples</th>\n",
       "      <th>No. of features</th>\n",
       "      <th>Performance features</th>\n",
       "      <th>Demographic features</th>\n",
       "      <th>Activity features</th>\n",
       "      <th>Other features</th>\n",
       "      <th>Categorical features</th>\n",
       "      <th>Total cardinality</th>\n",
       "      <th>% NA</th>\n",
       "      <th>Target $\\textbf{y} \\in$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cortez</th>\n",
       "      <td>480</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1..3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        No. of samples  No. of features  Performance features  \\\n",
       "cortez             480               17                     0   \n",
       "\n",
       "        Demographic features  Activity features  Other features  \\\n",
       "cortez                     4                  5               7   \n",
       "\n",
       "        Categorical features  Total cardinality  % NA Target $\\textbf{y} \\in$  \n",
       "cortez                     7                 59   0.0                  [1..3]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_df_dict = {\"No. of samples\": df.shape[0],\n",
    "           \"No. of features\": df.shape[1],\n",
    "           \"Performance features\": len(perf_cols),\n",
    "           \"Demographic features\": len(demographic_cols),\n",
    "           \"Activity features\": len(activity_cols),\n",
    "           \"Other features\": len(other_cols),\n",
    "           \"Categorical features\": len(df.columns[list(np.logical_and(df.nunique() > 2, df.dtypes == \"object\"))]),     \n",
    "           \"Total cardinality\": df[df.columns[list(np.logical_and(df.nunique() > 2, df.dtypes == \"object\"))]].nunique().sum(),     \n",
    "           \"% NA\": df.isna().sum().sum()/sum(df.shape),\n",
    "           \"Target $\\textbf{y} \\in$\": f\"[1..{df[y_col].nunique()}]\",\n",
    "#            \"High cardinality levels\":  list(df.loc[:,list(df.columns[list(np.logical_and(df.nunique() >= 10, df.dtypes == \"object\"))])].nunique().sort_values().values),\n",
    "          \n",
    "}\n",
    "desc_df = pd.DataFrame([desc_df_dict],index=[\"cortez\"])\n",
    "desc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0e1463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{ll}\n",
      "\\toprule\n",
      "{} &  cortez \\\\\n",
      "\\midrule\n",
      "No. of samples          &     480 \\\\\n",
      "No. of features         &      17 \\\\\n",
      "Performance features    &       0 \\\\\n",
      "Demographic features    &       4 \\\\\n",
      "Activity features       &       5 \\\\\n",
      "Other features          &       7 \\\\\n",
      "Categorical features    &       7 \\\\\n",
      "Total cardinality       &      59 \\\\\n",
      "\\% NA                    &     0.0 \\\\\n",
      "Target \\$\\textbackslash textbf\\{y\\} \\textbackslash in\\$ &  [1..3] \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(desc_df.transpose().to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6390a689",
   "metadata": {},
   "source": [
    "### Preprocessing and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5cc6a91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = f\"{mode}_RS{RS}_hct{hct}\"\n",
    "if mode == \"cv\":\n",
    "    data_path += f\"_{folds}folds\"\n",
    "elif mode == \"train_test\":\n",
    "    data_path += f\"_split{1-test_ratio*100}-{test_ratio*100}\"\n",
    "elif mode == \"train_val_test\":\n",
    "    data_path += f\"_split{round(100-(test_ratio+val_ratio)*100)}-{round(test_ratio*100)}-{round(val_ratio*100)}\"\n",
    "\n",
    "\n",
    "# If no data_dict for the configuration exists, run preprocessing, else load data_dict\n",
    "if not os.path.exists(f\"../data/prepared/{dataset_name}/\"+data_path+\"/data_dict.pickle\"):\n",
    "    dataset_preprocessing.process_dataset(dataset_name, target, mode, RS, hct, test_ratio, val_ratio, folds)\n",
    "with open(f\"../data/prepared/{dataset_name}/{data_path}/data_dict.pickle\", 'rb') as handle:\n",
    "        data_dict = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4a0d47",
   "metadata": {},
   "source": [
    "## Evaluation of categorical data treatment methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db4f3c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\"ignore\", \"ohe\", \"target\", \"ordinal\", \"catboost\", \"glmm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b7cda26",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_rounds = 10\n",
    "max_evals = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a55880a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing results for fold 0, condition=ignore\n",
      "SCORE: 0.8798341003235166                                                                                              \n",
      "SCORE: 0.8783411219628767                                                                                              \n",
      "SCORE: 0.8795992681771894                                                                                              \n",
      "SCORE: 0.878736098145853                                                                                               \n",
      "SCORE: 0.8800708660503507                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.32trial/s, best loss: 0.8783411219628767]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.42084757045798626}\n",
      "Default performance on Test: 1.1234728318618208\n",
      "SCORE: 0.9780210454034544                                                                                              \n",
      "SCORE: 0.964824076542653                                                                                               \n",
      "SCORE: 0.9613315779792403                                                                                              \n",
      "SCORE: 0.9609486172718457                                                                                              \n",
      "SCORE: 1.0806119991756111                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.82trial/s, best loss: 0.9609486172718457]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4637041608932692, 'n_estimators': 331.0}\n",
      "Test Performance after first tuning round: 1.3609888260867482\n",
      "SCORE: 0.9116808737570341                                                                                              \n",
      "SCORE: 0.9108379145275205                                                                                              \n",
      "SCORE: 0.9612547967627052                                                                                              \n",
      "SCORE: 0.9147414290157888                                                                                              \n",
      "SCORE: 0.9372141900228448                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.85trial/s, best loss: 0.9108379145275205]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4637041608932692, 'n_estimators': 331.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 4.0}\n",
      "Test Performance after second tuning round: 1.1669895904903866\n",
      "SCORE: 0.9191519595768514                                                                                              \n",
      "SCORE: 0.9038890630649646                                                                                              \n",
      "SCORE: 0.9259299135879255                                                                                              \n",
      "SCORE: 0.9119764071111647                                                                                              \n",
      "SCORE: 0.9108593052585576                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.94trial/s, best loss: 0.9038890630649646]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4637041608932692, 'n_estimators': 331.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.856849637343861, 'subsample': 0.5349622847501405}\n",
      "Test Performance after third tuning round: 1.0676200008876335\n",
      "SCORE: 0.9453263121889982                                                                                              \n",
      "SCORE: 1.0575146540196336                                                                                              \n",
      "SCORE: 1.035451473945816                                                                                               \n",
      "SCORE: 0.9683880987545015                                                                                              \n",
      "SCORE: 0.9352230793602695                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.01trial/s, best loss: 0.9352230793602695]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4637041608932692, 'n_estimators': 331.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.856849637343861, 'subsample': 0.5349622847501405, 'gamma': 5.903274647146145, 'reg_alpha': 0.0, 'reg_lambda': 2.0571197407496173}\n",
      "Test Performance after last tuning round: 0.9531904304189158\n",
      "Preparing results for fold 0, condition=ohe\n",
      "SCORE: 0.8747993735200188                                                                                              \n",
      "SCORE: 0.8746467400849524                                                                                              \n",
      "SCORE: 0.8644700368674616                                                                                              \n",
      "SCORE: 0.8539858019935422                                                                                              \n",
      "SCORE: 0.8608030818259864                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 17.06trial/s, best loss: 0.8539858019935422]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.19268038191200243}\n",
      "Default performance on Test: 1.1580609041662429\n",
      "SCORE: 0.9253963837791123                                                                                              \n",
      "SCORE: 0.8930366053621256                                                                                              \n",
      "SCORE: 0.8732409566010224                                                                                              \n",
      "SCORE: 0.8919206280411001                                                                                              \n",
      "SCORE: 0.8921050854314286                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.94trial/s, best loss: 0.8732409566010224]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3739805635515384, 'n_estimators': 454.0}\n",
      "Test Performance after first tuning round: 1.7551742688088305\n",
      "SCORE: 0.925087825271912                                                                                               \n",
      "SCORE: 0.9081954133311102                                                                                              \n",
      "SCORE: 0.887201669680533                                                                                               \n",
      "SCORE: 1.0783381546990531                                                                                              \n",
      "SCORE: 0.9375136984895182                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.75trial/s, best loss: 0.887201669680533]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3739805635515384, 'n_estimators': 454.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 4.0}\n",
      "Test Performance after second tuning round: 1.337231355701156\n",
      "SCORE: 0.8596929362804134                                                                                              \n",
      "SCORE: 0.8608905529035248                                                                                              \n",
      "SCORE: 0.8844139668458075                                                                                              \n",
      "SCORE: 0.8646229238122579                                                                                              \n",
      "SCORE: 0.856945866338817                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.00trial/s, best loss: 0.856945866338817]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3739805635515384, 'n_estimators': 454.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.7890896057568629, 'subsample': 0.6516332526271862}\n",
      "Test Performance after third tuning round: 1.3748161063200282\n",
      "SCORE: 1.048764601433804                                                                                               \n",
      "SCORE: 1.0350845483973157                                                                                              \n",
      "SCORE: 1.0116941415168967                                                                                              \n",
      "SCORE: 1.0324158838131863                                                                                              \n",
      "SCORE: 1.0145463993586088                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.37trial/s, best loss: 1.0116941415168967]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3739805635515384, 'n_estimators': 454.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.7890896057568629, 'subsample': 0.6516332526271862, 'gamma': 1.880411153416599, 'reg_alpha': 8.0, 'reg_lambda': 2.271937537788405}\n",
      "Test Performance after last tuning round: 0.9823922912598188\n",
      "Preparing results for fold 0, condition=target\n",
      "SCORE: 0.8417080778505381                                                                                              \n",
      "SCORE: 0.8406674062563221                                                                                              \n",
      "SCORE: 0.8704833944225758                                                                                              \n",
      "SCORE: 0.8406777185302975                                                                                              \n",
      "SCORE: 0.8409511234802475                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.38trial/s, best loss: 0.8406674062563221]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.5379705336715372}\n",
      "Default performance on Test: 1.3305589501596788\n",
      "SCORE: 0.8783340585584897                                                                                              \n",
      "SCORE: 0.9162997749815528                                                                                              \n",
      "SCORE: 0.8998536453638503                                                                                              \n",
      "SCORE: 0.9101052152436496                                                                                              \n",
      "SCORE: 0.8969615854925783                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.27trial/s, best loss: 0.8783340585584897]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.1439242662019434, 'n_estimators': 291.0}\n",
      "Test Performance after first tuning round: 1.4237545459964114\n",
      "SCORE: 0.9627924614301954                                                                                              \n",
      "SCORE: 0.9092567472794038                                                                                              \n",
      "SCORE: 0.9415185719559022                                                                                              \n",
      "SCORE: 0.8819455857457331                                                                                              \n",
      "SCORE: 0.9133396264162992                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.35trial/s, best loss: 0.8819455857457331]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.1439242662019434, 'n_estimators': 291.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 2.0}\n",
      "Test Performance after second tuning round: 1.4187690616118538\n",
      "SCORE: 0.889004566379122                                                                                               \n",
      "SCORE: 0.8909015504301351                                                                                              \n",
      "SCORE: 0.8837390914143594                                                                                              \n",
      "SCORE: 0.865338671969979                                                                                               \n",
      "SCORE: 0.8726517887094163                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.39trial/s, best loss: 0.865338671969979]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.1439242662019434, 'n_estimators': 291.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.8254187281012426, 'subsample': 0.7707497909191248}\n",
      "Test Performance after third tuning round: 1.310927122947534\n",
      "SCORE: 0.9621058178508057                                                                                              \n",
      "SCORE: 1.0660825373154033                                                                                              \n",
      "SCORE: 0.9305964876870888                                                                                              \n",
      "SCORE: 0.9424756343592955                                                                                              \n",
      "SCORE: 0.9958002815677167                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.59trial/s, best loss: 0.9305964876870888]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.1439242662019434, 'n_estimators': 291.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.8254187281012426, 'subsample': 0.7707497909191248, 'gamma': 2.254135293261546, 'reg_alpha': 2.0, 'reg_lambda': 3.5473352522291624}\n",
      "Test Performance after last tuning round: 0.9558777870762515\n",
      "Preparing results for fold 0, condition=ordinal\n",
      "SCORE: 0.8945560874509159                                                                                              \n",
      "SCORE: 0.8902837042037991                                                                                              \n",
      "SCORE: 0.8918737049748111                                                                                              \n",
      "SCORE: 0.8879680988074667                                                                                              \n",
      "SCORE: 0.8914808267967214                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.08trial/s, best loss: 0.8879680988074667]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.4037658372500901}\n",
      "Default performance on Test: 1.2290040420796886\n",
      "SCORE: 0.9344759655473936                                                                                              \n",
      "SCORE: 0.9019455956315576                                                                                              \n",
      "SCORE: 0.9272126087274352                                                                                              \n",
      "SCORE: 0.9203064451299486                                                                                              \n",
      "SCORE: 0.9044311331380527                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.90trial/s, best loss: 0.9019455956315576]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4525452424385113, 'n_estimators': 397.0}\n",
      "Test Performance after first tuning round: 1.8275372376568135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.9108753814782297                                                                                              \n",
      "SCORE: 0.9151152259593136                                                                                              \n",
      "SCORE: 0.8692311553803732                                                                                              \n",
      "SCORE: 0.8780864245524848                                                                                              \n",
      "SCORE: 0.9215494196400966                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.51trial/s, best loss: 0.8692311553803732]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4525452424385113, 'n_estimators': 397.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 1.483721996057133\n",
      "SCORE: 0.8841513268782064                                                                                              \n",
      "SCORE: 0.8630717363344573                                                                                              \n",
      "SCORE: 0.8577836470744693                                                                                              \n",
      "SCORE: 0.8636660928889416                                                                                              \n",
      "SCORE: 0.876612221503615                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.35trial/s, best loss: 0.8577836470744693]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4525452424385113, 'n_estimators': 397.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.7800831856469055, 'subsample': 0.807092821607162}\n",
      "Test Performance after third tuning round: 1.3218833523969802\n",
      "SCORE: 0.8899647673862546                                                                                              \n",
      "SCORE: 1.022948004066418                                                                                               \n",
      "SCORE: 0.8811284546833763                                                                                              \n",
      "SCORE: 1.0035838226564162                                                                                              \n",
      "SCORE: 0.9597702603705187                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.32trial/s, best loss: 0.8811284546833763]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4525452424385113, 'n_estimators': 397.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.7800831856469055, 'subsample': 0.807092821607162, 'gamma': 1.7984064028974245, 'reg_alpha': 2.0, 'reg_lambda': 3.990889680737999}\n",
      "Test Performance after last tuning round: 0.9508673532284097\n",
      "Preparing results for fold 0, condition=catboost\n",
      "SCORE: 0.8618667786546679                                                                                              \n",
      "SCORE: 0.8610223787023799                                                                                              \n",
      "SCORE: 0.8608116368717109                                                                                              \n",
      "SCORE: 0.8791023124504314                                                                                              \n",
      "SCORE: 0.8613307524788556                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 16.12trial/s, best loss: 0.8608116368717109]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.4530843798335997}\n",
      "Default performance on Test: 1.4585411502296612\n",
      "SCORE: 0.977926804276767                                                                                               \n",
      "SCORE: 1.020035398693242                                                                                               \n",
      "SCORE: 1.006595584132485                                                                                               \n",
      "SCORE: 0.9857457135330492                                                                                              \n",
      "SCORE: 1.0155223639594977                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.80trial/s, best loss: 0.977926804276767]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4093405674755602, 'n_estimators': 274.0}\n",
      "Test Performance after first tuning round: 1.6898800399648195\n",
      "SCORE: 1.1087829149040966                                                                                              \n",
      "SCORE: 0.9766661354644175                                                                                              \n",
      "SCORE: 1.0080755153969418                                                                                              \n",
      "SCORE: 1.1301222413647607                                                                                              \n",
      "SCORE: 0.925974885516933                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.81trial/s, best loss: 0.925974885516933]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4093405674755602, 'n_estimators': 274.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 1.3330447998215458\n",
      "SCORE: 0.9276517293978059                                                                                              \n",
      "SCORE: 0.9265245111710698                                                                                              \n",
      "SCORE: 0.9097002245549601                                                                                              \n",
      "SCORE: 0.9066508822343453                                                                                              \n",
      "SCORE: 0.9063955290481711                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.48trial/s, best loss: 0.9063955290481711]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4093405674755602, 'n_estimators': 274.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.768572762105217, 'subsample': 0.660929107254508}\n",
      "Test Performance after third tuning round: 1.2655603534694526\n",
      "SCORE: 1.0404944430288305                                                                                              \n",
      "SCORE: 0.9265085226654477                                                                                              \n",
      "SCORE: 1.0434013473710708                                                                                              \n",
      "SCORE: 1.0234223991896838                                                                                              \n",
      "SCORE: 1.054305382734724                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.13trial/s, best loss: 0.9265085226654477]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4093405674755602, 'n_estimators': 274.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.768572762105217, 'subsample': 0.660929107254508, 'gamma': 3.9315232277623227, 'reg_alpha': 1.0, 'reg_lambda': 2.3949163523816233}\n",
      "Test Performance after last tuning round: 0.9451381490576237\n",
      "Preparing results for fold 0, condition=glmm\n",
      "SCORE: 0.8521623727739991                                                                                              \n",
      "SCORE: 0.8509693167087766                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.8626258437485405                                                                                              \n",
      "SCORE: 0.8505568062540055                                                                                              \n",
      "SCORE: 0.8590646995580512                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.01trial/s, best loss: 0.8505568062540055]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.8032610235731569}\n",
      "Default performance on Test: 1.6843542987805684\n",
      "SCORE: 0.9308800616842079                                                                                              \n",
      "SCORE: 0.9517452421072633                                                                                              \n",
      "SCORE: 0.9913711298730569                                                                                              \n",
      "SCORE: 0.9580707447428136                                                                                              \n",
      "SCORE: 0.9926382559671916                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.74trial/s, best loss: 0.9308800616842079]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3490576269377649, 'n_estimators': 99.0}\n",
      "Test Performance after first tuning round: 1.7710911173286992\n",
      "SCORE: 0.9242537907306309                                                                                              \n",
      "SCORE: 0.8861481203393152                                                                                              \n",
      "SCORE: 0.9096605942988978                                                                                              \n",
      "SCORE: 0.9357716772214703                                                                                              \n",
      "SCORE: 0.942480319087038                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.93trial/s, best loss: 0.8861481203393152]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3490576269377649, 'n_estimators': 99.0, 'seed': 0, 'max_depth': 18.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 1.419443323851769\n",
      "SCORE: 0.906082917129074                                                                                               \n",
      "SCORE: 0.9054056628056639                                                                                              \n",
      "SCORE: 0.9110397504984175                                                                                              \n",
      "SCORE: 0.8961550121556339                                                                                              \n",
      "SCORE: 0.9031519506587431                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.98trial/s, best loss: 0.8961550121556339]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3490576269377649, 'n_estimators': 99.0, 'seed': 0, 'max_depth': 18.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.7099849837047385, 'subsample': 0.8499253552980192}\n",
      "Test Performance after third tuning round: 1.313470439575211\n",
      "SCORE: 0.8696019129780754                                                                                              \n",
      "SCORE: 0.9363057347793091                                                                                              \n",
      "SCORE: 0.9707401483784894                                                                                              \n",
      "SCORE: 0.8795645072313573                                                                                              \n",
      "SCORE: 1.0447054201753012                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.26trial/s, best loss: 0.8696019129780754]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3490576269377649, 'n_estimators': 99.0, 'seed': 0, 'max_depth': 18.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.7099849837047385, 'subsample': 0.8499253552980192, 'gamma': 0.382419220703474, 'reg_alpha': 2.0, 'reg_lambda': 1.1604610461196314}\n",
      "Test Performance after last tuning round: 1.0961046190504422\n",
      "Preparing results for fold 1, condition=ignore\n",
      "SCORE: 0.9016413878184902                                                                                              \n",
      "SCORE: 0.9005087650167847                                                                                              \n",
      "SCORE: 0.9021469123653677                                                                                              \n",
      "SCORE: 0.901593339156633                                                                                               \n",
      "SCORE: 0.9007237754053629                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 26.08trial/s, best loss: 0.9005087650167847]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.600740981424724}\n",
      "Default performance on Test: 1.1153459281485825\n",
      "SCORE: 0.9357137218638865                                                                                              \n",
      "SCORE: 0.9608043823499385                                                                                              \n",
      "SCORE: 0.9488294371603718                                                                                              \n",
      "SCORE: 0.9711495788903216                                                                                              \n",
      "SCORE: 0.931584559809034                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.30trial/s, best loss: 0.931584559809034]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.38198733495557513, 'n_estimators': 435.0}\n",
      "Test Performance after first tuning round: 1.3738586110489968\n",
      "SCORE: 0.9490154061298011                                                                                              \n",
      "SCORE: 0.9627976589833095                                                                                              \n",
      "SCORE: 0.926531311041348                                                                                               \n",
      "SCORE: 1.00021066749435                                                                                                \n",
      "SCORE: 0.9557168287172807                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.83trial/s, best loss: 0.926531311041348]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.38198733495557513, 'n_estimators': 435.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 8.0}\n",
      "Test Performance after second tuning round: 0.8974169479915858\n",
      "SCORE: 0.9231466686585621                                                                                              \n",
      "SCORE: 0.9178056395924244                                                                                              \n",
      "SCORE: 0.9249771313705448                                                                                              \n",
      "SCORE: 0.9195502681150993                                                                                              \n",
      "SCORE: 0.9244596086786178                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.09trial/s, best loss: 0.9178056395924244]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.38198733495557513, 'n_estimators': 435.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.8902879397346706, 'subsample': 0.6138428844173514}\n",
      "Test Performance after third tuning round: 0.9254777124243064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.9807192879600727                                                                                              \n",
      "SCORE: 1.0152803819409257                                                                                              \n",
      "SCORE: 1.0301589735719108                                                                                              \n",
      "SCORE: 0.9654007492590331                                                                                              \n",
      "SCORE: 1.0295770785822747                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.99trial/s, best loss: 0.9654007492590331]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.38198733495557513, 'n_estimators': 435.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.8902879397346706, 'subsample': 0.6138428844173514, 'gamma': 0.7990362995007777, 'reg_alpha': 5.0, 'reg_lambda': 3.1944575290976975}\n",
      "Test Performance after last tuning round: 0.84670343361278\n",
      "Preparing results for fold 1, condition=ohe\n",
      "SCORE: 0.8654449664310695                                                                                              \n",
      "SCORE: 0.9477134814577252                                                                                              \n",
      "SCORE: 0.8655911821482147                                                                                              \n",
      "SCORE: 0.8666538365151796                                                                                              \n",
      "SCORE: 0.8688255800607682                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 18.18trial/s, best loss: 0.8654449664310695]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.4155721404104319}\n",
      "Default performance on Test: 0.9992599361112057\n",
      "SCORE: 0.9389350872500579                                                                                              \n",
      "SCORE: 1.0269591882011078                                                                                              \n",
      "SCORE: 0.903970848616923                                                                                               \n",
      "SCORE: 0.9761290263685849                                                                                              \n",
      "SCORE: 0.9580671657998512                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.11trial/s, best loss: 0.903970848616923]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.44704677859830194, 'n_estimators': 182.0}\n",
      "Test Performance after first tuning round: 1.2900704236751672\n",
      "SCORE: 0.9470784753068757                                                                                              \n",
      "SCORE: 0.8780173024621322                                                                                              \n",
      "SCORE: 0.9887422801575376                                                                                              \n",
      "SCORE: 0.9442612507349096                                                                                              \n",
      "SCORE: 0.9491618123657982                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.05trial/s, best loss: 0.8780173024621322]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.44704677859830194, 'n_estimators': 182.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 0.0}\n",
      "Test Performance after second tuning round: 1.2805987842020203\n",
      "SCORE: 0.8968466365196365                                                                                              \n",
      "SCORE: 0.8830400343571713                                                                                              \n",
      "SCORE: 0.8327472408866609                                                                                              \n",
      "SCORE: 0.9150924568958707                                                                                              \n",
      "SCORE: 0.932541947486129                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.28trial/s, best loss: 0.8327472408866609]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.44704677859830194, 'n_estimators': 182.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 0.0, 'colsample_bytree': 0.6952544033186969, 'subsample': 0.9801836525819756}\n",
      "Test Performance after third tuning round: 1.313805539714653\n",
      "SCORE: 0.9577918431074546                                                                                              \n",
      "SCORE: 1.0538277914703233                                                                                              \n",
      "SCORE: 0.9571102826571606                                                                                              \n",
      "SCORE: 0.9329658573999247                                                                                              \n",
      "SCORE: 1.0281708655422044                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.36trial/s, best loss: 0.9329658573999247]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.44704677859830194, 'n_estimators': 182.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 0.0, 'colsample_bytree': 0.6952544033186969, 'subsample': 0.9801836525819756, 'gamma': 2.992705818161788, 'reg_alpha': 3.0, 'reg_lambda': 1.3491246771579473}\n",
      "Test Performance after last tuning round: 0.861920010553022\n",
      "Preparing results for fold 1, condition=target\n",
      "SCORE: 0.8724614916937915                                                                                              \n",
      "SCORE: 0.8650485473810271                                                                                              \n",
      "SCORE: 0.8650002686511966                                                                                              \n",
      "SCORE: 0.8710429721397619                                                                                              \n",
      "SCORE: 0.8792895678385431                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 15.25trial/s, best loss: 0.8650002686511966]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.7898645564934926}\n",
      "Default performance on Test: 1.1075915839938173\n",
      "SCORE: 0.8926088803252122                                                                                              \n",
      "SCORE: 0.8966100766720573                                                                                              \n",
      "SCORE: 0.883934823855267                                                                                               \n",
      "SCORE: 0.8786889699229748                                                                                              \n",
      "SCORE: 0.900998844336721                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.02trial/s, best loss: 0.8786889699229748]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4467224431775137, 'n_estimators': 294.0}\n",
      "Test Performance after first tuning round: 1.572462380163633\n",
      "SCORE: 0.9073699545875739                                                                                              \n",
      "SCORE: 0.9045964223818432                                                                                              \n",
      "SCORE: 0.8783546811795672                                                                                              \n",
      "SCORE: 0.9039044407849852                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 1.1237539280396183                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.29trial/s, best loss: 0.8783546811795672]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4467224431775137, 'n_estimators': 294.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 8.0}\n",
      "Test Performance after second tuning round: 1.1944304671784396\n",
      "SCORE: 0.8916148344658161                                                                                              \n",
      "SCORE: 0.885453886495417                                                                                               \n",
      "SCORE: 0.9081556001681141                                                                                              \n",
      "SCORE: 0.8972128934595679                                                                                              \n",
      "SCORE: 0.8824161571464678                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.19trial/s, best loss: 0.8824161571464678]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4467224431775137, 'n_estimators': 294.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.9144406756663718, 'subsample': 0.6151076686493809}\n",
      "Test Performance after third tuning round: 1.060445165006671\n",
      "SCORE: 1.0402185453428054                                                                                              \n",
      "SCORE: 0.9969188954236514                                                                                              \n",
      "SCORE: 1.0104510798242468                                                                                              \n",
      "SCORE: 1.0538438018829717                                                                                              \n",
      "SCORE: 1.044370788538963                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.53trial/s, best loss: 0.9969188954236514]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4467224431775137, 'n_estimators': 294.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.9144406756663718, 'subsample': 0.6151076686493809, 'gamma': 2.567738248240815, 'reg_alpha': 4.0, 'reg_lambda': 3.531892630534251}\n",
      "Test Performance after last tuning round: 0.8596432352535134\n",
      "Preparing results for fold 1, condition=ordinal\n",
      "SCORE: 0.9274307117420946                                                                                              \n",
      "SCORE: 0.9115601297529432                                                                                              \n",
      "SCORE: 0.908977431012012                                                                                               \n",
      "SCORE: 0.9113238800101708                                                                                              \n",
      "SCORE: 0.9077384262300277                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.16trial/s, best loss: 0.9077384262300277]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.41805915135176985}\n",
      "Default performance on Test: 1.1005223790133452\n",
      "SCORE: 0.9511440645714888                                                                                              \n",
      "SCORE: 0.90415005784933                                                                                                \n",
      "SCORE: 0.9484409660480118                                                                                              \n",
      "SCORE: 0.9228375468105078                                                                                              \n",
      "SCORE: 0.9107343490114257                                                                                              \n",
      "100%|████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.29trial/s, best loss: 0.90415005784933]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4687712219585255, 'n_estimators': 497.0}\n",
      "Test Performance after first tuning round: 1.7175568413631446\n",
      "SCORE: 0.8895610508102294                                                                                              \n",
      "SCORE: 0.876776722009391                                                                                               \n",
      "SCORE: 0.8822116472280335                                                                                              \n",
      "SCORE: 1.0122999761652927                                                                                              \n",
      "SCORE: 0.9301307621088218                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.99trial/s, best loss: 0.876776722009391]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4687712219585255, 'n_estimators': 497.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 4.0}\n",
      "Test Performance after second tuning round: 0.9733373317115285\n",
      "SCORE: 0.893315020037645                                                                                               \n",
      "SCORE: 0.8763669970164653                                                                                              \n",
      "SCORE: 0.8853713402461677                                                                                              \n",
      "SCORE: 0.894201744071465                                                                                               \n",
      "SCORE: 0.8603055768234196                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.04trial/s, best loss: 0.8603055768234196]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4687712219585255, 'n_estimators': 497.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.903992672535388, 'subsample': 0.886519125242515}\n",
      "Test Performance after third tuning round: 1.1460252866940837\n",
      "SCORE: 1.0313992615007668                                                                                              \n",
      "SCORE: 0.9836372651431157                                                                                              \n",
      "SCORE: 1.0423550665140797                                                                                              \n",
      "SCORE: 1.004463295875517                                                                                               \n",
      "SCORE: 0.978489605850126                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.36trial/s, best loss: 0.978489605850126]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4687712219585255, 'n_estimators': 497.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.903992672535388, 'subsample': 0.886519125242515, 'gamma': 2.4803487316824206, 'reg_alpha': 6.0, 'reg_lambda': 2.215902452170181}\n",
      "Test Performance after last tuning round: 0.8829563654016774\n",
      "Preparing results for fold 1, condition=catboost\n",
      "SCORE: 0.9000407016344608                                                                                              \n",
      "SCORE: 0.9027200384189877                                                                                              \n",
      "SCORE: 0.9000695214574318                                                                                              \n",
      "SCORE: 0.8999374754093739                                                                                              \n",
      "SCORE: 0.9033026241506225                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 15.67trial/s, best loss: 0.8999374754093739]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.394909808580168}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default performance on Test: 1.489482734850217\n",
      "SCORE: 0.9420023555707662                                                                                              \n",
      "SCORE: 1.020354367655274                                                                                               \n",
      "SCORE: 0.9524906970595651                                                                                              \n",
      "SCORE: 1.0054881554802382                                                                                              \n",
      "SCORE: 1.0059818139148002                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.57trial/s, best loss: 0.9420023555707662]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.07118446676738699, 'n_estimators': 336.0}\n",
      "Test Performance after first tuning round: 1.4462703991661099\n",
      "SCORE: 0.9585874786285178                                                                                              \n",
      "SCORE: 0.9280305034172939                                                                                              \n",
      "SCORE: 0.9560211623628877                                                                                              \n",
      "SCORE: 0.9442313357139763                                                                                              \n",
      "SCORE: 0.963833372641357                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.94trial/s, best loss: 0.9280305034172939]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.07118446676738699, 'n_estimators': 336.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 1.2121006822779015\n",
      "SCORE: 0.9577471009496199                                                                                              \n",
      "SCORE: 0.9889747923257884                                                                                              \n",
      "SCORE: 1.034704663229629                                                                                               \n",
      "SCORE: 1.00435598857917                                                                                                \n",
      "SCORE: 1.0250802648425121                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.38trial/s, best loss: 0.9577471009496199]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.07118446676738699, 'n_estimators': 336.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.8266011377821871, 'subsample': 0.503632077987133}\n",
      "Test Performance after third tuning round: 0.9904259656026341\n",
      "SCORE: 1.0864117098442645                                                                                              \n",
      "SCORE: 1.0772076149723324                                                                                              \n",
      "SCORE: 1.0764575301593284                                                                                              \n",
      "SCORE: 1.0642466189355448                                                                                              \n",
      "SCORE: 1.0839194986943848                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.69trial/s, best loss: 1.0642466189355448]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.07118446676738699, 'n_estimators': 336.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.8266011377821871, 'subsample': 0.503632077987133, 'gamma': 8.309661922258574, 'reg_alpha': 2.0, 'reg_lambda': 1.2632299697077978}\n",
      "Test Performance after last tuning round: 0.960435815178997\n",
      "Preparing results for fold 1, condition=glmm\n",
      "SCORE: 0.8670279479227485                                                                                              \n",
      "SCORE: 0.8828579919714501                                                                                              \n",
      "SCORE: 0.9642833445725975                                                                                              \n",
      "SCORE: 0.916120763228613                                                                                               \n",
      "SCORE: 0.8646737346175879                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 17.43trial/s, best loss: 0.8646737346175879]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.40406644540850273}\n",
      "Default performance on Test: 1.222174169001579\n",
      "SCORE: 0.9930234538953859                                                                                              \n",
      "SCORE: 1.0560175697529801                                                                                              \n",
      "SCORE: 1.0382745904956026                                                                                              \n",
      "SCORE: 1.0234113362763226                                                                                              \n",
      "SCORE: 1.0737141608889056                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.74trial/s, best loss: 0.9930234538953859]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.22050324793450163, 'n_estimators': 423.0}\n",
      "Test Performance after first tuning round: 1.3400383644262923\n",
      "SCORE: 1.0438564782073898                                                                                              \n",
      "SCORE: 0.9851014282037329                                                                                              \n",
      "SCORE: 0.9702170290584776                                                                                              \n",
      "SCORE: 0.97642838451267                                                                                                \n",
      "SCORE: 0.9987648668170885                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.15trial/s, best loss: 0.9702170290584776]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.22050324793450163, 'n_estimators': 423.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 1.2303249568394274\n",
      "SCORE: 0.9243448041805371                                                                                              \n",
      "SCORE: 0.9452021316512174                                                                                              \n",
      "SCORE: 0.9424145057549671                                                                                              \n",
      "SCORE: 0.9445085730105752                                                                                              \n",
      "SCORE: 0.9474307492112748                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.18trial/s, best loss: 0.9243448041805371]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.22050324793450163, 'n_estimators': 423.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.560081285128315, 'subsample': 0.783211822897376}\n",
      "Test Performance after third tuning round: 1.2408892122450512\n",
      "SCORE: 0.9703229313884927                                                                                              \n",
      "SCORE: 0.9260463357250053                                                                                              \n",
      "SCORE: 1.035049096715016                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.9299465599002789                                                                                              \n",
      "SCORE: 0.9277801076353966                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.09trial/s, best loss: 0.9260463357250053]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.22050324793450163, 'n_estimators': 423.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.560081285128315, 'subsample': 0.783211822897376, 'gamma': 0.9631493794002354, 'reg_alpha': 4.0, 'reg_lambda': 2.5684841370233342}\n",
      "Test Performance after last tuning round: 0.8282233893070948\n",
      "Preparing results for fold 2, condition=ignore\n",
      "SCORE: 0.8834430563892981                                                                                              \n",
      "SCORE: 0.8852208895411966                                                                                              \n",
      "SCORE: 0.8821164790365901                                                                                              \n",
      "SCORE: 0.883135610757304                                                                                               \n",
      "SCORE: 0.8822149586072994                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 16.57trial/s, best loss: 0.8821164790365901]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.3231739914920591}\n",
      "Default performance on Test: 1.3796053895432105\n",
      "SCORE: 0.9547185695185595                                                                                              \n",
      "SCORE: 0.9374519078059068                                                                                              \n",
      "SCORE: 0.9437498493896885                                                                                              \n",
      "SCORE: 0.9634605177982511                                                                                              \n",
      "SCORE: 1.0111474749609095                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.59trial/s, best loss: 0.9374519078059068]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.47612607732230394, 'n_estimators': 98.0}\n",
      "Test Performance after first tuning round: 1.484715676134474\n",
      "SCORE: 0.885207445640215                                                                                               \n",
      "SCORE: 0.9071351695975152                                                                                              \n",
      "SCORE: 0.9133412330767069                                                                                              \n",
      "SCORE: 0.9177939546389566                                                                                              \n",
      "SCORE: 0.8972575700733676                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.01trial/s, best loss: 0.885207445640215]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.47612607732230394, 'n_estimators': 98.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 0.9266112596085282\n",
      "SCORE: 0.8831242432520746                                                                                              \n",
      "SCORE: 0.9016819073291711                                                                                              \n",
      "SCORE: 0.8777478493082841                                                                                              \n",
      "SCORE: 0.8844480302655008                                                                                              \n",
      "SCORE: 0.8931621170289968                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.81trial/s, best loss: 0.8777478493082841]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.47612607732230394, 'n_estimators': 98.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.8884440209309112, 'subsample': 0.5231850303510748}\n",
      "Test Performance after third tuning round: 0.9407465991074834\n",
      "SCORE: 1.083902580979583                                                                                               \n",
      "SCORE: 1.007857813164597                                                                                               \n",
      "SCORE: 1.0472565168824173                                                                                              \n",
      "SCORE: 1.0110009052915103                                                                                              \n",
      "SCORE: 0.9465287862997865                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.68trial/s, best loss: 0.9465287862997865]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.47612607732230394, 'n_estimators': 98.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.8884440209309112, 'subsample': 0.5231850303510748, 'gamma': 0.1830119430680972, 'reg_alpha': 4.0, 'reg_lambda': 1.18982044366838}\n",
      "Test Performance after last tuning round: 0.9104986471908186\n",
      "Preparing results for fold 2, condition=ohe\n",
      "SCORE: 0.9490449565305527                                                                                              \n",
      "SCORE: 0.9644936740567897                                                                                              \n",
      "SCORE: 0.8893402804402415                                                                                              \n",
      "SCORE: 0.8839421662332201                                                                                              \n",
      "SCORE: 0.8736864887434533                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.62trial/s, best loss: 0.8736864887434533]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.3254345994673464}\n",
      "Default performance on Test: 1.0989615130205996\n",
      "SCORE: 0.8771582261970915                                                                                              \n",
      "SCORE: 0.8301800716034752                                                                                              \n",
      "SCORE: 0.959024498006052                                                                                               \n",
      "SCORE: 0.8935838394883128                                                                                              \n",
      "SCORE: 0.8659653650158008                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.63trial/s, best loss: 0.8301800716034752]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3304679921219226, 'n_estimators': 143.0}\n",
      "Test Performance after first tuning round: 1.2026242965500167\n",
      "SCORE: 0.8722668622395394                                                                                              \n",
      "SCORE: 0.850841832626198                                                                                               \n",
      "SCORE: 0.850841832626198                                                                                               \n",
      "SCORE: 0.8639921097725616                                                                                              \n",
      "SCORE: 0.9551275607478426                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.47trial/s, best loss: 0.850841832626198]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3304679921219226, 'n_estimators': 143.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 1.1613610283108915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.845144668126909                                                                                               \n",
      "SCORE: 0.8442993448077225                                                                                              \n",
      "SCORE: 0.868506156587791                                                                                               \n",
      "SCORE: 0.8307472861207778                                                                                              \n",
      "SCORE: 0.8880169765071851                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.05trial/s, best loss: 0.8307472861207778]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3304679921219226, 'n_estimators': 143.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.8786954953423174, 'subsample': 0.6690114903066742}\n",
      "Test Performance after third tuning round: 1.0682740590681303\n",
      "SCORE: 1.0423515072246146                                                                                              \n",
      "SCORE: 1.0402496532071688                                                                                              \n",
      "SCORE: 0.9638178915743081                                                                                              \n",
      "SCORE: 1.0560769593004462                                                                                              \n",
      "SCORE: 0.8812365169298548                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.19trial/s, best loss: 0.8812365169298548]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3304679921219226, 'n_estimators': 143.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.8786954953423174, 'subsample': 0.6690114903066742, 'gamma': 0.014175880342821183, 'reg_alpha': 6.0, 'reg_lambda': 3.5362484692890854}\n",
      "Test Performance after last tuning round: 0.8554731459778107\n",
      "Preparing results for fold 2, condition=target\n",
      "SCORE: 0.8612750457793533                                                                                              \n",
      "SCORE: 0.9202062339149306                                                                                              \n",
      "SCORE: 0.8640641380393065                                                                                              \n",
      "SCORE: 0.8627223511019603                                                                                              \n",
      "SCORE: 0.861194183266527                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.58trial/s, best loss: 0.861194183266527]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.2939273051277792}\n",
      "Default performance on Test: 1.0971239156176569\n",
      "SCORE: 0.884412752299704                                                                                               \n",
      "SCORE: 0.8881480762396088                                                                                              \n",
      "SCORE: 0.8838696974442118                                                                                              \n",
      "SCORE: 1.0221806894861916                                                                                              \n",
      "SCORE: 0.8947313858936351                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.63trial/s, best loss: 0.8838696974442118]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.2493273286685618, 'n_estimators': 403.0}\n",
      "Test Performance after first tuning round: 1.2836917362604026\n",
      "SCORE: 0.8542187709075805                                                                                              \n",
      "SCORE: 0.8607642626945513                                                                                              \n",
      "SCORE: 0.855438134266352                                                                                               \n",
      "SCORE: 0.8715538126774393                                                                                              \n",
      "SCORE: 0.855438134266352                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.21trial/s, best loss: 0.8542187709075805]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.2493273286685618, 'n_estimators': 403.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 1.1216151045714957\n",
      "SCORE: 0.9070733730630487                                                                                              \n",
      "SCORE: 0.864091521222434                                                                                               \n",
      "SCORE: 0.8960520773233103                                                                                              \n",
      "SCORE: 0.873091843988111                                                                                               \n",
      "SCORE: 0.8670376140536297                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.65trial/s, best loss: 0.864091521222434]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.2493273286685618, 'n_estimators': 403.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.7479270950053474, 'subsample': 0.9646426554792586}\n",
      "Test Performance after third tuning round: 1.1257305563555604\n",
      "SCORE: 0.9705442529363101                                                                                              \n",
      "SCORE: 1.060861789406185                                                                                               \n",
      "SCORE: 1.0248764645130493                                                                                              \n",
      "SCORE: 0.983204651925571                                                                                               \n",
      "SCORE: 0.99072741055496                                                                                                \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.38trial/s, best loss: 0.9705442529363101]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.2493273286685618, 'n_estimators': 403.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.7479270950053474, 'subsample': 0.9646426554792586, 'gamma': 3.694235320188683, 'reg_alpha': 4.0, 'reg_lambda': 1.981636954588328}\n",
      "Test Performance after last tuning round: 0.9194161043823142\n",
      "Preparing results for fold 2, condition=ordinal\n",
      "SCORE: 0.9041853672819705                                                                                              \n",
      "SCORE: 0.9058741542817499                                                                                              \n",
      "SCORE: 0.9041088363165416                                                                                              \n",
      "SCORE: 0.9058022066852006                                                                                              \n",
      "SCORE: 0.9019889493096942                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.12trial/s, best loss: 0.9019889493096942]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.1860890311830593}\n",
      "Default performance on Test: 1.1211933110908479\n",
      "SCORE: 0.989426152202519                                                                                               \n",
      "SCORE: 0.9667798008105535                                                                                              \n",
      "SCORE: 1.0690800805022787                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.9317663100029557                                                                                              \n",
      "SCORE: 0.9104088588030675                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.59trial/s, best loss: 0.9104088588030675]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.22820772086317553, 'n_estimators': 498.0}\n",
      "Test Performance after first tuning round: 1.3857443105479852\n",
      "SCORE: 0.8452834650122252                                                                                              \n",
      "SCORE: 0.8889706313052343                                                                                              \n",
      "SCORE: 0.8330490957290575                                                                                              \n",
      "SCORE: 1.0562719467518504                                                                                              \n",
      "SCORE: 0.9641321417546465                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.56trial/s, best loss: 0.8330490957290575]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.22820772086317553, 'n_estimators': 498.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 1.107612832256666\n",
      "SCORE: 0.8903810535384311                                                                                              \n",
      "SCORE: 0.9109339309976846                                                                                              \n",
      "SCORE: 0.9024301912314565                                                                                              \n",
      "SCORE: 0.8584915870986908                                                                                              \n",
      "SCORE: 0.8753205866578379                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.67trial/s, best loss: 0.8584915870986908]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.22820772086317553, 'n_estimators': 498.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.9321653726972343, 'subsample': 0.5991260550412445}\n",
      "Test Performance after third tuning round: 1.097624460518472\n",
      "SCORE: 1.0546231052157202                                                                                              \n",
      "SCORE: 1.0585564512201961                                                                                              \n",
      "SCORE: 1.0372128660960278                                                                                              \n",
      "SCORE: 0.9365363929954789                                                                                              \n",
      "SCORE: 1.0307278067719419                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.64trial/s, best loss: 0.9365363929954789]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.22820772086317553, 'n_estimators': 498.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.9321653726972343, 'subsample': 0.5991260550412445, 'gamma': 2.1532846565011132, 'reg_alpha': 1.0, 'reg_lambda': 2.0692587170651784}\n",
      "Test Performance after last tuning round: 0.8494756150443576\n",
      "Preparing results for fold 2, condition=catboost\n",
      "SCORE: 0.896403963838547                                                                                               \n",
      "SCORE: 0.901678204151878                                                                                               \n",
      "SCORE: 0.8939536264719605                                                                                              \n",
      "SCORE: 0.9112355397194687                                                                                              \n",
      "SCORE: 0.8943130703021593                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.51trial/s, best loss: 0.8939536264719605]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.3692599837314473}\n",
      "Default performance on Test: 1.648711879660856\n",
      "SCORE: 0.9618518940413017                                                                                              \n",
      "SCORE: 1.0710192125112645                                                                                              \n",
      "SCORE: 0.9532927289215033                                                                                              \n",
      "SCORE: 0.9798850934227531                                                                                              \n",
      "SCORE: 1.0240767383579876                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.45trial/s, best loss: 0.9532927289215033]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.1819098580442707, 'n_estimators': 195.0}\n",
      "Test Performance after first tuning round: 1.732468740059608\n",
      "SCORE: 0.935024658610552                                                                                               \n",
      "SCORE: 1.0493517132593349                                                                                              \n",
      "SCORE: 1.0660542013538885                                                                                              \n",
      "SCORE: 0.9388506918824145                                                                                              \n",
      "SCORE: 0.9495156315858946                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.34trial/s, best loss: 0.935024658610552]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.1819098580442707, 'n_estimators': 195.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 2.0}\n",
      "Test Performance after second tuning round: 1.1335427215763183\n",
      "SCORE: 0.9863378994214681                                                                                              \n",
      "SCORE: 1.0082482667903254                                                                                              \n",
      "SCORE: 0.952731845719945                                                                                               \n",
      "SCORE: 0.9780035243855572                                                                                              \n",
      "SCORE: 1.0205221872368446                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.02trial/s, best loss: 0.952731845719945]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.1819098580442707, 'n_estimators': 195.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.5018681087307828, 'subsample': 0.9602067132888112}\n",
      "Test Performance after third tuning round: 1.1744849165395148\n",
      "SCORE: 0.9538123006750576                                                                                              \n",
      "SCORE: 1.032107920394044                                                                                               \n",
      "SCORE: 0.9971046130412399                                                                                              \n",
      "SCORE: 1.0131758548977805                                                                                              \n",
      "SCORE: 0.9895661468898428                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.20trial/s, best loss: 0.9538123006750576]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.1819098580442707, 'n_estimators': 195.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.5018681087307828, 'subsample': 0.9602067132888112, 'gamma': 3.417669101478971, 'reg_alpha': 3.0, 'reg_lambda': 1.782107629504051}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance after last tuning round: 0.9279253863779443\n",
      "Preparing results for fold 2, condition=glmm\n",
      "SCORE: 0.8788830791437713                                                                                              \n",
      "SCORE: 0.87361025507759                                                                                                \n",
      "SCORE: 0.8735308942926319                                                                                              \n",
      "SCORE: 1.0393906530858925                                                                                              \n",
      "SCORE: 0.8769524809446322                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 14.04trial/s, best loss: 0.8735308942926319]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.25631029737223443}\n",
      "Default performance on Test: 1.226677160295867\n",
      "SCORE: 0.9778374117315074                                                                                              \n",
      "SCORE: 0.9660612305485363                                                                                              \n",
      "SCORE: 0.9680408720355306                                                                                              \n",
      "SCORE: 1.011215405653491                                                                                               \n",
      "SCORE: 0.9804437495700373                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.45trial/s, best loss: 0.9660612305485363]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.1527503695266052, 'n_estimators': 220.0}\n",
      "Test Performance after first tuning round: 1.2874616154396454\n",
      "SCORE: 0.9941294283248148                                                                                              \n",
      "SCORE: 0.9760733076678896                                                                                              \n",
      "SCORE: 0.9388526120112483                                                                                              \n",
      "SCORE: 0.9941294283248148                                                                                              \n",
      "SCORE: 0.9950133236625586                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.70trial/s, best loss: 0.9388526120112483]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.1527503695266052, 'n_estimators': 220.0, 'seed': 0, 'max_depth': 12.0, 'min_child_weight': 10.0}\n",
      "Test Performance after second tuning round: 1.0120786828287374\n",
      "SCORE: 0.9635465226270992                                                                                              \n",
      "SCORE: 0.9786838400905872                                                                                              \n",
      "SCORE: 0.9424533420799956                                                                                              \n",
      "SCORE: 0.9285267329319025                                                                                              \n",
      "SCORE: 0.964920087878688                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.76trial/s, best loss: 0.9285267329319025]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.1527503695266052, 'n_estimators': 220.0, 'seed': 0, 'max_depth': 12.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.8461866010143148, 'subsample': 0.7316614631516093}\n",
      "Test Performance after third tuning round: 0.9189324907315225\n",
      "SCORE: 1.0611042408709006                                                                                              \n",
      "SCORE: 0.9293241191366972                                                                                              \n",
      "SCORE: 0.9551163372642086                                                                                              \n",
      "SCORE: 1.0575164412407774                                                                                              \n",
      "SCORE: 1.0654286734803795                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.17trial/s, best loss: 0.9293241191366972]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.1527503695266052, 'n_estimators': 220.0, 'seed': 0, 'max_depth': 12.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.8461866010143148, 'subsample': 0.7316614631516093, 'gamma': 0.9035988126070068, 'reg_alpha': 2.0, 'reg_lambda': 1.0017570573796273}\n",
      "Test Performance after last tuning round: 0.860379682767352\n",
      "Preparing results for fold 3, condition=ignore\n",
      "SCORE: 0.8879063822824043                                                                                              \n",
      "SCORE: 0.8869920757899022                                                                                              \n",
      "SCORE: 0.8878503456883358                                                                                              \n",
      "SCORE: 0.8871413505329866                                                                                              \n",
      "SCORE: 0.8871537456727294                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.39trial/s, best loss: 0.8869920757899022]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.5569652972897572}\n",
      "Default performance on Test: 1.1937651307097845\n",
      "SCORE: 0.9661448234567833                                                                                              \n",
      "SCORE: 0.9525866858692206                                                                                              \n",
      "SCORE: 1.038383679410067                                                                                               \n",
      "SCORE: 0.960069436655224                                                                                               \n",
      "SCORE: 0.9774316219266771                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.28trial/s, best loss: 0.9525866858692206]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4047510019495855, 'n_estimators': 193.0}\n",
      "Test Performance after first tuning round: 1.4228551008191168\n",
      "SCORE: 0.9403116178099719                                                                                              \n",
      "SCORE: 0.9406363168134135                                                                                              \n",
      "SCORE: 0.9866026934147554                                                                                              \n",
      "SCORE: 0.9881823173316361                                                                                              \n",
      "SCORE: 0.8901858602123742                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.99trial/s, best loss: 0.8901858602123742]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4047510019495855, 'n_estimators': 193.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 0.9332077616542304\n",
      "SCORE: 0.895383031839706                                                                                               \n",
      "SCORE: 0.9205238427876085                                                                                              \n",
      "SCORE: 0.8875113889550942                                                                                              \n",
      "SCORE: 0.9067947433919666                                                                                              \n",
      "SCORE: 0.9112683068194405                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.93trial/s, best loss: 0.8875113889550942]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4047510019495855, 'n_estimators': 193.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.6223100489799032, 'subsample': 0.9501267912191095}\n",
      "Test Performance after third tuning round: 0.9213199220724286\n",
      "SCORE: 0.9474615243486308                                                                                              \n",
      "SCORE: 1.0157444328665626                                                                                              \n",
      "SCORE: 0.9755496836480002                                                                                              \n",
      "SCORE: 1.0373673291574514                                                                                              \n",
      "SCORE: 0.9422894083007624                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.19trial/s, best loss: 0.9422894083007624]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4047510019495855, 'n_estimators': 193.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.6223100489799032, 'subsample': 0.9501267912191095, 'gamma': 6.022250177330997, 'reg_alpha': 1.0, 'reg_lambda': 1.644181077417951}\n",
      "Test Performance after last tuning round: 0.9537504336585899\n",
      "Preparing results for fold 3, condition=ohe\n",
      "SCORE: 0.8572425373701067                                                                                              \n",
      "SCORE: 0.8582357502020941                                                                                              \n",
      "SCORE: 0.8567908389838415                                                                                              \n",
      "SCORE: 0.8580227674182028                                                                                              \n",
      "SCORE: 0.8576112646879667                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.47trial/s, best loss: 0.8567908389838415]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.3565205690987917}\n",
      "Default performance on Test: 1.0972099749764073\n",
      "SCORE: 0.8632808293148045                                                                                              \n",
      "SCORE: 0.8526492351355823                                                                                              \n",
      "SCORE: 0.8559732873728592                                                                                              \n",
      "SCORE: 0.8702131739164992                                                                                              \n",
      "SCORE: 0.8544399571456893                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.36trial/s, best loss: 0.8526492351355823]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.42344538157631256, 'n_estimators': 402.0}\n",
      "Test Performance after first tuning round: 1.6095226270059448\n",
      "SCORE: 0.874126863219591                                                                                               \n",
      "SCORE: 0.890763008045289                                                                                               \n",
      "SCORE: 0.8678297738314414                                                                                              \n",
      "SCORE: 0.8750821642829922                                                                                              \n",
      "SCORE: 0.8678297738314414                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.84trial/s, best loss: 0.8678297738314414]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.42344538157631256, 'n_estimators': 402.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 1.287722976492362\n",
      "SCORE: 0.8725375141645383                                                                                              \n",
      "SCORE: 0.8916081311610325                                                                                              \n",
      "SCORE: 0.872178222822307                                                                                               \n",
      "SCORE: 0.8840271394245718                                                                                              \n",
      "SCORE: 0.8926531759415829                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.95trial/s, best loss: 0.872178222822307]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.42344538157631256, 'n_estimators': 402.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.7482971055196894, 'subsample': 0.7948887793813821}\n",
      "Test Performance after third tuning round: 1.2254384616459768\n",
      "SCORE: 0.8835565413090569                                                                                              \n",
      "SCORE: 0.928856486115202                                                                                               \n",
      "SCORE: 1.0369535949039945                                                                                              \n",
      "SCORE: 0.9553843474912878                                                                                              \n",
      "SCORE: 0.9172190152769553                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.01trial/s, best loss: 0.8835565413090569]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.42344538157631256, 'n_estimators': 402.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.7482971055196894, 'subsample': 0.7948887793813821, 'gamma': 0.9459975064902837, 'reg_alpha': 1.0, 'reg_lambda': 1.8911984500016008}\n",
      "Test Performance after last tuning round: 0.8570969813258614\n",
      "Preparing results for fold 3, condition=target\n",
      "SCORE: 0.8801430555125009                                                                                              \n",
      "SCORE: 0.8640213343219516                                                                                              \n",
      "SCORE: 0.8897531315355313                                                                                              \n",
      "SCORE: 0.8988789327843694                                                                                              \n",
      "SCORE: 0.8630739357184595                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.74trial/s, best loss: 0.8630739357184595]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.9478739612911757}\n",
      "Default performance on Test: 1.015626114964725\n",
      "SCORE: 0.8611946997474904                                                                                              \n",
      "SCORE: 0.8655246795395323                                                                                              \n",
      "SCORE: 0.8791701658286686                                                                                              \n",
      "SCORE: 0.8543903858373388                                                                                              \n",
      "SCORE: 0.8538558427130786                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.64trial/s, best loss: 0.8538558427130786]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.1327111432106453, 'n_estimators': 81.0}\n",
      "Test Performance after first tuning round: 0.8411148560186265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.9451509195732687                                                                                              \n",
      "SCORE: 0.923477767564548                                                                                               \n",
      "SCORE: 0.9174471407364158                                                                                              \n",
      "SCORE: 0.9353364878275228                                                                                              \n",
      "SCORE: 0.8774877864430015                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.30trial/s, best loss: 0.8774877864430015]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.1327111432106453, 'n_estimators': 81.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 0.8352922051726703\n",
      "SCORE: 0.8819222280112557                                                                                              \n",
      "SCORE: 0.9689804654431619                                                                                              \n",
      "SCORE: 0.9418914581267634                                                                                              \n",
      "SCORE: 0.931529289288304                                                                                               \n",
      "SCORE: 0.897390714487105                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.08trial/s, best loss: 0.8819222280112557]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.1327111432106453, 'n_estimators': 81.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.6179508655878427, 'subsample': 0.8009850628318748}\n",
      "Test Performance after third tuning round: 0.8006236317890779\n",
      "SCORE: 1.0071774905935202                                                                                              \n",
      "SCORE: 1.0118789669814097                                                                                              \n",
      "SCORE: 0.93394753676319                                                                                                \n",
      "SCORE: 1.0670035542746916                                                                                              \n",
      "SCORE: 0.891078504965661                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.08trial/s, best loss: 0.891078504965661]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.1327111432106453, 'n_estimators': 81.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.6179508655878427, 'subsample': 0.8009850628318748, 'gamma': 0.43245033895230495, 'reg_alpha': 2.0, 'reg_lambda': 1.2299454524684474}\n",
      "Test Performance after last tuning round: 0.8208007370049847\n",
      "Preparing results for fold 3, condition=ordinal\n",
      "SCORE: 0.887379783545604                                                                                               \n",
      "SCORE: 0.9283657157963079                                                                                              \n",
      "SCORE: 0.888369834111138                                                                                               \n",
      "SCORE: 0.8860054581323652                                                                                              \n",
      "SCORE: 1.0359727467016004                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.61trial/s, best loss: 0.8860054581323652]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.2326857967227455}\n",
      "Default performance on Test: 1.1180512064844477\n",
      "SCORE: 0.9194064401080446                                                                                              \n",
      "SCORE: 0.9087680992485743                                                                                              \n",
      "SCORE: 1.001129963441412                                                                                               \n",
      "SCORE: 0.9026726663535605                                                                                              \n",
      "SCORE: 0.9415469542618677                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.88trial/s, best loss: 0.9026726663535605]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.43744288690359434, 'n_estimators': 347.0}\n",
      "Test Performance after first tuning round: 1.4599368314222956\n",
      "SCORE: 0.889045190305648                                                                                               \n",
      "SCORE: 0.8689331261627918                                                                                              \n",
      "SCORE: 0.8853040581231093                                                                                              \n",
      "SCORE: 0.8454565904929666                                                                                              \n",
      "SCORE: 0.8689331261627918                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.50trial/s, best loss: 0.8454565904929666]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.43744288690359434, 'n_estimators': 347.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 2.0}\n",
      "Test Performance after second tuning round: 1.2088195939303166\n",
      "SCORE: 0.8446150656292767                                                                                              \n",
      "SCORE: 0.8681293903583256                                                                                              \n",
      "SCORE: 0.8609044377418955                                                                                              \n",
      "SCORE: 0.8561942607905092                                                                                              \n",
      "SCORE: 0.9120967327427568                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.73trial/s, best loss: 0.8446150656292767]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.43744288690359434, 'n_estimators': 347.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.7356251434516646, 'subsample': 0.9733465009755989}\n",
      "Test Performance after third tuning round: 1.2201514469308912\n",
      "SCORE: 0.9746424013934829                                                                                              \n",
      "SCORE: 1.0446311027242479                                                                                              \n",
      "SCORE: 0.9098897879635899                                                                                              \n",
      "SCORE: 0.8538600414868899                                                                                              \n",
      "SCORE: 1.0285370948549617                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.01trial/s, best loss: 0.8538600414868899]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.43744288690359434, 'n_estimators': 347.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.7356251434516646, 'subsample': 0.9733465009755989, 'gamma': 0.24721789792082302, 'reg_alpha': 3.0, 'reg_lambda': 2.3082232037105084}\n",
      "Test Performance after last tuning round: 0.8098351537303227\n",
      "Preparing results for fold 3, condition=catboost\n",
      "SCORE: 1.004719452324224                                                                                               \n",
      "SCORE: 0.8828185571543591                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.9007722438815021                                                                                              \n",
      "SCORE: 0.8842201438321607                                                                                              \n",
      "SCORE: 0.8836491858692895                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.44trial/s, best loss: 0.8828185571543591]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.5161294942596787}\n",
      "Default performance on Test: 1.1825214653187477\n",
      "SCORE: 1.0344073764434816                                                                                              \n",
      "SCORE: 0.989164846149953                                                                                               \n",
      "SCORE: 1.0086961091702353                                                                                              \n",
      "SCORE: 1.0344227824332914                                                                                              \n",
      "SCORE: 0.9707144609778121                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.09trial/s, best loss: 0.9707144609778121]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.30723314306162935, 'n_estimators': 195.0}\n",
      "Test Performance after first tuning round: 1.3353775433799326\n",
      "SCORE: 0.9582881842056699                                                                                              \n",
      "SCORE: 0.9668650698311418                                                                                              \n",
      "SCORE: 0.9502225020023924                                                                                              \n",
      "SCORE: 1.0470076356411293                                                                                              \n",
      "SCORE: 0.9582881842056699                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.95trial/s, best loss: 0.9502225020023924]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.30723314306162935, 'n_estimators': 195.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 9.0}\n",
      "Test Performance after second tuning round: 1.1903005523052947\n",
      "SCORE: 0.9575331946569964                                                                                              \n",
      "SCORE: 0.9818797066690268                                                                                              \n",
      "SCORE: 0.9907104697677674                                                                                              \n",
      "SCORE: 0.9851737478543239                                                                                              \n",
      "SCORE: 0.9819770017121335                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.48trial/s, best loss: 0.9575331946569964]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.30723314306162935, 'n_estimators': 195.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.5005920592052954, 'subsample': 0.8285851758713398}\n",
      "Test Performance after third tuning round: 1.149050642987609\n",
      "SCORE: 1.0553108578251769                                                                                              \n",
      "SCORE: 0.9532908418032136                                                                                              \n",
      "SCORE: 0.964777301532127                                                                                               \n",
      "SCORE: 0.9921825236928923                                                                                              \n",
      "SCORE: 0.943012317179593                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.01trial/s, best loss: 0.943012317179593]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.30723314306162935, 'n_estimators': 195.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.5005920592052954, 'subsample': 0.8285851758713398, 'gamma': 4.4836358201244835, 'reg_alpha': 1.0, 'reg_lambda': 3.9452067678627722}\n",
      "Test Performance after last tuning round: 0.9566463028598363\n",
      "Preparing results for fold 3, condition=glmm\n",
      "SCORE: 0.8768771088116258                                                                                              \n",
      "SCORE: 0.8755707116899949                                                                                              \n",
      "SCORE: 0.8770986538999672                                                                                              \n",
      "SCORE: 0.8750681969058574                                                                                              \n",
      "SCORE: 0.8750727321373333                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.83trial/s, best loss: 0.8750681969058574]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.46451235855659584}\n",
      "Default performance on Test: 1.2441348948505369\n",
      "SCORE: 0.9457213050085894                                                                                              \n",
      "SCORE: 0.9669509083220287                                                                                              \n",
      "SCORE: 0.9162397647867337                                                                                              \n",
      "SCORE: 0.9341565447695966                                                                                              \n",
      "SCORE: 0.9304495735555605                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.21trial/s, best loss: 0.9162397647867337]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4792466794319517, 'n_estimators': 398.0}\n",
      "Test Performance after first tuning round: 1.6102501494778618\n",
      "SCORE: 0.8836453343078189                                                                                              \n",
      "SCORE: 0.8534479395623222                                                                                              \n",
      "SCORE: 1.0387261930663367                                                                                              \n",
      "SCORE: 1.0292954298741872                                                                                              \n",
      "SCORE: 0.958848703344102                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.69trial/s, best loss: 0.8534479395623222]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4792466794319517, 'n_estimators': 398.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 1.5018003645799147\n",
      "SCORE: 0.8308443079542194                                                                                              \n",
      "SCORE: 0.8417157331619409                                                                                              \n",
      "SCORE: 0.9180604194934485                                                                                              \n",
      "SCORE: 0.8509718952765274                                                                                              \n",
      "SCORE: 0.8345133953382525                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.42trial/s, best loss: 0.8308443079542194]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4792466794319517, 'n_estimators': 398.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.9947939789105118, 'subsample': 0.7739666400613744}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance after third tuning round: 1.5934893259539418\n",
      "SCORE: 0.8855459781277737                                                                                              \n",
      "SCORE: 1.0217347708518703                                                                                              \n",
      "SCORE: 0.9772207560128903                                                                                              \n",
      "SCORE: 0.9084034882719181                                                                                              \n",
      "SCORE: 0.8961366281927351                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.31trial/s, best loss: 0.8855459781277737]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4792466794319517, 'n_estimators': 398.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.9947939789105118, 'subsample': 0.7739666400613744, 'gamma': 0.0008835192329055705, 'reg_alpha': 6.0, 'reg_lambda': 3.3371367818257327}\n",
      "Test Performance after last tuning round: 0.8839849682462687\n",
      "Preparing results for fold 4, condition=ignore\n",
      "SCORE: 0.8970533811658175                                                                                              \n",
      "SCORE: 0.8922576904478593                                                                                              \n",
      "SCORE: 0.8921171230491977                                                                                              \n",
      "SCORE: 0.8941721161139728                                                                                              \n",
      "SCORE: 0.8962777254900042                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 14.36trial/s, best loss: 0.8921171230491977]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.3078482537996332}\n",
      "Default performance on Test: 1.266797130428288\n",
      "SCORE: 0.9594886832266137                                                                                              \n",
      "SCORE: 0.9343903292746646                                                                                              \n",
      "SCORE: 0.9300978699723199                                                                                              \n",
      "SCORE: 0.9362291261643689                                                                                              \n",
      "SCORE: 0.9301976314604294                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.79trial/s, best loss: 0.9300978699723199]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3736686007274834, 'n_estimators': 452.0}\n",
      "Test Performance after first tuning round: 1.6123920640557339\n",
      "SCORE: 0.9130835931744257                                                                                              \n",
      "SCORE: 0.8923727709606192                                                                                              \n",
      "SCORE: 0.9195775262393099                                                                                              \n",
      "SCORE: 0.9330417027474258                                                                                              \n",
      "SCORE: 0.8869818179326282                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.70trial/s, best loss: 0.8869818179326282]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3736686007274834, 'n_estimators': 452.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 1.0183900475337084\n",
      "SCORE: 0.9291909543843235                                                                                              \n",
      "SCORE: 0.884646761290732                                                                                               \n",
      "SCORE: 0.9117104369788116                                                                                              \n",
      "SCORE: 0.9109939210511906                                                                                              \n",
      "SCORE: 0.9403912950799151                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.24trial/s, best loss: 0.884646761290732]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3736686007274834, 'n_estimators': 452.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.5412162954139231, 'subsample': 0.9334109257247127}\n",
      "Test Performance after third tuning round: 1.0197627047597675\n",
      "SCORE: 0.9737050297224983                                                                                              \n",
      "SCORE: 0.953338226006324                                                                                               \n",
      "SCORE: 0.9803357352337233                                                                                              \n",
      "SCORE: 0.9855119494538922                                                                                              \n",
      "SCORE: 1.0392097330622467                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.45trial/s, best loss: 0.953338226006324]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3736686007274834, 'n_estimators': 452.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.5412162954139231, 'subsample': 0.9334109257247127, 'gamma': 5.05378320167801, 'reg_alpha': 1.0, 'reg_lambda': 1.6274908817688347}\n",
      "Test Performance after last tuning round: 0.8901916556647004\n",
      "Preparing results for fold 4, condition=ohe\n",
      "SCORE: 0.896534429884315                                                                                               \n",
      "SCORE: 0.9000782096455705                                                                                              \n",
      "SCORE: 0.8921234462344343                                                                                              \n",
      "SCORE: 0.8970197273544495                                                                                              \n",
      "SCORE: 0.8811556659764015                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.27trial/s, best loss: 0.8811556659764015]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.4071865079270024}\n",
      "Default performance on Test: 1.155103955085711\n",
      "SCORE: 0.8542275575422756                                                                                              \n",
      "SCORE: 0.8360970935987158                                                                                              \n",
      "SCORE: 0.8483375401285903                                                                                              \n",
      "SCORE: 0.9656132446062241                                                                                              \n",
      "SCORE: 0.8621353042423268                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.00trial/s, best loss: 0.8360970935987158]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.40531543101622397, 'n_estimators': 283.0}\n",
      "Test Performance after first tuning round: 1.6000889066192847\n",
      "SCORE: 0.841709867101258                                                                                               \n",
      "SCORE: 0.9358526323736797                                                                                              \n",
      "SCORE: 0.989429280481845                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.8842844502206562                                                                                              \n",
      "SCORE: 0.8898891206647763                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.12trial/s, best loss: 0.841709867101258]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.40531543101622397, 'n_estimators': 283.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 1.5761075569670862\n",
      "SCORE: 0.8478840573313235                                                                                              \n",
      "SCORE: 0.8694533225590882                                                                                              \n",
      "SCORE: 0.9030119937665638                                                                                              \n",
      "SCORE: 0.8854665665083801                                                                                              \n",
      "SCORE: 0.9029741130941339                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.09trial/s, best loss: 0.8478840573313235]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.40531543101622397, 'n_estimators': 283.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.6564124714335443, 'subsample': 0.9813378490001772}\n",
      "Test Performance after third tuning round: 1.488477158524187\n",
      "SCORE: 1.0494655236784893                                                                                              \n",
      "SCORE: 1.0344475598031049                                                                                              \n",
      "SCORE: 1.0471356791797661                                                                                              \n",
      "SCORE: 0.9763807981356726                                                                                              \n",
      "SCORE: 0.9657302702891775                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.78trial/s, best loss: 0.9657302702891775]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.40531543101622397, 'n_estimators': 283.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.6564124714335443, 'subsample': 0.9813378490001772, 'gamma': 1.629347319378353, 'reg_alpha': 5.0, 'reg_lambda': 2.349891952859315}\n",
      "Test Performance after last tuning round: 0.8622249849815198\n",
      "Preparing results for fold 4, condition=target\n",
      "SCORE: 0.8718996795607652                                                                                              \n",
      "SCORE: 0.8740138485499788                                                                                              \n",
      "SCORE: 0.8815048843431763                                                                                              \n",
      "SCORE: 0.8864837703895931                                                                                              \n",
      "SCORE: 0.8725278271857908                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.50trial/s, best loss: 0.8718996795607652]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.41980889648201464}\n",
      "Default performance on Test: 1.1979385862862568\n",
      "SCORE: 0.8747389112039853                                                                                              \n",
      "SCORE: 0.8448928737654479                                                                                              \n",
      "SCORE: 0.8490286593762215                                                                                              \n",
      "SCORE: 0.8388278292131407                                                                                              \n",
      "SCORE: 0.8361105050814736                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.73trial/s, best loss: 0.8361105050814736]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.14223500446601184, 'n_estimators': 486.0}\n",
      "Test Performance after first tuning round: 1.4522381271591203\n",
      "SCORE: 0.868160097685035                                                                                               \n",
      "SCORE: 0.868160097685035                                                                                               \n",
      "SCORE: 0.9275426745853549                                                                                              \n",
      "SCORE: 0.9178774493039722                                                                                              \n",
      "SCORE: 0.8712705794581226                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.48trial/s, best loss: 0.868160097685035]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.14223500446601184, 'n_estimators': 486.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 0.9396918082041146\n",
      "SCORE: 0.9046344868611751                                                                                              \n",
      "SCORE: 0.9198623113670186                                                                                              \n",
      "SCORE: 0.9552639050993739                                                                                              \n",
      "SCORE: 0.924061596446134                                                                                               \n",
      "SCORE: 0.8740459499668845                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.19trial/s, best loss: 0.8740459499668845]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.14223500446601184, 'n_estimators': 486.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.8154504015033643, 'subsample': 0.7065839851158617}\n",
      "Test Performance after third tuning round: 0.9872295461023706\n",
      "SCORE: 1.0095348457609572                                                                                              \n",
      "SCORE: 1.0956444256140805                                                                                              \n",
      "SCORE: 1.0293804468698131                                                                                              \n",
      "SCORE: 1.0286728025787943                                                                                              \n",
      "SCORE: 1.0683536403651064                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.64trial/s, best loss: 1.0095348457609572]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.14223500446601184, 'n_estimators': 486.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.8154504015033643, 'subsample': 0.7065839851158617, 'gamma': 3.9761952774523417, 'reg_alpha': 1.0, 'reg_lambda': 1.476821814765149}\n",
      "Test Performance after last tuning round: 0.8336893935767414\n",
      "Preparing results for fold 4, condition=ordinal\n",
      "SCORE: 0.8982607922687146                                                                                              \n",
      "SCORE: 0.9012162252950361                                                                                              \n",
      "SCORE: 0.89917788739229                                                                                                \n",
      "SCORE: 0.8974981595728637                                                                                              \n",
      "SCORE: 0.9012179800807463                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.53trial/s, best loss: 0.8974981595728637]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.19692593286626767}\n",
      "Default performance on Test: 1.3575898321473925\n",
      "SCORE: 0.8658366679907944                                                                                              \n",
      "SCORE: 0.9896118322232761                                                                                              \n",
      "SCORE: 0.8614647465207333                                                                                              \n",
      "SCORE: 0.8678570214725589                                                                                              \n",
      "SCORE: 0.9886576539211095                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.11trial/s, best loss: 0.8614647465207333]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.2259378109859138, 'n_estimators': 457.0}\n",
      "Test Performance after first tuning round: 1.7532597193220392\n",
      "SCORE: 0.8398322826094852                                                                                              \n",
      "SCORE: 0.8469143324675169                                                                                              \n",
      "SCORE: 0.8558955393437524                                                                                              \n",
      "SCORE: 0.8743033023744247                                                                                              \n",
      "SCORE: 0.9260577570655915                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.32trial/s, best loss: 0.8398322826094852]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.2259378109859138, 'n_estimators': 457.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 1.3986781146954925\n",
      "SCORE: 0.8634678650473765                                                                                              \n",
      "SCORE: 0.952473952289869                                                                                               \n",
      "SCORE: 0.8906197610400775                                                                                              \n",
      "SCORE: 0.8708198801161726                                                                                              \n",
      "SCORE: 0.8512229200527865                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.32trial/s, best loss: 0.8512229200527865]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.2259378109859138, 'n_estimators': 457.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.9695083537328297, 'subsample': 0.7001763759003923}\n",
      "Test Performance after third tuning round: 1.3088544843293848\n",
      "SCORE: 0.9911022015913215                                                                                              \n",
      "SCORE: 1.0698918311328063                                                                                              \n",
      "SCORE: 1.0158258477628985                                                                                              \n",
      "SCORE: 1.0942629478972736                                                                                              \n",
      "SCORE: 0.8835670067728332                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.76trial/s, best loss: 0.8835670067728332]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.2259378109859138, 'n_estimators': 457.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.9695083537328297, 'subsample': 0.7001763759003923, 'gamma': 0.12141928306183718, 'reg_alpha': 4.0, 'reg_lambda': 2.4091917109787317}\n",
      "Test Performance after last tuning round: 0.8467975014023358\n",
      "Preparing results for fold 4, condition=catboost\n",
      "SCORE: 0.8848737117957024                                                                                              \n",
      "SCORE: 0.8846425261339217                                                                                              \n",
      "SCORE: 0.8829720954512433                                                                                              \n",
      "SCORE: 0.8816686143734888                                                                                              \n",
      "SCORE: 0.8816913555477452                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.57trial/s, best loss: 0.8816686143734888]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.3447041901711918}\n",
      "Default performance on Test: 1.259863723156834\n",
      "SCORE: 0.9497651480702831                                                                                              \n",
      "SCORE: 0.9634560522482631                                                                                              \n",
      "SCORE: 0.9927771607932202                                                                                              \n",
      "SCORE: 0.9458614288901369                                                                                              \n",
      "SCORE: 0.9540230092676565                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.50trial/s, best loss: 0.9458614288901369]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4536950188121696, 'n_estimators': 160.0}\n",
      "Test Performance after first tuning round: 1.4236500659671945\n",
      "SCORE: 0.9089371412632472                                                                                              \n",
      "SCORE: 0.9375585439221477                                                                                              \n",
      "SCORE: 0.910658065224478                                                                                               \n",
      "SCORE: 0.9375585439221477                                                                                              \n",
      "SCORE: 0.9428060458731469                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.50trial/s, best loss: 0.9089371412632472]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4536950188121696, 'n_estimators': 160.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 0.9010285824063926\n",
      "SCORE: 0.9134475494252076                                                                                              \n",
      "SCORE: 0.9106971541241116                                                                                              \n",
      "SCORE: 0.9017380821702284                                                                                              \n",
      "SCORE: 0.9208701053175034                                                                                              \n",
      "SCORE: 0.9217571733547555                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.23trial/s, best loss: 0.9017380821702284]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4536950188121696, 'n_estimators': 160.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.6946057834340434, 'subsample': 0.9881193177772198}\n",
      "Test Performance after third tuning round: 0.8935269776255547\n",
      "SCORE: 0.9883448247405072                                                                                              \n",
      "SCORE: 1.0408759806147236                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 1.057026018208245                                                                                               \n",
      "SCORE: 0.9692096728110686                                                                                              \n",
      "SCORE: 0.9136147749848439                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.04trial/s, best loss: 0.9136147749848439]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4536950188121696, 'n_estimators': 160.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.6946057834340434, 'subsample': 0.9881193177772198, 'gamma': 0.37990687218840985, 'reg_alpha': 3.0, 'reg_lambda': 2.0860549523124208}\n",
      "Test Performance after last tuning round: 0.8616773027730001\n",
      "Preparing results for fold 4, condition=glmm\n",
      "SCORE: 0.8671466176073185                                                                                              \n",
      "SCORE: 0.8673033240292212                                                                                              \n",
      "SCORE: 0.8657587068868011                                                                                              \n",
      "SCORE: 0.870473469532017                                                                                               \n",
      "SCORE: 0.8665756523363486                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.98trial/s, best loss: 0.8657587068868011]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.3432578150258816}\n",
      "Default performance on Test: 1.2731971440707393\n",
      "SCORE: 1.0650051196098307                                                                                              \n",
      "SCORE: 1.009258254194511                                                                                               \n",
      "SCORE: 1.0420398872883776                                                                                              \n",
      "SCORE: 1.0284398148363874                                                                                              \n",
      "SCORE: 1.0108966375670791                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.29trial/s, best loss: 1.009258254194511]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.18507253703432117, 'n_estimators': 348.0}\n",
      "Test Performance after first tuning round: 1.4745161382222591\n",
      "SCORE: 0.9685729603541781                                                                                              \n",
      "SCORE: 0.9386071599577225                                                                                              \n",
      "SCORE: 0.9660386468036073                                                                                              \n",
      "SCORE: 0.9726743971283985                                                                                              \n",
      "SCORE: 0.9894973136607546                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.15trial/s, best loss: 0.9386071599577225]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.18507253703432117, 'n_estimators': 348.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 0.9238090910274708\n",
      "SCORE: 0.958195589689908                                                                                               \n",
      "SCORE: 0.9268162628869907                                                                                              \n",
      "SCORE: 0.9484462730506878                                                                                              \n",
      "SCORE: 0.9026979457050706                                                                                              \n",
      "SCORE: 0.9232622721172883                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.52trial/s, best loss: 0.9026979457050706]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.18507253703432117, 'n_estimators': 348.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.8700172789491094, 'subsample': 0.7650241436036644}\n",
      "Test Performance after third tuning round: 0.944328616224749\n",
      "SCORE: 1.0234311627616326                                                                                              \n",
      "SCORE: 0.9955776364682551                                                                                              \n",
      "SCORE: 0.9493699923683014                                                                                              \n",
      "SCORE: 0.9900851394408013                                                                                              \n",
      "SCORE: 1.0468717318815428                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.66trial/s, best loss: 0.9493699923683014]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.18507253703432117, 'n_estimators': 348.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.8700172789491094, 'subsample': 0.7650241436036644, 'gamma': 0.6227829066903273, 'reg_alpha': 3.0, 'reg_lambda': 3.3357060649687247}\n",
      "Test Performance after last tuning round: 0.8194784959776991\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1ea9c_row0_col3, #T_1ea9c_row0_col4, #T_1ea9c_row0_col5, #T_1ea9c_row11_col0, #T_1ea9c_row11_col1, #T_1ea9c_row11_col2 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1ea9c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1ea9c_level0_col0\" class=\"col_heading level0 col0\" >Accuracy Train</th>\n",
       "      <th id=\"T_1ea9c_level0_col1\" class=\"col_heading level0 col1\" >F1 Train</th>\n",
       "      <th id=\"T_1ea9c_level0_col2\" class=\"col_heading level0 col2\" >AUROC Train</th>\n",
       "      <th id=\"T_1ea9c_level0_col3\" class=\"col_heading level0 col3\" >Accuracy Test</th>\n",
       "      <th id=\"T_1ea9c_level0_col4\" class=\"col_heading level0 col4\" >F1 Test</th>\n",
       "      <th id=\"T_1ea9c_level0_col5\" class=\"col_heading level0 col5\" >AUROC Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row0\" class=\"row_heading level0 row0\" >XGB_ohe</th>\n",
       "      <td id=\"T_1ea9c_row0_col0\" class=\"data row0 col0\" >0.924500</td>\n",
       "      <td id=\"T_1ea9c_row0_col1\" class=\"data row0 col1\" >0.923500</td>\n",
       "      <td id=\"T_1ea9c_row0_col2\" class=\"data row0 col2\" >0.992600</td>\n",
       "      <td id=\"T_1ea9c_row0_col3\" class=\"data row0 col3\" >0.572900</td>\n",
       "      <td id=\"T_1ea9c_row0_col4\" class=\"data row0 col4\" >0.586300</td>\n",
       "      <td id=\"T_1ea9c_row0_col5\" class=\"data row0 col5\" >0.781500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row1\" class=\"row_heading level0 row1\" >LR_ignore</th>\n",
       "      <td id=\"T_1ea9c_row1_col0\" class=\"data row1 col0\" >0.578100</td>\n",
       "      <td id=\"T_1ea9c_row1_col1\" class=\"data row1 col1\" >0.578800</td>\n",
       "      <td id=\"T_1ea9c_row1_col2\" class=\"data row1 col2\" >0.774700</td>\n",
       "      <td id=\"T_1ea9c_row1_col3\" class=\"data row1 col3\" >0.562500</td>\n",
       "      <td id=\"T_1ea9c_row1_col4\" class=\"data row1 col4\" >0.566500</td>\n",
       "      <td id=\"T_1ea9c_row1_col5\" class=\"data row1 col5\" >0.717400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row2\" class=\"row_heading level0 row2\" >LR_ignore_tuned</th>\n",
       "      <td id=\"T_1ea9c_row2_col0\" class=\"data row2 col0\" >0.580700</td>\n",
       "      <td id=\"T_1ea9c_row2_col1\" class=\"data row2 col1\" >0.579500</td>\n",
       "      <td id=\"T_1ea9c_row2_col2\" class=\"data row2 col2\" >0.773800</td>\n",
       "      <td id=\"T_1ea9c_row2_col3\" class=\"data row2 col3\" >0.552100</td>\n",
       "      <td id=\"T_1ea9c_row2_col4\" class=\"data row2 col4\" >0.560600</td>\n",
       "      <td id=\"T_1ea9c_row2_col5\" class=\"data row2 col5\" >0.717400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row3\" class=\"row_heading level0 row3\" >XGB_glmm</th>\n",
       "      <td id=\"T_1ea9c_row3_col0\" class=\"data row3 col0\" >0.976600</td>\n",
       "      <td id=\"T_1ea9c_row3_col1\" class=\"data row3 col1\" >0.975400</td>\n",
       "      <td id=\"T_1ea9c_row3_col2\" class=\"data row3 col2\" >0.999400</td>\n",
       "      <td id=\"T_1ea9c_row3_col3\" class=\"data row3 col3\" >0.541700</td>\n",
       "      <td id=\"T_1ea9c_row3_col4\" class=\"data row3 col4\" >0.544300</td>\n",
       "      <td id=\"T_1ea9c_row3_col5\" class=\"data row3 col5\" >0.729700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row4\" class=\"row_heading level0 row4\" >XGB_target</th>\n",
       "      <td id=\"T_1ea9c_row4_col0\" class=\"data row4 col0\" >0.929700</td>\n",
       "      <td id=\"T_1ea9c_row4_col1\" class=\"data row4 col1\" >0.929500</td>\n",
       "      <td id=\"T_1ea9c_row4_col2\" class=\"data row4 col2\" >0.993700</td>\n",
       "      <td id=\"T_1ea9c_row4_col3\" class=\"data row4 col3\" >0.531200</td>\n",
       "      <td id=\"T_1ea9c_row4_col4\" class=\"data row4 col4\" >0.538900</td>\n",
       "      <td id=\"T_1ea9c_row4_col5\" class=\"data row4 col5\" >0.748300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row5\" class=\"row_heading level0 row5\" >XGB_ordinal</th>\n",
       "      <td id=\"T_1ea9c_row5_col0\" class=\"data row5 col0\" >0.929700</td>\n",
       "      <td id=\"T_1ea9c_row5_col1\" class=\"data row5 col1\" >0.929300</td>\n",
       "      <td id=\"T_1ea9c_row5_col2\" class=\"data row5 col2\" >0.993800</td>\n",
       "      <td id=\"T_1ea9c_row5_col3\" class=\"data row5 col3\" >0.520800</td>\n",
       "      <td id=\"T_1ea9c_row5_col4\" class=\"data row5 col4\" >0.536200</td>\n",
       "      <td id=\"T_1ea9c_row5_col5\" class=\"data row5 col5\" >0.756800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row6\" class=\"row_heading level0 row6\" >LR_target_tuned</th>\n",
       "      <td id=\"T_1ea9c_row6_col0\" class=\"data row6 col0\" >0.604200</td>\n",
       "      <td id=\"T_1ea9c_row6_col1\" class=\"data row6 col1\" >0.598700</td>\n",
       "      <td id=\"T_1ea9c_row6_col2\" class=\"data row6 col2\" >0.806800</td>\n",
       "      <td id=\"T_1ea9c_row6_col3\" class=\"data row6 col3\" >0.520800</td>\n",
       "      <td id=\"T_1ea9c_row6_col4\" class=\"data row6 col4\" >0.526400</td>\n",
       "      <td id=\"T_1ea9c_row6_col5\" class=\"data row6 col5\" >0.713900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row7\" class=\"row_heading level0 row7\" >LR_target</th>\n",
       "      <td id=\"T_1ea9c_row7_col0\" class=\"data row7 col0\" >0.601600</td>\n",
       "      <td id=\"T_1ea9c_row7_col1\" class=\"data row7 col1\" >0.596700</td>\n",
       "      <td id=\"T_1ea9c_row7_col2\" class=\"data row7 col2\" >0.806800</td>\n",
       "      <td id=\"T_1ea9c_row7_col3\" class=\"data row7 col3\" >0.520800</td>\n",
       "      <td id=\"T_1ea9c_row7_col4\" class=\"data row7 col4\" >0.526400</td>\n",
       "      <td id=\"T_1ea9c_row7_col5\" class=\"data row7 col5\" >0.713700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row8\" class=\"row_heading level0 row8\" >LR_catboost_tuned</th>\n",
       "      <td id=\"T_1ea9c_row8_col0\" class=\"data row8 col0\" >0.588500</td>\n",
       "      <td id=\"T_1ea9c_row8_col1\" class=\"data row8 col1\" >0.584500</td>\n",
       "      <td id=\"T_1ea9c_row8_col2\" class=\"data row8 col2\" >0.789900</td>\n",
       "      <td id=\"T_1ea9c_row8_col3\" class=\"data row8 col3\" >0.531200</td>\n",
       "      <td id=\"T_1ea9c_row8_col4\" class=\"data row8 col4\" >0.521500</td>\n",
       "      <td id=\"T_1ea9c_row8_col5\" class=\"data row8 col5\" >0.709800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row9\" class=\"row_heading level0 row9\" >LR_catboost</th>\n",
       "      <td id=\"T_1ea9c_row9_col0\" class=\"data row9 col0\" >0.588500</td>\n",
       "      <td id=\"T_1ea9c_row9_col1\" class=\"data row9 col1\" >0.584400</td>\n",
       "      <td id=\"T_1ea9c_row9_col2\" class=\"data row9 col2\" >0.792200</td>\n",
       "      <td id=\"T_1ea9c_row9_col3\" class=\"data row9 col3\" >0.531200</td>\n",
       "      <td id=\"T_1ea9c_row9_col4\" class=\"data row9 col4\" >0.521500</td>\n",
       "      <td id=\"T_1ea9c_row9_col5\" class=\"data row9 col5\" >0.705200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row10\" class=\"row_heading level0 row10\" >XGB_ignore</th>\n",
       "      <td id=\"T_1ea9c_row10_col0\" class=\"data row10 col0\" >0.700500</td>\n",
       "      <td id=\"T_1ea9c_row10_col1\" class=\"data row10 col1\" >0.699100</td>\n",
       "      <td id=\"T_1ea9c_row10_col2\" class=\"data row10 col2\" >0.893300</td>\n",
       "      <td id=\"T_1ea9c_row10_col3\" class=\"data row10 col3\" >0.510400</td>\n",
       "      <td id=\"T_1ea9c_row10_col4\" class=\"data row10 col4\" >0.516000</td>\n",
       "      <td id=\"T_1ea9c_row10_col5\" class=\"data row10 col5\" >0.710100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row11\" class=\"row_heading level0 row11\" >XGB_catboost</th>\n",
       "      <td id=\"T_1ea9c_row11_col0\" class=\"data row11 col0\" >1.000000</td>\n",
       "      <td id=\"T_1ea9c_row11_col1\" class=\"data row11 col1\" >1.000000</td>\n",
       "      <td id=\"T_1ea9c_row11_col2\" class=\"data row11 col2\" >1.000000</td>\n",
       "      <td id=\"T_1ea9c_row11_col3\" class=\"data row11 col3\" >0.541700</td>\n",
       "      <td id=\"T_1ea9c_row11_col4\" class=\"data row11 col4\" >0.512500</td>\n",
       "      <td id=\"T_1ea9c_row11_col5\" class=\"data row11 col5\" >0.688200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row12\" class=\"row_heading level0 row12\" >LR_ordinal_tuned</th>\n",
       "      <td id=\"T_1ea9c_row12_col0\" class=\"data row12 col0\" >0.591100</td>\n",
       "      <td id=\"T_1ea9c_row12_col1\" class=\"data row12 col1\" >0.584800</td>\n",
       "      <td id=\"T_1ea9c_row12_col2\" class=\"data row12 col2\" >0.787000</td>\n",
       "      <td id=\"T_1ea9c_row12_col3\" class=\"data row12 col3\" >0.510400</td>\n",
       "      <td id=\"T_1ea9c_row12_col4\" class=\"data row12 col4\" >0.507200</td>\n",
       "      <td id=\"T_1ea9c_row12_col5\" class=\"data row12 col5\" >0.713500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row13\" class=\"row_heading level0 row13\" >XGB_ignore_tuned</th>\n",
       "      <td id=\"T_1ea9c_row13_col0\" class=\"data row13 col0\" >0.591100</td>\n",
       "      <td id=\"T_1ea9c_row13_col1\" class=\"data row13 col1\" >0.598400</td>\n",
       "      <td id=\"T_1ea9c_row13_col2\" class=\"data row13 col2\" >0.760200</td>\n",
       "      <td id=\"T_1ea9c_row13_col3\" class=\"data row13 col3\" >0.510400</td>\n",
       "      <td id=\"T_1ea9c_row13_col4\" class=\"data row13 col4\" >0.506400</td>\n",
       "      <td id=\"T_1ea9c_row13_col5\" class=\"data row13 col5\" >0.709600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row14\" class=\"row_heading level0 row14\" >XGB_ordinal_tuned</th>\n",
       "      <td id=\"T_1ea9c_row14_col0\" class=\"data row14 col0\" >0.669300</td>\n",
       "      <td id=\"T_1ea9c_row14_col1\" class=\"data row14 col1\" >0.663600</td>\n",
       "      <td id=\"T_1ea9c_row14_col2\" class=\"data row14 col2\" >0.832000</td>\n",
       "      <td id=\"T_1ea9c_row14_col3\" class=\"data row14 col3\" >0.510400</td>\n",
       "      <td id=\"T_1ea9c_row14_col4\" class=\"data row14 col4\" >0.506300</td>\n",
       "      <td id=\"T_1ea9c_row14_col5\" class=\"data row14 col5\" >0.719500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row15\" class=\"row_heading level0 row15\" >LR_ohe_tuned</th>\n",
       "      <td id=\"T_1ea9c_row15_col0\" class=\"data row15 col0\" >0.645800</td>\n",
       "      <td id=\"T_1ea9c_row15_col1\" class=\"data row15 col1\" >0.636500</td>\n",
       "      <td id=\"T_1ea9c_row15_col2\" class=\"data row15 col2\" >0.834500</td>\n",
       "      <td id=\"T_1ea9c_row15_col3\" class=\"data row15 col3\" >0.500000</td>\n",
       "      <td id=\"T_1ea9c_row15_col4\" class=\"data row15 col4\" >0.505500</td>\n",
       "      <td id=\"T_1ea9c_row15_col5\" class=\"data row15 col5\" >0.704100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row16\" class=\"row_heading level0 row16\" >LR_glmm</th>\n",
       "      <td id=\"T_1ea9c_row16_col0\" class=\"data row16 col0\" >0.609400</td>\n",
       "      <td id=\"T_1ea9c_row16_col1\" class=\"data row16 col1\" >0.607500</td>\n",
       "      <td id=\"T_1ea9c_row16_col2\" class=\"data row16 col2\" >0.805600</td>\n",
       "      <td id=\"T_1ea9c_row16_col3\" class=\"data row16 col3\" >0.500000</td>\n",
       "      <td id=\"T_1ea9c_row16_col4\" class=\"data row16 col4\" >0.499700</td>\n",
       "      <td id=\"T_1ea9c_row16_col5\" class=\"data row16 col5\" >0.716500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row17\" class=\"row_heading level0 row17\" >LR_glmm_tuned</th>\n",
       "      <td id=\"T_1ea9c_row17_col0\" class=\"data row17 col0\" >0.601600</td>\n",
       "      <td id=\"T_1ea9c_row17_col1\" class=\"data row17 col1\" >0.599100</td>\n",
       "      <td id=\"T_1ea9c_row17_col2\" class=\"data row17 col2\" >0.804500</td>\n",
       "      <td id=\"T_1ea9c_row17_col3\" class=\"data row17 col3\" >0.500000</td>\n",
       "      <td id=\"T_1ea9c_row17_col4\" class=\"data row17 col4\" >0.499700</td>\n",
       "      <td id=\"T_1ea9c_row17_col5\" class=\"data row17 col5\" >0.716100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row18\" class=\"row_heading level0 row18\" >LR_ohe</th>\n",
       "      <td id=\"T_1ea9c_row18_col0\" class=\"data row18 col0\" >0.648400</td>\n",
       "      <td id=\"T_1ea9c_row18_col1\" class=\"data row18 col1\" >0.644100</td>\n",
       "      <td id=\"T_1ea9c_row18_col2\" class=\"data row18 col2\" >0.847600</td>\n",
       "      <td id=\"T_1ea9c_row18_col3\" class=\"data row18 col3\" >0.489600</td>\n",
       "      <td id=\"T_1ea9c_row18_col4\" class=\"data row18 col4\" >0.494600</td>\n",
       "      <td id=\"T_1ea9c_row18_col5\" class=\"data row18 col5\" >0.716400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row19\" class=\"row_heading level0 row19\" >XGB_catboost_tuned</th>\n",
       "      <td id=\"T_1ea9c_row19_col0\" class=\"data row19 col0\" >0.622400</td>\n",
       "      <td id=\"T_1ea9c_row19_col1\" class=\"data row19 col1\" >0.619300</td>\n",
       "      <td id=\"T_1ea9c_row19_col2\" class=\"data row19 col2\" >0.819900</td>\n",
       "      <td id=\"T_1ea9c_row19_col3\" class=\"data row19 col3\" >0.500000</td>\n",
       "      <td id=\"T_1ea9c_row19_col4\" class=\"data row19 col4\" >0.493400</td>\n",
       "      <td id=\"T_1ea9c_row19_col5\" class=\"data row19 col5\" >0.698300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row20\" class=\"row_heading level0 row20\" >LR_ordinal</th>\n",
       "      <td id=\"T_1ea9c_row20_col0\" class=\"data row20 col0\" >0.585900</td>\n",
       "      <td id=\"T_1ea9c_row20_col1\" class=\"data row20 col1\" >0.580900</td>\n",
       "      <td id=\"T_1ea9c_row20_col2\" class=\"data row20 col2\" >0.786800</td>\n",
       "      <td id=\"T_1ea9c_row20_col3\" class=\"data row20 col3\" >0.489600</td>\n",
       "      <td id=\"T_1ea9c_row20_col4\" class=\"data row20 col4\" >0.488700</td>\n",
       "      <td id=\"T_1ea9c_row20_col5\" class=\"data row20 col5\" >0.711600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row21\" class=\"row_heading level0 row21\" >XGB_target_tuned</th>\n",
       "      <td id=\"T_1ea9c_row21_col0\" class=\"data row21 col0\" >0.669300</td>\n",
       "      <td id=\"T_1ea9c_row21_col1\" class=\"data row21 col1\" >0.663300</td>\n",
       "      <td id=\"T_1ea9c_row21_col2\" class=\"data row21 col2\" >0.836000</td>\n",
       "      <td id=\"T_1ea9c_row21_col3\" class=\"data row21 col3\" >0.468800</td>\n",
       "      <td id=\"T_1ea9c_row21_col4\" class=\"data row21 col4\" >0.464100</td>\n",
       "      <td id=\"T_1ea9c_row21_col5\" class=\"data row21 col5\" >0.715900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row22\" class=\"row_heading level0 row22\" >XGB_glmm_tuned</th>\n",
       "      <td id=\"T_1ea9c_row22_col0\" class=\"data row22 col0\" >0.856800</td>\n",
       "      <td id=\"T_1ea9c_row22_col1\" class=\"data row22 col1\" >0.855500</td>\n",
       "      <td id=\"T_1ea9c_row22_col2\" class=\"data row22 col2\" >0.962000</td>\n",
       "      <td id=\"T_1ea9c_row22_col3\" class=\"data row22 col3\" >0.458300</td>\n",
       "      <td id=\"T_1ea9c_row22_col4\" class=\"data row22 col4\" >0.431200</td>\n",
       "      <td id=\"T_1ea9c_row22_col5\" class=\"data row22 col5\" >0.698600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row23\" class=\"row_heading level0 row23\" >XGB_ohe_tuned</th>\n",
       "      <td id=\"T_1ea9c_row23_col0\" class=\"data row23 col0\" >0.593800</td>\n",
       "      <td id=\"T_1ea9c_row23_col1\" class=\"data row23 col1\" >0.575300</td>\n",
       "      <td id=\"T_1ea9c_row23_col2\" class=\"data row23 col2\" >0.789200</td>\n",
       "      <td id=\"T_1ea9c_row23_col3\" class=\"data row23 col3\" >0.437500</td>\n",
       "      <td id=\"T_1ea9c_row23_col4\" class=\"data row23 col4\" >0.423500</td>\n",
       "      <td id=\"T_1ea9c_row23_col5\" class=\"data row23 col5\" >0.714900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1ea9c_level0_row24\" class=\"row_heading level0 row24\" >Baseline</th>\n",
       "      <td id=\"T_1ea9c_row24_col0\" class=\"data row24 col0\" >0.455700</td>\n",
       "      <td id=\"T_1ea9c_row24_col1\" class=\"data row24 col1\" >0.208700</td>\n",
       "      <td id=\"T_1ea9c_row24_col2\" class=\"data row24 col2\" >0.500000</td>\n",
       "      <td id=\"T_1ea9c_row24_col3\" class=\"data row24 col3\" >0.375000</td>\n",
       "      <td id=\"T_1ea9c_row24_col4\" class=\"data row24 col4\" >0.181800</td>\n",
       "      <td id=\"T_1ea9c_row24_col5\" class=\"data row24 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b81cc2b550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(f\"../results/{dataset_name}/{experiment_name}/results_encodings.pickle\"):\n",
    "\n",
    "    results_encodings = {}\n",
    "    results_encodings_feature_importances = {}\n",
    "\n",
    "    for fold in range(folds):\n",
    "        results_encodings[fold] = {}\n",
    "        results_encodings_feature_importances[fold] = {}\n",
    "        # Create baseline\n",
    "        y_train = data_dict[f\"y_train_{fold}\"]\n",
    "        y_val = data_dict[f\"y_val_{fold}\"]\n",
    "        y_test = data_dict[f\"y_test_{fold}\"]\n",
    "        y_train_val = np.concatenate([y_train,y_val])\n",
    "\n",
    "        u,c = np.unique(y_train_val,return_counts=True)\n",
    "        nb_classes = len(u)\n",
    "        baseline = np.argmax(c)\n",
    "\n",
    "        y_train_val_pred_base = np.ones(y_train_val.shape[0])*baseline\n",
    "        y_test_pred_base = np.ones(y_test.shape[0])*baseline\n",
    "\n",
    "        results_encodings[fold][\"Baseline\"] = {}\n",
    "        eval_res_train = get_metrics(get_one_hot(y_train_val, nb_classes), get_one_hot(y_train_val_pred_base.astype(int), nb_classes), target=target)\n",
    "        for metric in eval_res_train.keys():\n",
    "            results_encodings[fold][\"Baseline\"][metric + \" Train\"] = eval_res_train[metric]\n",
    "        eval_res_test = get_metrics(get_one_hot(y_test, nb_classes), get_one_hot(y_test_pred_base.astype(int), nb_classes), target=target)\n",
    "        for metric in eval_res_test.keys():\n",
    "            results_encodings[fold][\"Baseline\"][metric + \" Test\"] = eval_res_test[metric]\n",
    "\n",
    "\n",
    "        for condition in conditions:\n",
    "            print(f\"Preparing results for fold {fold}, condition={condition}\")\n",
    "            # Retrieve data\n",
    "            z_cols = data_dict[\"z_cols\"]\n",
    "\n",
    "            X_train = data_dict[f\"X_train_{fold}\"]\n",
    "            y_train = data_dict[f\"y_train_{fold}\"]\n",
    "\n",
    "            X_val = data_dict[f\"X_val_{fold}\"]\n",
    "            y_val = data_dict[f\"y_val_{fold}\"]\n",
    "\n",
    "            X_test = data_dict[f\"X_test_{fold}\"]\n",
    "            y_test = data_dict[f\"y_test_{fold}\"]\n",
    "\n",
    "    ## ALL BUT PERFORMANCE:\n",
    "            # Define data subset for evaluation\n",
    "    #         X_train = X_train[[i for i in X_train.columns if all([j not in i for j in perf_cols])]]\n",
    "    #         X_val = X_val[[i for i in X_val.columns if all([j not in i for j in perf_cols])]]\n",
    "    #         X_test = X_test[[i for i in X_test.columns if all([j not in i for j in perf_cols])]]\n",
    "\n",
    "            # Define condition data subset\n",
    "    #         if condition != \"ignore\":\n",
    "    #             z_encoded_train = data_dict[f\"z_{condition}_encoded_train_{fold}\"] \n",
    "    #             z_encoded_val = data_dict[f\"z_{condition}_encoded_val_{fold}\"] \n",
    "    #             z_encoded_test = data_dict[f\"z_{condition}_encoded_test_{fold}\"] \n",
    "\n",
    "    #             X_train = pd.concat([X_train,z_encoded_train],axis=1)\n",
    "    #             X_val = pd.concat([X_val,z_encoded_val],axis=1)\n",
    "    #             X_test = pd.concat([X_test,z_encoded_test],axis=1)\n",
    "\n",
    "    # ALL BUT PERFORMANCE & ACTIVITY:\n",
    "    #         Define data subset for evaluation\n",
    "            X_train = X_train[[i for i in X_train.columns if all([j not in i for j in perf_cols+activity_cols])]]\n",
    "            X_val = X_val[[i for i in X_val.columns if all([j not in i for j in perf_cols+activity_cols])]]\n",
    "            X_test = X_test[[i for i in X_test.columns if all([j not in i for j in perf_cols+activity_cols])]]\n",
    "\n",
    "    #         Define condition data subset\n",
    "            if condition != \"ignore\":\n",
    "                z_encoded_train = data_dict[f\"z_{condition}_encoded_train_{fold}\"] \n",
    "                z_encoded_val = data_dict[f\"z_{condition}_encoded_val_{fold}\"] \n",
    "                z_encoded_test = data_dict[f\"z_{condition}_encoded_test_{fold}\"] \n",
    "\n",
    "                X_train = pd.concat([X_train,z_encoded_train],axis=1)\n",
    "                X_val = pd.concat([X_val,z_encoded_val],axis=1)\n",
    "                X_test = pd.concat([X_test,z_encoded_test],axis=1)\n",
    "\n",
    "\n",
    "    ## ONLY CATEGORICAL: --> Produces trash as almost never better than baseline\n",
    "    #         if condition != \"ignore\":        \n",
    "    #             X_train = data_dict[f\"z_{condition}_encoded_train_{fold}\"] \n",
    "    #             X_val = data_dict[f\"z_{condition}_encoded_val_{fold}\"] \n",
    "    #             X_test = data_dict[f\"z_{condition}_encoded_test_{fold}\"] \n",
    "\n",
    "    #         else:\n",
    "    #             continue\n",
    "\n",
    "            X_train_val = pd.concat([X_train,X_val])\n",
    "            y_train_val = np.concatenate([y_train,y_val])\n",
    "\n",
    "            # Train base models\n",
    "            res, feats = evaluate_logreg(X_train_val, y_train_val, X_test, y_test, target=target,tune=False, seed=RS)\n",
    "            results_encodings[fold][\"LR_\"+condition] = res\n",
    "            results_encodings_feature_importances[fold][\"LR_\"+condition] = feats\n",
    "\n",
    "            res, feats = evaluate_xgb(X_train_val, y_train_val, X_test, y_test, target, tune=False, max_evals=max_evals, early_stopping_rounds=early_stopping_rounds, seed=RS)\n",
    "            results_encodings[fold][\"XGB_\"+condition] = res\n",
    "            results_encodings_feature_importances[fold][\"XGB_\"+condition] = feats\n",
    "\n",
    "            # Train tuned models\n",
    "            res, feats = evaluate_logreg(X_train_val, y_train_val, X_test, y_test, target=target, max_evals=max_evals, tune=True, seed=RS)\n",
    "            results_encodings[fold][\"LR_\"+condition+\"_tuned\"] = res\n",
    "            results_encodings_feature_importances[fold][\"LR_\"+condition+\"_tuned\"] = feats\n",
    "\n",
    "            res, feats = evaluate_xgb(X_train_val, y_train_val, X_test, y_test, target, tune=True, max_evals=max_evals, early_stopping_rounds=early_stopping_rounds, seed=RS)\n",
    "            results_encodings[fold][\"XGB_\"+condition+\"_tuned\"] = res\n",
    "            results_encodings_feature_importances[fold][\"XGB_\"+condition+\"_tuned\"] = feats\n",
    "    \n",
    "    if not os.path.exists(f\"../results/{dataset_name}/{experiment_name}\"):\n",
    "        os.makedirs(f\"../results/{dataset_name}/{experiment_name}\")\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_encodings.pickle\", 'wb') as handle:\n",
    "        pickle.dump(results_encodings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_encodings_feature_importances.pickle\", 'wb') as handle:\n",
    "        pickle.dump(results_encodings_feature_importances, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "else:\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_encodings.pickle\", 'rb') as handle:\n",
    "        results_encodings = pickle.load(handle)\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_encodings_feature_importances.pickle\", 'rb') as handle:\n",
    "        results_encodings_feature_importances = pickle.load(handle)\n",
    "        \n",
    "        \n",
    "results_encodings_df = pd.DataFrame(results_encodings[0]).transpose().sort_values(\"F1 Test\",ascending=False).round(4)\n",
    "results_encodings_df[[\"Accuracy Train\", \"F1 Train\", \"AUROC Train\", \"Accuracy Test\", \"F1 Test\", \"AUROC Test\"]].style.highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07ab0956",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_af6b0_row0_col3, #T_af6b0_row0_col4, #T_af6b0_row4_col5, #T_af6b0_row11_col0, #T_af6b0_row11_col1, #T_af6b0_row11_col2 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_af6b0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_af6b0_level0_col0\" class=\"col_heading level0 col0\" >Accuracy Train</th>\n",
       "      <th id=\"T_af6b0_level0_col1\" class=\"col_heading level0 col1\" >F1 Train</th>\n",
       "      <th id=\"T_af6b0_level0_col2\" class=\"col_heading level0 col2\" >AUROC Train</th>\n",
       "      <th id=\"T_af6b0_level0_col3\" class=\"col_heading level0 col3\" >Accuracy Test</th>\n",
       "      <th id=\"T_af6b0_level0_col4\" class=\"col_heading level0 col4\" >F1 Test</th>\n",
       "      <th id=\"T_af6b0_level0_col5\" class=\"col_heading level0 col5\" >AUROC Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row0\" class=\"row_heading level0 row0\" >XGB_ohe</th>\n",
       "      <td id=\"T_af6b0_row0_col0\" class=\"data row0 col0\" >0.916700</td>\n",
       "      <td id=\"T_af6b0_row0_col1\" class=\"data row0 col1\" >0.917500</td>\n",
       "      <td id=\"T_af6b0_row0_col2\" class=\"data row0 col2\" >0.991900</td>\n",
       "      <td id=\"T_af6b0_row0_col3\" class=\"data row0 col3\" >0.614600</td>\n",
       "      <td id=\"T_af6b0_row0_col4\" class=\"data row0 col4\" >0.612700</td>\n",
       "      <td id=\"T_af6b0_row0_col5\" class=\"data row0 col5\" >0.791400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row1\" class=\"row_heading level0 row1\" >XGB_ohe_tuned</th>\n",
       "      <td id=\"T_af6b0_row1_col0\" class=\"data row1 col0\" >0.601600</td>\n",
       "      <td id=\"T_af6b0_row1_col1\" class=\"data row1 col1\" >0.590700</td>\n",
       "      <td id=\"T_af6b0_row1_col2\" class=\"data row1 col2\" >0.774500</td>\n",
       "      <td id=\"T_af6b0_row1_col3\" class=\"data row1 col3\" >0.604200</td>\n",
       "      <td id=\"T_af6b0_row1_col4\" class=\"data row1 col4\" >0.596700</td>\n",
       "      <td id=\"T_af6b0_row1_col5\" class=\"data row1 col5\" >0.797300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row2\" class=\"row_heading level0 row2\" >XGB_glmm</th>\n",
       "      <td id=\"T_af6b0_row2_col0\" class=\"data row2 col0\" >0.976600</td>\n",
       "      <td id=\"T_af6b0_row2_col1\" class=\"data row2 col1\" >0.976500</td>\n",
       "      <td id=\"T_af6b0_row2_col2\" class=\"data row2 col2\" >0.999400</td>\n",
       "      <td id=\"T_af6b0_row2_col3\" class=\"data row2 col3\" >0.593800</td>\n",
       "      <td id=\"T_af6b0_row2_col4\" class=\"data row2 col4\" >0.580600</td>\n",
       "      <td id=\"T_af6b0_row2_col5\" class=\"data row2 col5\" >0.750900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row3\" class=\"row_heading level0 row3\" >LR_ohe_tuned</th>\n",
       "      <td id=\"T_af6b0_row3_col0\" class=\"data row3 col0\" >0.632800</td>\n",
       "      <td id=\"T_af6b0_row3_col1\" class=\"data row3 col1\" >0.633900</td>\n",
       "      <td id=\"T_af6b0_row3_col2\" class=\"data row3 col2\" >0.830600</td>\n",
       "      <td id=\"T_af6b0_row3_col3\" class=\"data row3 col3\" >0.583300</td>\n",
       "      <td id=\"T_af6b0_row3_col4\" class=\"data row3 col4\" >0.578900</td>\n",
       "      <td id=\"T_af6b0_row3_col5\" class=\"data row3 col5\" >0.769900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row4\" class=\"row_heading level0 row4\" >XGB_glmm_tuned</th>\n",
       "      <td id=\"T_af6b0_row4_col0\" class=\"data row4 col0\" >0.731800</td>\n",
       "      <td id=\"T_af6b0_row4_col1\" class=\"data row4 col1\" >0.732400</td>\n",
       "      <td id=\"T_af6b0_row4_col2\" class=\"data row4 col2\" >0.883700</td>\n",
       "      <td id=\"T_af6b0_row4_col3\" class=\"data row4 col3\" >0.583300</td>\n",
       "      <td id=\"T_af6b0_row4_col4\" class=\"data row4 col4\" >0.574400</td>\n",
       "      <td id=\"T_af6b0_row4_col5\" class=\"data row4 col5\" >0.805500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row5\" class=\"row_heading level0 row5\" >XGB_ignore</th>\n",
       "      <td id=\"T_af6b0_row5_col0\" class=\"data row5 col0\" >0.700500</td>\n",
       "      <td id=\"T_af6b0_row5_col1\" class=\"data row5 col1\" >0.699500</td>\n",
       "      <td id=\"T_af6b0_row5_col2\" class=\"data row5 col2\" >0.886400</td>\n",
       "      <td id=\"T_af6b0_row5_col3\" class=\"data row5 col3\" >0.583300</td>\n",
       "      <td id=\"T_af6b0_row5_col4\" class=\"data row5 col4\" >0.570400</td>\n",
       "      <td id=\"T_af6b0_row5_col5\" class=\"data row5 col5\" >0.720400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row6\" class=\"row_heading level0 row6\" >XGB_target</th>\n",
       "      <td id=\"T_af6b0_row6_col0\" class=\"data row6 col0\" >0.919300</td>\n",
       "      <td id=\"T_af6b0_row6_col1\" class=\"data row6 col1\" >0.919900</td>\n",
       "      <td id=\"T_af6b0_row6_col2\" class=\"data row6 col2\" >0.992300</td>\n",
       "      <td id=\"T_af6b0_row6_col3\" class=\"data row6 col3\" >0.572900</td>\n",
       "      <td id=\"T_af6b0_row6_col4\" class=\"data row6 col4\" >0.566100</td>\n",
       "      <td id=\"T_af6b0_row6_col5\" class=\"data row6 col5\" >0.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row7\" class=\"row_heading level0 row7\" >LR_catboost</th>\n",
       "      <td id=\"T_af6b0_row7_col0\" class=\"data row7 col0\" >0.578100</td>\n",
       "      <td id=\"T_af6b0_row7_col1\" class=\"data row7 col1\" >0.580600</td>\n",
       "      <td id=\"T_af6b0_row7_col2\" class=\"data row7 col2\" >0.776200</td>\n",
       "      <td id=\"T_af6b0_row7_col3\" class=\"data row7 col3\" >0.562500</td>\n",
       "      <td id=\"T_af6b0_row7_col4\" class=\"data row7 col4\" >0.564500</td>\n",
       "      <td id=\"T_af6b0_row7_col5\" class=\"data row7 col5\" >0.763800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row8\" class=\"row_heading level0 row8\" >LR_ohe</th>\n",
       "      <td id=\"T_af6b0_row8_col0\" class=\"data row8 col0\" >0.658900</td>\n",
       "      <td id=\"T_af6b0_row8_col1\" class=\"data row8 col1\" >0.662300</td>\n",
       "      <td id=\"T_af6b0_row8_col2\" class=\"data row8 col2\" >0.835800</td>\n",
       "      <td id=\"T_af6b0_row8_col3\" class=\"data row8 col3\" >0.562500</td>\n",
       "      <td id=\"T_af6b0_row8_col4\" class=\"data row8 col4\" >0.560100</td>\n",
       "      <td id=\"T_af6b0_row8_col5\" class=\"data row8 col5\" >0.766100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row9\" class=\"row_heading level0 row9\" >LR_ignore_tuned</th>\n",
       "      <td id=\"T_af6b0_row9_col0\" class=\"data row9 col0\" >0.599000</td>\n",
       "      <td id=\"T_af6b0_row9_col1\" class=\"data row9 col1\" >0.602600</td>\n",
       "      <td id=\"T_af6b0_row9_col2\" class=\"data row9 col2\" >0.765200</td>\n",
       "      <td id=\"T_af6b0_row9_col3\" class=\"data row9 col3\" >0.552100</td>\n",
       "      <td id=\"T_af6b0_row9_col4\" class=\"data row9 col4\" >0.559000</td>\n",
       "      <td id=\"T_af6b0_row9_col5\" class=\"data row9 col5\" >0.765700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row10\" class=\"row_heading level0 row10\" >LR_catboost_tuned</th>\n",
       "      <td id=\"T_af6b0_row10_col0\" class=\"data row10 col0\" >0.591100</td>\n",
       "      <td id=\"T_af6b0_row10_col1\" class=\"data row10 col1\" >0.593500</td>\n",
       "      <td id=\"T_af6b0_row10_col2\" class=\"data row10 col2\" >0.775900</td>\n",
       "      <td id=\"T_af6b0_row10_col3\" class=\"data row10 col3\" >0.552100</td>\n",
       "      <td id=\"T_af6b0_row10_col4\" class=\"data row10 col4\" >0.555400</td>\n",
       "      <td id=\"T_af6b0_row10_col5\" class=\"data row10 col5\" >0.767200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row11\" class=\"row_heading level0 row11\" >XGB_catboost</th>\n",
       "      <td id=\"T_af6b0_row11_col0\" class=\"data row11 col0\" >1.000000</td>\n",
       "      <td id=\"T_af6b0_row11_col1\" class=\"data row11 col1\" >1.000000</td>\n",
       "      <td id=\"T_af6b0_row11_col2\" class=\"data row11 col2\" >1.000000</td>\n",
       "      <td id=\"T_af6b0_row11_col3\" class=\"data row11 col3\" >0.552100</td>\n",
       "      <td id=\"T_af6b0_row11_col4\" class=\"data row11 col4\" >0.553600</td>\n",
       "      <td id=\"T_af6b0_row11_col5\" class=\"data row11 col5\" >0.717900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row12\" class=\"row_heading level0 row12\" >XGB_target_tuned</th>\n",
       "      <td id=\"T_af6b0_row12_col0\" class=\"data row12 col0\" >0.570300</td>\n",
       "      <td id=\"T_af6b0_row12_col1\" class=\"data row12 col1\" >0.567600</td>\n",
       "      <td id=\"T_af6b0_row12_col2\" class=\"data row12 col2\" >0.777500</td>\n",
       "      <td id=\"T_af6b0_row12_col3\" class=\"data row12 col3\" >0.552100</td>\n",
       "      <td id=\"T_af6b0_row12_col4\" class=\"data row12 col4\" >0.552100</td>\n",
       "      <td id=\"T_af6b0_row12_col5\" class=\"data row12 col5\" >0.772800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row13\" class=\"row_heading level0 row13\" >XGB_ordinal_tuned</th>\n",
       "      <td id=\"T_af6b0_row13_col0\" class=\"data row13 col0\" >0.588500</td>\n",
       "      <td id=\"T_af6b0_row13_col1\" class=\"data row13 col1\" >0.592900</td>\n",
       "      <td id=\"T_af6b0_row13_col2\" class=\"data row13 col2\" >0.761800</td>\n",
       "      <td id=\"T_af6b0_row13_col3\" class=\"data row13 col3\" >0.541700</td>\n",
       "      <td id=\"T_af6b0_row13_col4\" class=\"data row13 col4\" >0.551900</td>\n",
       "      <td id=\"T_af6b0_row13_col5\" class=\"data row13 col5\" >0.749500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row14\" class=\"row_heading level0 row14\" >LR_ignore</th>\n",
       "      <td id=\"T_af6b0_row14_col0\" class=\"data row14 col0\" >0.599000</td>\n",
       "      <td id=\"T_af6b0_row14_col1\" class=\"data row14 col1\" >0.601900</td>\n",
       "      <td id=\"T_af6b0_row14_col2\" class=\"data row14 col2\" >0.765500</td>\n",
       "      <td id=\"T_af6b0_row14_col3\" class=\"data row14 col3\" >0.541700</td>\n",
       "      <td id=\"T_af6b0_row14_col4\" class=\"data row14 col4\" >0.546900</td>\n",
       "      <td id=\"T_af6b0_row14_col5\" class=\"data row14 col5\" >0.765900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row15\" class=\"row_heading level0 row15\" >XGB_ordinal</th>\n",
       "      <td id=\"T_af6b0_row15_col0\" class=\"data row15 col0\" >0.919300</td>\n",
       "      <td id=\"T_af6b0_row15_col1\" class=\"data row15 col1\" >0.919800</td>\n",
       "      <td id=\"T_af6b0_row15_col2\" class=\"data row15 col2\" >0.992500</td>\n",
       "      <td id=\"T_af6b0_row15_col3\" class=\"data row15 col3\" >0.541700</td>\n",
       "      <td id=\"T_af6b0_row15_col4\" class=\"data row15 col4\" >0.542700</td>\n",
       "      <td id=\"T_af6b0_row15_col5\" class=\"data row15 col5\" >0.773800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row16\" class=\"row_heading level0 row16\" >XGB_ignore_tuned</th>\n",
       "      <td id=\"T_af6b0_row16_col0\" class=\"data row16 col0\" >0.601600</td>\n",
       "      <td id=\"T_af6b0_row16_col1\" class=\"data row16 col1\" >0.604500</td>\n",
       "      <td id=\"T_af6b0_row16_col2\" class=\"data row16 col2\" >0.765500</td>\n",
       "      <td id=\"T_af6b0_row16_col3\" class=\"data row16 col3\" >0.531200</td>\n",
       "      <td id=\"T_af6b0_row16_col4\" class=\"data row16 col4\" >0.541200</td>\n",
       "      <td id=\"T_af6b0_row16_col5\" class=\"data row16 col5\" >0.766900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row17\" class=\"row_heading level0 row17\" >LR_glmm_tuned</th>\n",
       "      <td id=\"T_af6b0_row17_col0\" class=\"data row17 col0\" >0.580700</td>\n",
       "      <td id=\"T_af6b0_row17_col1\" class=\"data row17 col1\" >0.585100</td>\n",
       "      <td id=\"T_af6b0_row17_col2\" class=\"data row17 col2\" >0.787900</td>\n",
       "      <td id=\"T_af6b0_row17_col3\" class=\"data row17 col3\" >0.531200</td>\n",
       "      <td id=\"T_af6b0_row17_col4\" class=\"data row17 col4\" >0.535000</td>\n",
       "      <td id=\"T_af6b0_row17_col5\" class=\"data row17 col5\" >0.768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row18\" class=\"row_heading level0 row18\" >LR_target</th>\n",
       "      <td id=\"T_af6b0_row18_col0\" class=\"data row18 col0\" >0.609400</td>\n",
       "      <td id=\"T_af6b0_row18_col1\" class=\"data row18 col1\" >0.609100</td>\n",
       "      <td id=\"T_af6b0_row18_col2\" class=\"data row18 col2\" >0.792500</td>\n",
       "      <td id=\"T_af6b0_row18_col3\" class=\"data row18 col3\" >0.531200</td>\n",
       "      <td id=\"T_af6b0_row18_col4\" class=\"data row18 col4\" >0.532300</td>\n",
       "      <td id=\"T_af6b0_row18_col5\" class=\"data row18 col5\" >0.746600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row19\" class=\"row_heading level0 row19\" >LR_target_tuned</th>\n",
       "      <td id=\"T_af6b0_row19_col0\" class=\"data row19 col0\" >0.612000</td>\n",
       "      <td id=\"T_af6b0_row19_col1\" class=\"data row19 col1\" >0.611200</td>\n",
       "      <td id=\"T_af6b0_row19_col2\" class=\"data row19 col2\" >0.791900</td>\n",
       "      <td id=\"T_af6b0_row19_col3\" class=\"data row19 col3\" >0.520800</td>\n",
       "      <td id=\"T_af6b0_row19_col4\" class=\"data row19 col4\" >0.523400</td>\n",
       "      <td id=\"T_af6b0_row19_col5\" class=\"data row19 col5\" >0.748300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row20\" class=\"row_heading level0 row20\" >LR_ordinal</th>\n",
       "      <td id=\"T_af6b0_row20_col0\" class=\"data row20 col0\" >0.591100</td>\n",
       "      <td id=\"T_af6b0_row20_col1\" class=\"data row20 col1\" >0.591200</td>\n",
       "      <td id=\"T_af6b0_row20_col2\" class=\"data row20 col2\" >0.773400</td>\n",
       "      <td id=\"T_af6b0_row20_col3\" class=\"data row20 col3\" >0.520800</td>\n",
       "      <td id=\"T_af6b0_row20_col4\" class=\"data row20 col4\" >0.521400</td>\n",
       "      <td id=\"T_af6b0_row20_col5\" class=\"data row20 col5\" >0.739800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row21\" class=\"row_heading level0 row21\" >LR_ordinal_tuned</th>\n",
       "      <td id=\"T_af6b0_row21_col0\" class=\"data row21 col0\" >0.591100</td>\n",
       "      <td id=\"T_af6b0_row21_col1\" class=\"data row21 col1\" >0.590400</td>\n",
       "      <td id=\"T_af6b0_row21_col2\" class=\"data row21 col2\" >0.772600</td>\n",
       "      <td id=\"T_af6b0_row21_col3\" class=\"data row21 col3\" >0.520800</td>\n",
       "      <td id=\"T_af6b0_row21_col4\" class=\"data row21 col4\" >0.521400</td>\n",
       "      <td id=\"T_af6b0_row21_col5\" class=\"data row21 col5\" >0.738700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row22\" class=\"row_heading level0 row22\" >LR_glmm</th>\n",
       "      <td id=\"T_af6b0_row22_col0\" class=\"data row22 col0\" >0.585900</td>\n",
       "      <td id=\"T_af6b0_row22_col1\" class=\"data row22 col1\" >0.586400</td>\n",
       "      <td id=\"T_af6b0_row22_col2\" class=\"data row22 col2\" >0.792600</td>\n",
       "      <td id=\"T_af6b0_row22_col3\" class=\"data row22 col3\" >0.510400</td>\n",
       "      <td id=\"T_af6b0_row22_col4\" class=\"data row22 col4\" >0.509900</td>\n",
       "      <td id=\"T_af6b0_row22_col5\" class=\"data row22 col5\" >0.765100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row23\" class=\"row_heading level0 row23\" >XGB_catboost_tuned</th>\n",
       "      <td id=\"T_af6b0_row23_col0\" class=\"data row23 col0\" >0.489600</td>\n",
       "      <td id=\"T_af6b0_row23_col1\" class=\"data row23 col1\" >0.365200</td>\n",
       "      <td id=\"T_af6b0_row23_col2\" class=\"data row23 col2\" >0.717800</td>\n",
       "      <td id=\"T_af6b0_row23_col3\" class=\"data row23 col3\" >0.479200</td>\n",
       "      <td id=\"T_af6b0_row23_col4\" class=\"data row23 col4\" >0.375700</td>\n",
       "      <td id=\"T_af6b0_row23_col5\" class=\"data row23 col5\" >0.729800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_af6b0_level0_row24\" class=\"row_heading level0 row24\" >Baseline</th>\n",
       "      <td id=\"T_af6b0_row24_col0\" class=\"data row24 col0\" >0.437500</td>\n",
       "      <td id=\"T_af6b0_row24_col1\" class=\"data row24 col1\" >0.202900</td>\n",
       "      <td id=\"T_af6b0_row24_col2\" class=\"data row24 col2\" >0.500000</td>\n",
       "      <td id=\"T_af6b0_row24_col3\" class=\"data row24 col3\" >0.447900</td>\n",
       "      <td id=\"T_af6b0_row24_col4\" class=\"data row24 col4\" >0.206200</td>\n",
       "      <td id=\"T_af6b0_row24_col5\" class=\"data row24 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b80841aee0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_encodings_df = pd.DataFrame(results_encodings[1]).transpose().sort_values(\"F1 Test\",ascending=False).round(4)\n",
    "results_encodings_df[[\"Accuracy Train\", \"F1 Train\", \"AUROC Train\", \"Accuracy Test\", \"F1 Test\", \"AUROC Test\"]].style.highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c0bf64",
   "metadata": {},
   "source": [
    "### Effectiveness of Parameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a7e1b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_823e7_row0_col0, #T_823e7_row0_col1, #T_823e7_row1_col0, #T_823e7_row2_col0, #T_823e7_row2_col1, #T_823e7_row3_col1, #T_823e7_row4_col0, #T_823e7_row4_col1, #T_823e7_row5_col0, #T_823e7_row6_col1, #T_823e7_row7_col0, #T_823e7_row7_col1, #T_823e7_row8_col0, #T_823e7_row8_col1, #T_823e7_row9_col1, #T_823e7_row10_col0, #T_823e7_row11_col0, #T_823e7_row12_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_823e7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_823e7_level0_col0\" class=\"col_heading level0 col0\" >Untuned</th>\n",
       "      <th id=\"T_823e7_level0_col1\" class=\"col_heading level0 col1\" >Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_823e7_level0_row0\" class=\"row_heading level0 row0\" >Baseline</th>\n",
       "      <td id=\"T_823e7_row0_col0\" class=\"data row0 col0\" >0.200000</td>\n",
       "      <td id=\"T_823e7_row0_col1\" class=\"data row0 col1\" >0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_823e7_level0_row1\" class=\"row_heading level0 row1\" >LR_catboost</th>\n",
       "      <td id=\"T_823e7_row1_col0\" class=\"data row1 col0\" >0.550000</td>\n",
       "      <td id=\"T_823e7_row1_col1\" class=\"data row1 col1\" >0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_823e7_level0_row2\" class=\"row_heading level0 row2\" >LR_glmm</th>\n",
       "      <td id=\"T_823e7_row2_col0\" class=\"data row2 col0\" >0.530000</td>\n",
       "      <td id=\"T_823e7_row2_col1\" class=\"data row2 col1\" >0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_823e7_level0_row3\" class=\"row_heading level0 row3\" >LR_ignore</th>\n",
       "      <td id=\"T_823e7_row3_col0\" class=\"data row3 col0\" >0.540000</td>\n",
       "      <td id=\"T_823e7_row3_col1\" class=\"data row3 col1\" >0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_823e7_level0_row4\" class=\"row_heading level0 row4\" >LR_ohe</th>\n",
       "      <td id=\"T_823e7_row4_col0\" class=\"data row4 col0\" >0.560000</td>\n",
       "      <td id=\"T_823e7_row4_col1\" class=\"data row4 col1\" >0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_823e7_level0_row5\" class=\"row_heading level0 row5\" >LR_ordinal</th>\n",
       "      <td id=\"T_823e7_row5_col0\" class=\"data row5 col0\" >0.530000</td>\n",
       "      <td id=\"T_823e7_row5_col1\" class=\"data row5 col1\" >0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_823e7_level0_row6\" class=\"row_heading level0 row6\" >LR_target</th>\n",
       "      <td id=\"T_823e7_row6_col0\" class=\"data row6 col0\" >0.530000</td>\n",
       "      <td id=\"T_823e7_row6_col1\" class=\"data row6 col1\" >0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_823e7_level0_row7\" class=\"row_heading level0 row7\" >XGB_catboost</th>\n",
       "      <td id=\"T_823e7_row7_col0\" class=\"data row7 col0\" >0.510000</td>\n",
       "      <td id=\"T_823e7_row7_col1\" class=\"data row7 col1\" >0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_823e7_level0_row8\" class=\"row_heading level0 row8\" >XGB_glmm</th>\n",
       "      <td id=\"T_823e7_row8_col0\" class=\"data row8 col0\" >0.550000</td>\n",
       "      <td id=\"T_823e7_row8_col1\" class=\"data row8 col1\" >0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_823e7_level0_row9\" class=\"row_heading level0 row9\" >XGB_ignore</th>\n",
       "      <td id=\"T_823e7_row9_col0\" class=\"data row9 col0\" >0.530000</td>\n",
       "      <td id=\"T_823e7_row9_col1\" class=\"data row9 col1\" >0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_823e7_level0_row10\" class=\"row_heading level0 row10\" >XGB_ohe</th>\n",
       "      <td id=\"T_823e7_row10_col0\" class=\"data row10 col0\" >0.600000</td>\n",
       "      <td id=\"T_823e7_row10_col1\" class=\"data row10 col1\" >0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_823e7_level0_row11\" class=\"row_heading level0 row11\" >XGB_ordinal</th>\n",
       "      <td id=\"T_823e7_row11_col0\" class=\"data row11 col0\" >0.580000</td>\n",
       "      <td id=\"T_823e7_row11_col1\" class=\"data row11 col1\" >0.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_823e7_level0_row12\" class=\"row_heading level0 row12\" >XGB_target</th>\n",
       "      <td id=\"T_823e7_row12_col0\" class=\"data row12 col0\" >0.600000</td>\n",
       "      <td id=\"T_823e7_row12_col1\" class=\"data row12 col1\" >0.570000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b8086adc70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = results_encodings[0].keys()\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "encodings_folds_df = pd.DataFrame([pd.DataFrame(results_encodings[fold_num]).loc[metric,models] for fold_num in results_encodings.keys()],index=results_encodings.keys())\n",
    "encodings_mean_df = encodings_folds_df.mean(axis=0)\n",
    "encodings_std_df = encodings_folds_df.std(axis=0)\n",
    "\n",
    "methods = sorted(list(encodings_mean_df.index))[1:]\n",
    "not_tuned = [\"Baseline\"]+methods[::2]\n",
    "tuned = [\"Baseline\"]+methods[1::2]\n",
    "\n",
    "res_df_tune_comp_mean = pd.DataFrame([encodings_mean_df.loc[not_tuned].values,encodings_mean_df.loc[tuned]],index=[\"Untuned\",\"Tuned\"],columns=not_tuned).transpose()\n",
    "res_df_tune_comp_std = pd.DataFrame([encodings_std_df.loc[not_tuned].values,encodings_std_df.loc[tuned]],index=[\"Untuned\",\"Tuned\"],columns=not_tuned).transpose()\n",
    "\n",
    "res_df_tune_comp_mean.round(2).style.highlight_max(color = 'lightgreen', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "344961c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a975d_row0_col0, #T_a975d_row0_col1, #T_a975d_row1_col0, #T_a975d_row2_col0, #T_a975d_row2_col1, #T_a975d_row3_col1, #T_a975d_row4_col0, #T_a975d_row4_col1, #T_a975d_row5_col0, #T_a975d_row6_col1, #T_a975d_row7_col0, #T_a975d_row7_col1, #T_a975d_row8_col0, #T_a975d_row8_col1, #T_a975d_row9_col1, #T_a975d_row10_col0, #T_a975d_row11_col0, #T_a975d_row12_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a975d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a975d_level0_col0\" class=\"col_heading level0 col0\" >Untuned</th>\n",
       "      <th id=\"T_a975d_level0_col1\" class=\"col_heading level0 col1\" >Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a975d_level0_row0\" class=\"row_heading level0 row0\" >Baseline</th>\n",
       "      <td id=\"T_a975d_row0_col0\" class=\"data row0 col0\" >0.200000</td>\n",
       "      <td id=\"T_a975d_row0_col1\" class=\"data row0 col1\" >0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a975d_level0_row1\" class=\"row_heading level0 row1\" >LR_catboost</th>\n",
       "      <td id=\"T_a975d_row1_col0\" class=\"data row1 col0\" >0.550000</td>\n",
       "      <td id=\"T_a975d_row1_col1\" class=\"data row1 col1\" >0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a975d_level0_row2\" class=\"row_heading level0 row2\" >LR_glmm</th>\n",
       "      <td id=\"T_a975d_row2_col0\" class=\"data row2 col0\" >0.530000</td>\n",
       "      <td id=\"T_a975d_row2_col1\" class=\"data row2 col1\" >0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a975d_level0_row3\" class=\"row_heading level0 row3\" >LR_ignore</th>\n",
       "      <td id=\"T_a975d_row3_col0\" class=\"data row3 col0\" >0.540000</td>\n",
       "      <td id=\"T_a975d_row3_col1\" class=\"data row3 col1\" >0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a975d_level0_row4\" class=\"row_heading level0 row4\" >LR_ohe</th>\n",
       "      <td id=\"T_a975d_row4_col0\" class=\"data row4 col0\" >0.560000</td>\n",
       "      <td id=\"T_a975d_row4_col1\" class=\"data row4 col1\" >0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a975d_level0_row5\" class=\"row_heading level0 row5\" >LR_ordinal</th>\n",
       "      <td id=\"T_a975d_row5_col0\" class=\"data row5 col0\" >0.530000</td>\n",
       "      <td id=\"T_a975d_row5_col1\" class=\"data row5 col1\" >0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a975d_level0_row6\" class=\"row_heading level0 row6\" >LR_target</th>\n",
       "      <td id=\"T_a975d_row6_col0\" class=\"data row6 col0\" >0.530000</td>\n",
       "      <td id=\"T_a975d_row6_col1\" class=\"data row6 col1\" >0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a975d_level0_row7\" class=\"row_heading level0 row7\" >XGB_catboost</th>\n",
       "      <td id=\"T_a975d_row7_col0\" class=\"data row7 col0\" >0.510000</td>\n",
       "      <td id=\"T_a975d_row7_col1\" class=\"data row7 col1\" >0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a975d_level0_row8\" class=\"row_heading level0 row8\" >XGB_glmm</th>\n",
       "      <td id=\"T_a975d_row8_col0\" class=\"data row8 col0\" >0.550000</td>\n",
       "      <td id=\"T_a975d_row8_col1\" class=\"data row8 col1\" >0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a975d_level0_row9\" class=\"row_heading level0 row9\" >XGB_ignore</th>\n",
       "      <td id=\"T_a975d_row9_col0\" class=\"data row9 col0\" >0.530000</td>\n",
       "      <td id=\"T_a975d_row9_col1\" class=\"data row9 col1\" >0.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a975d_level0_row10\" class=\"row_heading level0 row10\" >XGB_ohe</th>\n",
       "      <td id=\"T_a975d_row10_col0\" class=\"data row10 col0\" >0.600000</td>\n",
       "      <td id=\"T_a975d_row10_col1\" class=\"data row10 col1\" >0.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a975d_level0_row11\" class=\"row_heading level0 row11\" >XGB_ordinal</th>\n",
       "      <td id=\"T_a975d_row11_col0\" class=\"data row11 col0\" >0.580000</td>\n",
       "      <td id=\"T_a975d_row11_col1\" class=\"data row11 col1\" >0.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a975d_level0_row12\" class=\"row_heading level0 row12\" >XGB_target</th>\n",
       "      <td id=\"T_a975d_row12_col0\" class=\"data row12 col0\" >0.600000</td>\n",
       "      <td id=\"T_a975d_row12_col1\" class=\"data row12 col1\" >0.570000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b808367e50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = results_encodings[0].keys()\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "encodings_folds_df = pd.DataFrame([pd.DataFrame(results_encodings[fold_num]).loc[metric,models] for fold_num in results_encodings.keys()],index=results_encodings.keys())\n",
    "encodings_mean_df = encodings_folds_df.mean(axis=0)\n",
    "encodings_std_df = encodings_folds_df.std(axis=0)\n",
    "\n",
    "methods = sorted(list(encodings_mean_df.index))[1:]\n",
    "not_tuned = [\"Baseline\"]+methods[::2]\n",
    "tuned = [\"Baseline\"]+methods[1::2]\n",
    "\n",
    "res_df_tune_comp_mean = pd.DataFrame([encodings_mean_df.loc[not_tuned].values,encodings_mean_df.loc[tuned]],index=[\"Untuned\",\"Tuned\"],columns=not_tuned).transpose()\n",
    "res_df_tune_comp_std = pd.DataFrame([encodings_std_df.loc[not_tuned].values,encodings_std_df.loc[tuned]],index=[\"Untuned\",\"Tuned\"],columns=not_tuned).transpose()\n",
    "\n",
    "res_df_tune_comp_mean.round(2).style.highlight_max(color = 'lightgreen', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21c8bf56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Untuned</th>\n",
       "      <th>Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.2 (0.016)</td>\n",
       "      <td>0.2 (0.016)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_catboost</th>\n",
       "      <td>0.55 (0.017)</td>\n",
       "      <td>0.54 (0.014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_glmm</th>\n",
       "      <td>0.53 (0.03)</td>\n",
       "      <td>0.53 (0.024)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_ignore</th>\n",
       "      <td>0.54 (0.025)</td>\n",
       "      <td>0.55 (0.028)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_ohe</th>\n",
       "      <td>0.56 (0.04)</td>\n",
       "      <td>0.56 (0.032)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_ordinal</th>\n",
       "      <td>0.53 (0.027)</td>\n",
       "      <td>0.52 (0.018)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_target</th>\n",
       "      <td>0.53 (0.008)</td>\n",
       "      <td>0.55 (0.034)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_catboost</th>\n",
       "      <td>0.51 (0.084)</td>\n",
       "      <td>0.51 (0.083)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_glmm</th>\n",
       "      <td>0.55 (0.028)</td>\n",
       "      <td>0.55 (0.068)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_ignore</th>\n",
       "      <td>0.53 (0.032)</td>\n",
       "      <td>0.54 (0.028)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_ohe</th>\n",
       "      <td>0.6 (0.026)</td>\n",
       "      <td>0.56 (0.079)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_ordinal</th>\n",
       "      <td>0.58 (0.052)</td>\n",
       "      <td>0.57 (0.047)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_target</th>\n",
       "      <td>0.6 (0.061)</td>\n",
       "      <td>0.57 (0.07)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Untuned         Tuned\n",
       "Baseline       0.2 (0.016)   0.2 (0.016)\n",
       "LR_catboost   0.55 (0.017)  0.54 (0.014)\n",
       "LR_glmm        0.53 (0.03)  0.53 (0.024)\n",
       "LR_ignore     0.54 (0.025)  0.55 (0.028)\n",
       "LR_ohe         0.56 (0.04)  0.56 (0.032)\n",
       "LR_ordinal    0.53 (0.027)  0.52 (0.018)\n",
       "LR_target     0.53 (0.008)  0.55 (0.034)\n",
       "XGB_catboost  0.51 (0.084)  0.51 (0.083)\n",
       "XGB_glmm      0.55 (0.028)  0.55 (0.068)\n",
       "XGB_ignore    0.53 (0.032)  0.54 (0.028)\n",
       "XGB_ohe        0.6 (0.026)  0.56 (0.079)\n",
       "XGB_ordinal   0.58 (0.052)  0.57 (0.047)\n",
       "XGB_target     0.6 (0.061)   0.57 (0.07)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_df = res_df_tune_comp_mean.round(2).astype(str) + \" (\" +  res_df_tune_comp_std.round(3).astype(str) + \")\"\n",
    "latex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1888cc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lll}\n",
      "\\toprule\n",
      "{} &       Untuned &         Tuned \\\\\n",
      "\\midrule\n",
      "Baseline     &   0.2 (0.016) &   0.2 (0.016) \\\\\n",
      "LR\\_catboost  &  0.55 (0.017) &  0.54 (0.014) \\\\\n",
      "LR\\_glmm      &   0.53 (0.03) &  0.53 (0.024) \\\\\n",
      "LR\\_ignore    &  0.54 (0.025) &  0.55 (0.028) \\\\\n",
      "LR\\_ohe       &   0.56 (0.04) &  0.56 (0.032) \\\\\n",
      "LR\\_ordinal   &  0.53 (0.027) &  0.52 (0.018) \\\\\n",
      "LR\\_target    &  0.53 (0.008) &  0.55 (0.034) \\\\\n",
      "XGB\\_catboost &  0.51 (0.084) &  0.51 (0.083) \\\\\n",
      "XGB\\_glmm     &  0.55 (0.028) &  0.55 (0.068) \\\\\n",
      "XGB\\_ignore   &  0.53 (0.032) &  0.54 (0.028) \\\\\n",
      "XGB\\_ohe      &   0.6 (0.026) &  0.56 (0.079) \\\\\n",
      "XGB\\_ordinal  &  0.58 (0.052) &  0.57 (0.047) \\\\\n",
      "XGB\\_target   &   0.6 (0.061) &   0.57 (0.07) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_df.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f3abd6",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f617d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_8a6eb_row0_col1, #T_8a6eb_row0_col2, #T_8a6eb_row0_col3, #T_8a6eb_row0_col4, #T_8a6eb_row0_col5, #T_8a6eb_row0_col6 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_8a6eb\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_8a6eb_level0_col0\" class=\"col_heading level0 col0\" >Baseline</th>\n",
       "      <th id=\"T_8a6eb_level0_col1\" class=\"col_heading level0 col1\" >LR_ignore_tuned</th>\n",
       "      <th id=\"T_8a6eb_level0_col2\" class=\"col_heading level0 col2\" >LR_ohe_tuned</th>\n",
       "      <th id=\"T_8a6eb_level0_col3\" class=\"col_heading level0 col3\" >LR_target_tuned</th>\n",
       "      <th id=\"T_8a6eb_level0_col4\" class=\"col_heading level0 col4\" >LR_ordinal_tuned</th>\n",
       "      <th id=\"T_8a6eb_level0_col5\" class=\"col_heading level0 col5\" >LR_catboost_tuned</th>\n",
       "      <th id=\"T_8a6eb_level0_col6\" class=\"col_heading level0 col6\" >LR_glmm_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8a6eb_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_8a6eb_row0_col0\" class=\"data row0 col0\" >0.203 (0.016)</td>\n",
       "      <td id=\"T_8a6eb_row0_col1\" class=\"data row0 col1\" >0.551 (0.028)</td>\n",
       "      <td id=\"T_8a6eb_row0_col2\" class=\"data row0 col2\" >0.558 (0.032)</td>\n",
       "      <td id=\"T_8a6eb_row0_col3\" class=\"data row0 col3\" >0.549 (0.034)</td>\n",
       "      <td id=\"T_8a6eb_row0_col4\" class=\"data row0 col4\" >0.522 (0.018)</td>\n",
       "      <td id=\"T_8a6eb_row0_col5\" class=\"data row0 col5\" >0.544 (0.014)</td>\n",
       "      <td id=\"T_8a6eb_row0_col6\" class=\"data row0 col6\" >0.534 (0.024)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b80858ceb0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For LR\n",
    "models = [\"Baseline\"]+[i for i in results_encodings[0].keys() if (\"tuned\" in i and \"LR\" in i)]\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "use_df = pd.DataFrame([pd.DataFrame(results_encodings[fold_num]).loc[metric,models] for fold_num in results_encodings.keys()],index=results_encodings.keys())\n",
    "\n",
    "df_mean = pd.DataFrame((use_df).mean(axis=0).round(3).astype(str) + \" (\" + use_df.std(axis=0).round(3).astype(str) + \")\").transpose()\n",
    "model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "\n",
    "best_model = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "t_test_res = np.array([stats.ttest_rel(use_df[best_model].values, use_df[model].values)[1] for model in models]).round(3)\n",
    "t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    \n",
    "res_df_lr = pd.DataFrame([model_dict])\n",
    "\n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_res[i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "res_df_lr.style.apply(negative_bold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea7d97c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fbb1a_row0_col1, #T_fbb1a_row0_col2, #T_fbb1a_row0_col3, #T_fbb1a_row0_col4, #T_fbb1a_row0_col5, #T_fbb1a_row0_col6 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fbb1a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fbb1a_level0_col0\" class=\"col_heading level0 col0\" >Baseline</th>\n",
       "      <th id=\"T_fbb1a_level0_col1\" class=\"col_heading level0 col1\" >XGB_ignore_tuned</th>\n",
       "      <th id=\"T_fbb1a_level0_col2\" class=\"col_heading level0 col2\" >XGB_ohe_tuned</th>\n",
       "      <th id=\"T_fbb1a_level0_col3\" class=\"col_heading level0 col3\" >XGB_target_tuned</th>\n",
       "      <th id=\"T_fbb1a_level0_col4\" class=\"col_heading level0 col4\" >XGB_ordinal_tuned</th>\n",
       "      <th id=\"T_fbb1a_level0_col5\" class=\"col_heading level0 col5\" >XGB_catboost_tuned</th>\n",
       "      <th id=\"T_fbb1a_level0_col6\" class=\"col_heading level0 col6\" >XGB_glmm_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fbb1a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_fbb1a_row0_col0\" class=\"data row0 col0\" >0.203 (0.016)</td>\n",
       "      <td id=\"T_fbb1a_row0_col1\" class=\"data row0 col1\" >0.541 (0.028)</td>\n",
       "      <td id=\"T_fbb1a_row0_col2\" class=\"data row0 col2\" >0.558 (0.079)</td>\n",
       "      <td id=\"T_fbb1a_row0_col3\" class=\"data row0 col3\" >0.565 (0.07)</td>\n",
       "      <td id=\"T_fbb1a_row0_col4\" class=\"data row0 col4\" >0.573 (0.047)</td>\n",
       "      <td id=\"T_fbb1a_row0_col5\" class=\"data row0 col5\" >0.507 (0.083)</td>\n",
       "      <td id=\"T_fbb1a_row0_col6\" class=\"data row0 col6\" >0.551 (0.068)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b80840c250>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For LR\n",
    "models = [\"Baseline\"]+[i for i in results_encodings[0].keys() if (\"tuned\" in i and \"XGB\" in i)]\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "use_df = pd.DataFrame([pd.DataFrame(results_encodings[fold_num]).loc[metric,models] for fold_num in results_encodings.keys()],index=results_encodings.keys())\n",
    "\n",
    "df_mean = pd.DataFrame((use_df).mean(axis=0).round(3).astype(str) + \" (\" + use_df.std(axis=0).round(3).astype(str) + \")\").transpose()\n",
    "model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "\n",
    "best_model = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "t_test_res = np.array([stats.ttest_rel(use_df[best_model].values, use_df[model].values)[1] for model in models]).round(3)\n",
    "t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    \n",
    "res_df_xgb = pd.DataFrame([model_dict])\n",
    "\n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_res[i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "res_df_xgb.style.apply(negative_bold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f5b6b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline</th>\n",
       "      <th>ignore</th>\n",
       "      <th>ohe</th>\n",
       "      <th>target</th>\n",
       "      <th>ordinal</th>\n",
       "      <th>catboost</th>\n",
       "      <th>glmm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.203 (0.016)</td>\n",
       "      <td>0.551 (0.028)</td>\n",
       "      <td>0.558 (0.032)</td>\n",
       "      <td>0.549 (0.034)</td>\n",
       "      <td>0.522 (0.018)</td>\n",
       "      <td>0.544 (0.014)</td>\n",
       "      <td>0.534 (0.024)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>0.203 (0.016)</td>\n",
       "      <td>0.541 (0.028)</td>\n",
       "      <td>0.558 (0.079)</td>\n",
       "      <td>0.565 (0.07)</td>\n",
       "      <td>0.573 (0.047)</td>\n",
       "      <td>0.507 (0.083)</td>\n",
       "      <td>0.551 (0.068)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Baseline         ignore            ohe         target  \\\n",
       "LR   0.203 (0.016)  0.551 (0.028)  0.558 (0.032)  0.549 (0.034)   \n",
       "XGB  0.203 (0.016)  0.541 (0.028)  0.558 (0.079)   0.565 (0.07)   \n",
       "\n",
       "           ordinal       catboost           glmm  \n",
       "LR   0.522 (0.018)  0.544 (0.014)  0.534 (0.024)  \n",
       "XGB  0.573 (0.047)  0.507 (0.083)  0.551 (0.068)  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df_lr.columns = [i.split(\"_\")[1] if i != \"Baseline\" else \"Baseline\" for i in res_df_lr.columns]    \n",
    "res_df_xgb.columns = [i.split(\"_\")[1] if i != \"Baseline\" else \"Baseline\" for i in res_df_xgb.columns]    \n",
    "\n",
    "latex_df_encodings = pd.concat([res_df_lr,res_df_xgb],axis=0)\n",
    "latex_df_encodings.index = [\"LR\", \"XGB\"]\n",
    "latex_df_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cbae6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "{} &       Baseline &         ignore &            ohe &         target &        ordinal &       catboost &           glmm \\\\\n",
      "\\midrule\n",
      "LR  &  0.203 (0.016) &  0.551 (0.028) &  0.558 (0.032) &  0.549 (0.034) &  0.522 (0.018) &  0.544 (0.014) &  0.534 (0.024) \\\\\n",
      "XGB &  0.203 (0.016) &  0.541 (0.028) &  0.558 (0.079) &   0.565 (0.07) &  0.573 (0.047) &  0.507 (0.083) &  0.551 (0.068) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_df_encodings.round(2).to_latex())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72032a67",
   "metadata": {},
   "source": [
    "## Data Subset Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be34b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = {\n",
    "    \"demo_only\": demographic_cols,\n",
    "#            \"performance_only\": perf_cols,\n",
    "           \"activity_only\": activity_cols,\n",
    "           \"activity_and_demo\": activity_cols+demographic_cols,\n",
    "#            \"performance_and_demo\": perf_cols+demographic_cols,\n",
    "           \"all\": list(df.columns)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "078f1efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing results for fold 0, subset=demo_only\n",
      "SCORE: 0.9284024518814847                                                                                              \n",
      "SCORE: 0.9283580909083871                                                                                              \n",
      "SCORE: 0.9237805279262592                                                                                              \n",
      "SCORE: 0.9609442186439884                                                                                              \n",
      "SCORE: 0.9257057048826931                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 27.03trial/s, best loss: 0.9237805279262592]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.9503755133113749}\n",
      "Default performance on Test: 1.2392985448747174\n",
      "SCORE: 0.9349665292961238                                                                                              \n",
      "SCORE: 0.9559458906282252                                                                                              \n",
      "SCORE: 0.9559618627147252                                                                                              \n",
      "SCORE: 1.0819120270134372                                                                                              \n",
      "SCORE: 0.9727722477401519                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.48trial/s, best loss: 0.9349665292961238]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.45673952935192674, 'n_estimators': 188.0}\n",
      "Test Performance after first tuning round: 1.327476105265801\n",
      "SCORE: 0.9551181132413327                                                                                              \n",
      "SCORE: 0.97660397392516                                                                                                \n",
      "SCORE: 0.9578264251407095                                                                                              \n",
      "SCORE: 0.928140315739466                                                                                               \n",
      "SCORE: 0.9788803690752426                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.43trial/s, best loss: 0.928140315739466]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.45673952935192674, 'n_estimators': 188.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 1.1561584052409768\n",
      "SCORE: 0.9041226768445989                                                                                              \n",
      "SCORE: 0.9117535967050564                                                                                              \n",
      "SCORE: 0.9129241466060931                                                                                              \n",
      "SCORE: 0.9110616695999101                                                                                              \n",
      "SCORE: 0.9063640617884481                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.69trial/s, best loss: 0.9041226768445989]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.45673952935192674, 'n_estimators': 188.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.9665699508013066, 'subsample': 0.6148827774866481}\n",
      "Test Performance after third tuning round: 1.1189464069771273\n",
      "SCORE: 0.9620895470021553                                                                                              \n",
      "SCORE: 1.050294699724194                                                                                               \n",
      "SCORE: 0.9497781422863618                                                                                              \n",
      "SCORE: 1.035247102238645                                                                                               \n",
      "SCORE: 1.0497833214827834                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.35trial/s, best loss: 0.9497781422863618]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.45673952935192674, 'n_estimators': 188.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.9665699508013066, 'subsample': 0.6148827774866481, 'gamma': 2.746186009047124, 'reg_alpha': 0.0, 'reg_lambda': 2.117646924770292}\n",
      "Test Performance after last tuning round: 1.0194186024591216\n",
      "Preparing results for fold 0, subset=activity_only\n",
      "SCORE: 0.6114995598521481                                                                                              \n",
      "SCORE: 0.6072112913040145                                                                                              \n",
      "SCORE: 0.619169497650034                                                                                               \n",
      "SCORE: 0.6068001890931835                                                                                              \n",
      "SCORE: 0.6085132784367715                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 26.45trial/s, best loss: 0.6068001890931835]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.7823430977384692}\n",
      "Default performance on Test: 0.8915716147778131\n",
      "SCORE: 0.6621843890881361                                                                                              \n",
      "SCORE: 0.6690595227696712                                                                                              \n",
      "SCORE: 0.667585246856724                                                                                               \n",
      "SCORE: 0.7063325388569321                                                                                              \n",
      "SCORE: 0.6770104599358555                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.51trial/s, best loss: 0.6621843890881361]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.32831503112545807, 'n_estimators': 372.0}\n",
      "Test Performance after first tuning round: 1.0763555365054027\n",
      "SCORE: 0.7063950797153316                                                                                              \n",
      "SCORE: 0.7543119515472233                                                                                              \n",
      "SCORE: 0.6860780451537128                                                                                              \n",
      "SCORE: 0.6835715281508199                                                                                              \n",
      "SCORE: 0.7195594474075768                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.74trial/s, best loss: 0.6835715281508199]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.32831503112545807, 'n_estimators': 372.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 8.0}\n",
      "Test Performance after second tuning round: 0.8930888209904358\n",
      "SCORE: 0.6623021282408992                                                                                              \n",
      "SCORE: 0.6297444395996141                                                                                              \n",
      "SCORE: 0.6452554975198418                                                                                              \n",
      "SCORE: 0.6717533227986356                                                                                              \n",
      "SCORE: 0.6890935490203642                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.67trial/s, best loss: 0.6297444395996141]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.32831503112545807, 'n_estimators': 372.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.7572645676745111, 'subsample': 0.6508300373527265}\n",
      "Test Performance after third tuning round: 0.8635712573206189\n",
      "SCORE: 0.70256665405861                                                                                                \n",
      "SCORE: 0.7394172514247621                                                                                              \n",
      "SCORE: 0.7629937779977762                                                                                              \n",
      "SCORE: 0.6853960374502244                                                                                              \n",
      "SCORE: 0.9486332796805794                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.52trial/s, best loss: 0.6853960374502244]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.32831503112545807, 'n_estimators': 372.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.7572645676745111, 'subsample': 0.6508300373527265, 'gamma': 4.481460554114418, 'reg_alpha': 1.0, 'reg_lambda': 2.0823928413033226}\n",
      "Test Performance after last tuning round: 0.6577595065055724\n",
      "Preparing results for fold 0, subset=activity_and_demo\n",
      "SCORE: 0.5856932012918952                                                                                              \n",
      "SCORE: 0.5816744930922993                                                                                              \n",
      "SCORE: 0.6102259518230054                                                                                              \n",
      "SCORE: 0.5903037504678943                                                                                              \n",
      "SCORE: 0.6343705481889415                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 12.70trial/s, best loss: 0.5816744930922993]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.6507883175869572}\n",
      "Default performance on Test: 0.8303479991536276\n",
      "SCORE: 0.6579207483144047                                                                                              \n",
      "SCORE: 0.6584795692611373                                                                                              \n",
      "SCORE: 0.7136620597210915                                                                                              \n",
      "SCORE: 0.7154165339610422                                                                                              \n",
      "SCORE: 0.7486161522767234                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.87trial/s, best loss: 0.6579207483144047]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3240318555834164, 'n_estimators': 263.0}\n",
      "Test Performance after first tuning round: 0.972676015769868\n",
      "SCORE: 0.6600509806024839                                                                                              \n",
      "SCORE: 0.6735851883155302                                                                                              \n",
      "SCORE: 0.66879963593836                                                                                                \n",
      "SCORE: 0.6601220639705654                                                                                              \n",
      "SCORE: 0.6777654991229748                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.55trial/s, best loss: 0.6600509806024839]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3240318555834164, 'n_estimators': 263.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 0.8168271074574937\n",
      "SCORE: 0.6550455072030412                                                                                              \n",
      "SCORE: 0.6398616196366083                                                                                              \n",
      "SCORE: 0.6222542156644246                                                                                              \n",
      "SCORE: 0.5949643699905146                                                                                              \n",
      "SCORE: 0.6264815024104248                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.47trial/s, best loss: 0.5949643699905146]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3240318555834164, 'n_estimators': 263.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.6906115189091129, 'subsample': 0.7714707004887318}\n",
      "Test Performance after third tuning round: 0.7957863195456932\n",
      "SCORE: 0.6503989023096175                                                                                              \n",
      "SCORE: 0.6143615817307819                                                                                              \n",
      "SCORE: 0.955610553204709                                                                                               \n",
      "SCORE: 0.827860866467612                                                                                               \n",
      "SCORE: 0.7355905884160752                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.42trial/s, best loss: 0.6143615817307819]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3240318555834164, 'n_estimators': 263.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.6906115189091129, 'subsample': 0.7714707004887318, 'gamma': 0.23025306461949957, 'reg_alpha': 3.0, 'reg_lambda': 2.345300135733624}\n",
      "Test Performance after last tuning round: 0.6205646489781201\n",
      "Preparing results for fold 0, subset=all\n",
      "SCORE: 0.5830885159995537                                                                                              \n",
      "SCORE: 0.5787914042868906                                                                                              \n",
      "SCORE: 0.5809837074808388                                                                                              \n",
      "SCORE: 0.5732721933219918                                                                                              \n",
      "SCORE: 0.5729834604797529                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.57trial/s, best loss: 0.5729834604797529]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.9085596071418031}\n",
      "Default performance on Test: 0.6332357608301113\n",
      "SCORE: 0.6705097769670172                                                                                              \n",
      "SCORE: 0.6959918737675993                                                                                              \n",
      "SCORE: 0.7318109686491356                                                                                              \n",
      "SCORE: 0.6652508934710192                                                                                              \n",
      "SCORE: 0.686436898067041                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.52trial/s, best loss: 0.6652508934710192]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.31247310757641, 'n_estimators': 491.0}\n",
      "Test Performance after first tuning round: 0.7290276883469446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.7326937782595276                                                                                              \n",
      "SCORE: 0.6736150155072602                                                                                              \n",
      "SCORE: 0.5999509887363973                                                                                              \n",
      "SCORE: 0.5793830138752565                                                                                              \n",
      "SCORE: 0.6426714063096729                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.76trial/s, best loss: 0.5793830138752565]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.31247310757641, 'n_estimators': 491.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 0.8262056629538046\n",
      "SCORE: 0.5689616054824483                                                                                              \n",
      "SCORE: 0.5807471066030275                                                                                              \n",
      "SCORE: 0.5997581530416005                                                                                              \n",
      "SCORE: 0.5722105978861982                                                                                              \n",
      "SCORE: 0.5663587507815144                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.17trial/s, best loss: 0.5663587507815144]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.31247310757641, 'n_estimators': 491.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.6338881918264299, 'subsample': 0.8358253443619847}\n",
      "Test Performance after third tuning round: 0.7613575178689022\n",
      "SCORE: 0.9276694270399707                                                                                              \n",
      "SCORE: 0.6518504349207515                                                                                              \n",
      "SCORE: 0.650411799665787                                                                                               \n",
      "SCORE: 0.7471717711012505                                                                                              \n",
      "SCORE: 0.7631505013875254                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.46trial/s, best loss: 0.650411799665787]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.31247310757641, 'n_estimators': 491.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.6338881918264299, 'subsample': 0.8358253443619847, 'gamma': 2.0273808648866134, 'reg_alpha': 3.0, 'reg_lambda': 1.4049934335215073}\n",
      "Test Performance after last tuning round: 0.6395387712723822\n",
      "Preparing results for fold 1, subset=demo_only\n",
      "SCORE: 0.9396791447682012                                                                                              \n",
      "SCORE: 0.9464599746570421                                                                                              \n",
      "SCORE: 0.9398718079031884                                                                                              \n",
      "SCORE: 1.0162930926742533                                                                                              \n",
      "SCORE: 0.9394492128919941                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 15.29trial/s, best loss: 0.9394492128919941]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.9966610611338576}\n",
      "Default performance on Test: 0.9521366943849207\n",
      "SCORE: 1.0145613412665797                                                                                              \n",
      "SCORE: 0.9895427399422078                                                                                              \n",
      "SCORE: 0.9796159568521563                                                                                              \n",
      "SCORE: 1.0687114818722827                                                                                              \n",
      "SCORE: 1.0155788101354102                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.09trial/s, best loss: 0.9796159568521563]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.40719215849723966, 'n_estimators': 444.0}\n",
      "Test Performance after first tuning round: 1.0626709460719785\n",
      "SCORE: 0.9395584001059447                                                                                              \n",
      "SCORE: 0.9412191482556386                                                                                              \n",
      "SCORE: 0.9909135773483809                                                                                              \n",
      "SCORE: 0.9655718616200437                                                                                              \n",
      "SCORE: 0.9655718616200437                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.51trial/s, best loss: 0.9395584001059447]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.40719215849723966, 'n_estimators': 444.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 9.0}\n",
      "Test Performance after second tuning round: 0.8995043077691746\n",
      "SCORE: 0.9311546632321761                                                                                              \n",
      "SCORE: 0.9441501303441395                                                                                              \n",
      "SCORE: 0.9400369107908138                                                                                              \n",
      "SCORE: 0.9246685764054641                                                                                              \n",
      "SCORE: 0.925572708473676                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.98trial/s, best loss: 0.9246685764054641]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.40719215849723966, 'n_estimators': 444.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.649122393039058, 'subsample': 0.9481171261524988}\n",
      "Test Performance after third tuning round: 0.8877761237462559\n",
      "SCORE: 1.0574055043690087                                                                                              \n",
      "SCORE: 1.0660721014391474                                                                                              \n",
      "SCORE: 1.074171744258377                                                                                               \n",
      "SCORE: 1.052662277613106                                                                                               \n",
      "SCORE: 1.0510515186741654                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.53trial/s, best loss: 1.0510515186741654]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.40719215849723966, 'n_estimators': 444.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.649122393039058, 'subsample': 0.9481171261524988, 'gamma': 6.214449413021728, 'reg_alpha': 3.0, 'reg_lambda': 2.684863332656779}\n",
      "Test Performance after last tuning round: 1.00082164697386\n",
      "Preparing results for fold 1, subset=activity_only\n",
      "SCORE: 0.6511379615548305                                                                                              \n",
      "SCORE: 0.6406995142703682                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.6385612734018467                                                                                              \n",
      "SCORE: 0.6370686972993729                                                                                              \n",
      "SCORE: 0.6482135888560379                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 14.83trial/s, best loss: 0.6370686972993729]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.6152112699148091}\n",
      "Default performance on Test: 0.7469593248050793\n",
      "SCORE: 0.718413727621394                                                                                               \n",
      "SCORE: 0.6988667996851861                                                                                              \n",
      "SCORE: 0.6842484856752628                                                                                              \n",
      "SCORE: 0.6935299349870879                                                                                              \n",
      "SCORE: 0.6968357215465163                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.77trial/s, best loss: 0.6842484856752628]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4400155047688833, 'n_estimators': 436.0}\n",
      "Test Performance after first tuning round: 0.8519437611309684\n",
      "SCORE: 0.7066556111135701                                                                                              \n",
      "SCORE: 0.6929603101377433                                                                                              \n",
      "SCORE: 0.6869360333767677                                                                                              \n",
      "SCORE: 0.6897465006698578                                                                                              \n",
      "SCORE: 0.6929603101377433                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.46trial/s, best loss: 0.6869360333767677]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4400155047688833, 'n_estimators': 436.0, 'seed': 0, 'max_depth': 18.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 0.6882393765255627\n",
      "SCORE: 0.7072467872990862                                                                                              \n",
      "SCORE: 0.7201201181803473                                                                                              \n",
      "SCORE: 0.692606216111251                                                                                               \n",
      "SCORE: 0.7085183826890621                                                                                              \n",
      "SCORE: 0.6747651424889913                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.70trial/s, best loss: 0.6747651424889913]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4400155047688833, 'n_estimators': 436.0, 'seed': 0, 'max_depth': 18.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.6626084346676859, 'subsample': 0.6541444247947726}\n",
      "Test Performance after third tuning round: 0.7146593876951691\n",
      "SCORE: 0.7659070201134988                                                                                              \n",
      "SCORE: 0.8009864405153202                                                                                              \n",
      "SCORE: 0.9254644203398407                                                                                              \n",
      "SCORE: 0.9199046546199702                                                                                              \n",
      "SCORE: 0.7612706203633514                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.40trial/s, best loss: 0.7612706203633514]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4400155047688833, 'n_estimators': 436.0, 'seed': 0, 'max_depth': 18.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.6626084346676859, 'subsample': 0.6541444247947726, 'gamma': 0.406228022141556, 'reg_alpha': 7.0, 'reg_lambda': 2.8701575963953}\n",
      "Test Performance after last tuning round: 0.5723101913338325\n",
      "Preparing results for fold 1, subset=activity_and_demo\n",
      "SCORE: 0.6094613647235951                                                                                              \n",
      "SCORE: 0.6095643823666036                                                                                              \n",
      "SCORE: 0.6131545536185465                                                                                              \n",
      "SCORE: 0.6212437960185644                                                                                              \n",
      "SCORE: 0.6143473259064346                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.68trial/s, best loss: 0.6094613647235951]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.8773126424999764}\n",
      "Default performance on Test: 0.7147131397441736\n",
      "SCORE: 0.6694616641830915                                                                                              \n",
      "SCORE: 0.7854548508003868                                                                                              \n",
      "SCORE: 0.6482938219251382                                                                                              \n",
      "SCORE: 0.744652611974248                                                                                               \n",
      "SCORE: 0.7966698668089943                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.11trial/s, best loss: 0.6482938219251382]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.27431991854605897, 'n_estimators': 144.0}\n",
      "Test Performance after first tuning round: 0.7252038400502038\n",
      "SCORE: 0.6482968173800888                                                                                              \n",
      "SCORE: 0.7368762434878051                                                                                              \n",
      "SCORE: 0.5960667045856877                                                                                              \n",
      "SCORE: 0.654053949222986                                                                                               \n",
      "SCORE: 0.6482968173800888                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.66trial/s, best loss: 0.5960667045856877]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.27431991854605897, 'n_estimators': 144.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.5559727254788611\n",
      "SCORE: 0.6392365730562323                                                                                              \n",
      "SCORE: 0.6264167798086667                                                                                              \n",
      "SCORE: 0.6485410091589928                                                                                              \n",
      "SCORE: 0.6115108330567548                                                                                              \n",
      "SCORE: 0.6995689205335938                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.72trial/s, best loss: 0.6115108330567548]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.27431991854605897, 'n_estimators': 144.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.7572973008430173, 'subsample': 0.9473210363734621}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance after third tuning round: 0.5630515016521973\n",
      "SCORE: 0.767248851125461                                                                                               \n",
      "SCORE: 0.7572684669442167                                                                                              \n",
      "SCORE: 0.8332324200650799                                                                                              \n",
      "SCORE: 0.7330801407685773                                                                                              \n",
      "SCORE: 0.7751948993945572                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.72trial/s, best loss: 0.7330801407685773]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.27431991854605897, 'n_estimators': 144.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.7572973008430173, 'subsample': 0.9473210363734621, 'gamma': 0.7093452840200575, 'reg_alpha': 9.0, 'reg_lambda': 1.7355091929238051}\n",
      "Test Performance after last tuning round: 0.5807453120880518\n",
      "Preparing results for fold 1, subset=all\n",
      "SCORE: 0.617933600640504                                                                                               \n",
      "SCORE: 0.6298328471771293                                                                                              \n",
      "SCORE: 0.6184623608466792                                                                                              \n",
      "SCORE: 0.6442388874486611                                                                                              \n",
      "SCORE: 0.6193397303004498                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.18trial/s, best loss: 0.617933600640504]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.44935454971484196}\n",
      "Default performance on Test: 0.6598745499417085\n",
      "SCORE: 0.9378223339858411                                                                                              \n",
      "SCORE: 0.6516953168213002                                                                                              \n",
      "SCORE: 0.7165616047459911                                                                                              \n",
      "SCORE: 0.8173906873383828                                                                                              \n",
      "SCORE: 0.9395823362972179                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.87trial/s, best loss: 0.6516953168213002]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4020754577973428, 'n_estimators': 93.0}\n",
      "Test Performance after first tuning round: 0.7350654359691265\n",
      "SCORE: 0.6077428502025349                                                                                              \n",
      "SCORE: 0.6106550061841565                                                                                              \n",
      "SCORE: 0.6480705493109754                                                                                              \n",
      "SCORE: 0.6332190228193155                                                                                              \n",
      "SCORE: 0.6036337692037701                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.35trial/s, best loss: 0.6036337692037701]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4020754577973428, 'n_estimators': 93.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.5536252286697648\n",
      "SCORE: 0.5907845798303651                                                                                              \n",
      "SCORE: 0.5839015044307343                                                                                              \n",
      "SCORE: 0.6108580696743671                                                                                              \n",
      "SCORE: 0.5916028492010093                                                                                              \n",
      "SCORE: 0.6009428047550898                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.36trial/s, best loss: 0.5839015044307343]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4020754577973428, 'n_estimators': 93.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.5875748391652142, 'subsample': 0.5238399671862233}\n",
      "Test Performance after third tuning round: 0.5491248232473107\n",
      "SCORE: 0.8745879745053017                                                                                              \n",
      "SCORE: 0.8041069585749658                                                                                              \n",
      "SCORE: 0.8711434109857864                                                                                              \n",
      "SCORE: 0.7371349639428666                                                                                              \n",
      "SCORE: 0.8613195651180551                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.22trial/s, best loss: 0.7371349639428666]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4020754577973428, 'n_estimators': 93.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.5875748391652142, 'subsample': 0.5238399671862233, 'gamma': 4.492472838742196, 'reg_alpha': 1.0, 'reg_lambda': 2.7388142615605258}\n",
      "Test Performance after last tuning round: 0.5722264989364662\n",
      "Preparing results for fold 2, subset=demo_only\n",
      "SCORE: 0.9622604382530728                                                                                              \n",
      "SCORE: 0.9622008515605236                                                                                              \n",
      "SCORE: 0.9622020222413047                                                                                              \n",
      "SCORE: 0.9622085906819897                                                                                              \n",
      "SCORE: 0.9724857356027579                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 22.99trial/s, best loss: 0.9622008515605236]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.8782199725344628}\n",
      "Default performance on Test: 0.811265038584866\n",
      "SCORE: 0.9904390217253483                                                                                              \n",
      "SCORE: 0.994891518554675                                                                                               \n",
      "SCORE: 0.9913745427408042                                                                                              \n",
      "SCORE: 0.9876690370732579                                                                                              \n",
      "SCORE: 1.013514523598378                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.94trial/s, best loss: 0.9876690370732579]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4357091598496667, 'n_estimators': 484.0}\n",
      "Test Performance after first tuning round: 0.8194760939889498\n",
      "SCORE: 0.9672696564374048                                                                                              \n",
      "SCORE: 0.9997882598685157                                                                                              \n",
      "SCORE: 0.96885798340187                                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.9578826803147832                                                                                              \n",
      "SCORE: 0.9672696564374048                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.08trial/s, best loss: 0.9578826803147832]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4357091598496667, 'n_estimators': 484.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 0.8308879223193593\n",
      "SCORE: 0.976535189801781                                                                                               \n",
      "SCORE: 0.9485329211967197                                                                                              \n",
      "SCORE: 0.948240831765079                                                                                               \n",
      "SCORE: 0.9450811510458286                                                                                              \n",
      "SCORE: 0.9449268418879264                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.31trial/s, best loss: 0.9449268418879264]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4357091598496667, 'n_estimators': 484.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.7331515584219155, 'subsample': 0.5858890812582578}\n",
      "Test Performance after third tuning round: 0.8253843799605983\n",
      "SCORE: 1.05253548754381                                                                                                \n",
      "SCORE: 1.0312759586783717                                                                                              \n",
      "SCORE: 1.0638242304965542                                                                                              \n",
      "SCORE: 0.9977283155109646                                                                                              \n",
      "SCORE: 1.0645618995020718                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.44trial/s, best loss: 0.9977283155109646]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4357091598496667, 'n_estimators': 484.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.7331515584219155, 'subsample': 0.5858890812582578, 'gamma': 2.733320630895699, 'reg_alpha': 1.0, 'reg_lambda': 2.876815672374473}\n",
      "Test Performance after last tuning round: 0.9035708538577077\n",
      "Preparing results for fold 2, subset=activity_only\n",
      "SCORE: 0.6465653081420117                                                                                              \n",
      "SCORE: 0.6004262581395043                                                                                              \n",
      "SCORE: 0.6332707798817822                                                                                              \n",
      "SCORE: 0.6112326936898235                                                                                              \n",
      "SCORE: 0.600629130222381                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 15.07trial/s, best loss: 0.6004262581395043]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.9772673710112033}\n",
      "Default performance on Test: 0.9072342636668741\n",
      "SCORE: 0.9418714807789493                                                                                              \n",
      "SCORE: 0.7151183588772496                                                                                              \n",
      "SCORE: 0.7090281072694719                                                                                              \n",
      "SCORE: 0.6823449152169532                                                                                              \n",
      "SCORE: 0.7065136835358619                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.15trial/s, best loss: 0.6823449152169532]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.19997714651389795, 'n_estimators': 294.0}\n",
      "Test Performance after first tuning round: 1.00511910190026\n",
      "SCORE: 0.7509851961872146                                                                                              \n",
      "SCORE: 0.6334247407052541                                                                                              \n",
      "SCORE: 0.644415335568762                                                                                               \n",
      "SCORE: 0.6718503348777101                                                                                              \n",
      "SCORE: 0.6404191350576941                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.70trial/s, best loss: 0.6334247407052541]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.19997714651389795, 'n_estimators': 294.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.7349211991667012\n",
      "SCORE: 0.7113725496947781                                                                                              \n",
      "SCORE: 0.6307816325960083                                                                                              \n",
      "SCORE: 0.673994985324844                                                                                               \n",
      "SCORE: 0.6529662433055708                                                                                              \n",
      "SCORE: 0.6772878879897233                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.09trial/s, best loss: 0.6307816325960083]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.19997714651389795, 'n_estimators': 294.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.8800029509985379, 'subsample': 0.8865576484523636}\n",
      "Test Performance after third tuning round: 0.6957745380033145\n",
      "SCORE: 0.7177856170831632                                                                                              \n",
      "SCORE: 0.9484717132589966                                                                                              \n",
      "SCORE: 0.8910603112242852                                                                                              \n",
      "SCORE: 0.7511062281375496                                                                                              \n",
      "SCORE: 0.7072378830435714                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.67trial/s, best loss: 0.7072378830435714]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.19997714651389795, 'n_estimators': 294.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.8800029509985379, 'subsample': 0.8865576484523636, 'gamma': 0.8566578894359478, 'reg_alpha': 6.0, 'reg_lambda': 1.3495269572530313}\n",
      "Test Performance after last tuning round: 0.6623635716752764\n",
      "Preparing results for fold 2, subset=activity_and_demo\n",
      "SCORE: 0.6015276641556146                                                                                              \n",
      "SCORE: 0.5949557606055059                                                                                              \n",
      "SCORE: 0.6427788286156997                                                                                              \n",
      "SCORE: 0.7017704383419001                                                                                              \n",
      "SCORE: 0.6085543596922978                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 13.37trial/s, best loss: 0.5949557606055059]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.729922178488326}\n",
      "Default performance on Test: 0.7383795236810039\n",
      "SCORE: 0.68608914658954                                                                                                \n",
      "SCORE: 0.683530890397126                                                                                               \n",
      "SCORE: 0.6870703578636065                                                                                              \n",
      "SCORE: 0.7274299356600273                                                                                              \n",
      "SCORE: 0.6649230225896173                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.96trial/s, best loss: 0.6649230225896173]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.21188539351727695, 'n_estimators': 378.0}\n",
      "Test Performance after first tuning round: 0.7977784606217725\n",
      "SCORE: 0.6284534316656247                                                                                              \n",
      "SCORE: 0.6667967161195735                                                                                              \n",
      "SCORE: 0.6941688321485948                                                                                              \n",
      "SCORE: 0.6352133121796818                                                                                              \n",
      "SCORE: 0.6931075663136121                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.36trial/s, best loss: 0.6284534316656247]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.21188539351727695, 'n_estimators': 378.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 10.0}\n",
      "Test Performance after second tuning round: 0.6628740514965791\n",
      "SCORE: 0.6402524094511781                                                                                              \n",
      "SCORE: 0.6363266654985933                                                                                              \n",
      "SCORE: 0.6910358004147847                                                                                              \n",
      "SCORE: 0.6275414295690015                                                                                              \n",
      "SCORE: 0.6370725455019455                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.04trial/s, best loss: 0.6275414295690015]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.21188539351727695, 'n_estimators': 378.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.5306656641123991, 'subsample': 0.6889812363293158}\n",
      "Test Performance after third tuning round: 0.5891576837949674\n",
      "SCORE: 0.8780776891768479                                                                                              \n",
      "SCORE: 0.7340248386337423                                                                                              \n",
      "SCORE: 0.853451407879537                                                                                               \n",
      "SCORE: 0.777035291706669                                                                                               \n",
      "SCORE: 0.7094062764147371                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.21trial/s, best loss: 0.7094062764147371]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.21188539351727695, 'n_estimators': 378.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.5306656641123991, 'subsample': 0.6889812363293158, 'gamma': 3.3222386312025014, 'reg_alpha': 2.0, 'reg_lambda': 3.9218506468619414}\n",
      "Test Performance after last tuning round: 0.6310161345943262\n",
      "Preparing results for fold 2, subset=all\n",
      "SCORE: 0.5809962398624653                                                                                              \n",
      "SCORE: 0.5900668466051023                                                                                              \n",
      "SCORE: 0.621210697777569                                                                                               \n",
      "SCORE: 0.5823957923463551                                                                                              \n",
      "SCORE: 0.5895062730405162                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.79trial/s, best loss: 0.5809962398624653]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.7006063253846259}\n",
      "Default performance on Test: 0.7212954557348779\n",
      "SCORE: 0.7213535197885781                                                                                              \n",
      "SCORE: 0.6324351714382069                                                                                              \n",
      "SCORE: 0.6014781698139915                                                                                              \n",
      "SCORE: 0.9029050974169225                                                                                              \n",
      "SCORE: 0.6309704447525055                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.39trial/s, best loss: 0.6014781698139915]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.20044997198492498, 'n_estimators': 230.0}\n",
      "Test Performance after first tuning round: 0.7059244262191954\n",
      "SCORE: 0.5915407200963791                                                                                              \n",
      "SCORE: 0.6421617303980466                                                                                              \n",
      "SCORE: 0.7125923433369283                                                                                              \n",
      "SCORE: 0.5951695231549439                                                                                              \n",
      "SCORE: 0.5820138520335594                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.63trial/s, best loss: 0.5820138520335594]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.20044997198492498, 'n_estimators': 230.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 10.0}\n",
      "Test Performance after second tuning round: 0.5916012822465989\n",
      "SCORE: 0.5933233183397169                                                                                              \n",
      "SCORE: 0.6728784723528924                                                                                              \n",
      "SCORE: 0.5976681044290129                                                                                              \n",
      "SCORE: 0.6121714560241835                                                                                              \n",
      "SCORE: 0.6085854563666008                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.28trial/s, best loss: 0.5933233183397169]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.20044997198492498, 'n_estimators': 230.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.8918572896091755, 'subsample': 0.7832926872518551}\n",
      "Test Performance after third tuning round: 0.5624476831019919\n",
      "SCORE: 0.778581901989581                                                                                               \n",
      "SCORE: 0.7036516894920382                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.7775991689368109                                                                                              \n",
      "SCORE: 0.6418313621151266                                                                                              \n",
      "SCORE: 0.8113167951232146                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.43trial/s, best loss: 0.6418313621151266]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.20044997198492498, 'n_estimators': 230.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.8918572896091755, 'subsample': 0.7832926872518551, 'gamma': 1.2477729866084581, 'reg_alpha': 4.0, 'reg_lambda': 1.3561151146772312}\n",
      "Test Performance after last tuning round: 0.6093868890526372\n",
      "Preparing results for fold 3, subset=demo_only\n",
      "SCORE: 0.9348298603914715                                                                                              \n",
      "SCORE: 0.9399695524605078                                                                                              \n",
      "SCORE: 0.9337713923307126                                                                                              \n",
      "SCORE: 0.9347907529463839                                                                                              \n",
      "SCORE: 0.9338384849025342                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 13.49trial/s, best loss: 0.9337713923307126]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.7637816351645935}\n",
      "Default performance on Test: 1.1933703211204445\n",
      "SCORE: 0.9052013142948626                                                                                              \n",
      "SCORE: 0.9800723290563571                                                                                              \n",
      "SCORE: 0.9692896377130277                                                                                              \n",
      "SCORE: 0.902840413503535                                                                                               \n",
      "SCORE: 0.9083480140479925                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.06trial/s, best loss: 0.902840413503535]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.34042557621632497, 'n_estimators': 476.0}\n",
      "Test Performance after first tuning round: 1.3172867130106527\n",
      "SCORE: 0.9227514249140162                                                                                              \n",
      "SCORE: 0.9262506297179659                                                                                              \n",
      "SCORE: 0.9103595546469384                                                                                              \n",
      "SCORE: 0.9726352993552609                                                                                              \n",
      "SCORE: 0.9616079367598884                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.30trial/s, best loss: 0.9103595546469384]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.34042557621632497, 'n_estimators': 476.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 10.0}\n",
      "Test Performance after second tuning round: 1.0342382959669087\n",
      "SCORE: 0.941791522250625                                                                                               \n",
      "SCORE: 0.9119738489899231                                                                                              \n",
      "SCORE: 0.9383194409022982                                                                                              \n",
      "SCORE: 0.919652361923263                                                                                               \n",
      "SCORE: 0.9400820221992798                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.68trial/s, best loss: 0.9119738489899231]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.34042557621632497, 'n_estimators': 476.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.7576474922641022, 'subsample': 0.860812210681201}\n",
      "Test Performance after third tuning round: 1.0591056583434701\n",
      "SCORE: 1.0544530822286593                                                                                              \n",
      "SCORE: 1.0555391948207236                                                                                              \n",
      "SCORE: 0.9332575786862476                                                                                              \n",
      "SCORE: 0.9514132383690839                                                                                              \n",
      "SCORE: 0.9743309840347214                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.55trial/s, best loss: 0.9332575786862476]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.34042557621632497, 'n_estimators': 476.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.7576474922641022, 'subsample': 0.860812210681201, 'gamma': 0.3592166375051809, 'reg_alpha': 3.0, 'reg_lambda': 2.532315044790774}\n",
      "Test Performance after last tuning round: 1.010245332826016\n",
      "Preparing results for fold 3, subset=activity_only\n",
      "SCORE: 0.6114568282092226                                                                                              \n",
      "SCORE: 0.6120163416831534                                                                                              \n",
      "SCORE: 0.6097707579879165                                                                                              \n",
      "SCORE: 0.614999682633204                                                                                               \n",
      "SCORE: 0.6237191936111456                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 15.87trial/s, best loss: 0.6097707579879165]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.9837817922563711}\n",
      "Default performance on Test: 0.774072389332793\n",
      "SCORE: 0.7911340798832438                                                                                              \n",
      "SCORE: 0.7711288540674592                                                                                              \n",
      "SCORE: 1.0669650329306668                                                                                              \n",
      "SCORE: 0.7390447434595611                                                                                              \n",
      "SCORE: 0.6936882531035538                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.74trial/s, best loss: 0.6936882531035538]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4867790023268621, 'n_estimators': 139.0}\n",
      "Test Performance after first tuning round: 0.9299164531680253\n",
      "SCORE: 0.7140661760478377                                                                                              \n",
      "SCORE: 0.6869108475606576                                                                                              \n",
      "SCORE: 0.6663369990927335                                                                                              \n",
      "SCORE: 0.6870726491620632                                                                                              \n",
      "SCORE: 0.723199853394748                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.81trial/s, best loss: 0.6663369990927335]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4867790023268621, 'n_estimators': 139.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.7615378935393168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.6887497879527299                                                                                              \n",
      "SCORE: 0.6828615705371565                                                                                              \n",
      "SCORE: 0.6463485788852614                                                                                              \n",
      "SCORE: 0.6730916037322585                                                                                              \n",
      "SCORE: 0.717170927691312                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.31trial/s, best loss: 0.6463485788852614]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4867790023268621, 'n_estimators': 139.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.7371179841858468, 'subsample': 0.810769344777706}\n",
      "Test Performance after third tuning round: 0.7557950358372593\n",
      "SCORE: 0.8064581151942602                                                                                              \n",
      "SCORE: 0.8372540845715288                                                                                              \n",
      "SCORE: 0.6496739109396905                                                                                              \n",
      "SCORE: 0.6632965419454203                                                                                              \n",
      "SCORE: 0.9244181281367766                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.82trial/s, best loss: 0.6496739109396905]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4867790023268621, 'n_estimators': 139.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.7371179841858468, 'subsample': 0.810769344777706, 'gamma': 0.1559260222593759, 'reg_alpha': 3.0, 'reg_lambda': 2.595015346698714}\n",
      "Test Performance after last tuning round: 0.5854628966527294\n",
      "Preparing results for fold 3, subset=activity_and_demo\n",
      "SCORE: 0.5937517304824419                                                                                              \n",
      "SCORE: 0.5916304089074267                                                                                              \n",
      "SCORE: 0.5935351640378987                                                                                              \n",
      "SCORE: 0.6138580443156132                                                                                              \n",
      "SCORE: 0.6354582805093256                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.89trial/s, best loss: 0.5916304089074267]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.68628122116735}\n",
      "Default performance on Test: 0.7900564293205222\n",
      "SCORE: 0.6419542869126511                                                                                              \n",
      "SCORE: 0.6813024705648612                                                                                              \n",
      "SCORE: 0.8433568782738481                                                                                              \n",
      "SCORE: 0.6535966357691722                                                                                              \n",
      "SCORE: 0.834907047507086                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.11trial/s, best loss: 0.6419542869126511]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4241780763025039, 'n_estimators': 181.0}\n",
      "Test Performance after first tuning round: 0.8926740173082611\n",
      "SCORE: 0.7153728241236028                                                                                              \n",
      "SCORE: 0.5869502832764038                                                                                              \n",
      "SCORE: 0.6072492727733815                                                                                              \n",
      "SCORE: 0.5866613301502088                                                                                              \n",
      "SCORE: 0.5992197266776979                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.23trial/s, best loss: 0.5866613301502088]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4241780763025039, 'n_estimators': 181.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 9.0}\n",
      "Test Performance after second tuning round: 0.7722972654042385\n",
      "SCORE: 0.5727361571498355                                                                                              \n",
      "SCORE: 0.564819281339388                                                                                               \n",
      "SCORE: 0.589903790233544                                                                                               \n",
      "SCORE: 0.5747343246985689                                                                                              \n",
      "SCORE: 0.6007901218586544                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.15trial/s, best loss: 0.564819281339388]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4241780763025039, 'n_estimators': 181.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.8020014400304212, 'subsample': 0.7496148089799588}\n",
      "Test Performance after third tuning round: 0.7423321351342715\n",
      "SCORE: 0.8166631295364379                                                                                              \n",
      "SCORE: 0.8796330850334015                                                                                              \n",
      "SCORE: 0.729008506759477                                                                                               \n",
      "SCORE: 0.7631567666781509                                                                                              \n",
      "SCORE: 0.8556543361465826                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.99trial/s, best loss: 0.729008506759477]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4241780763025039, 'n_estimators': 181.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.8020014400304212, 'subsample': 0.7496148089799588, 'gamma': 2.4171572186743715, 'reg_alpha': 3.0, 'reg_lambda': 3.754317249516024}\n",
      "Test Performance after last tuning round: 0.6246336499052884\n",
      "Preparing results for fold 3, subset=all\n",
      "SCORE: 0.6512395743787417                                                                                              \n",
      "SCORE: 0.602606061570696                                                                                               \n",
      "SCORE: 0.603151797718688                                                                                               \n",
      "SCORE: 0.6290965280374137                                                                                              \n",
      "SCORE: 0.6029712697247586                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.62trial/s, best loss: 0.602606061570696]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.4489248840518399}\n",
      "Default performance on Test: 0.6638805488319858\n",
      "SCORE: 0.6291744003870596                                                                                              \n",
      "SCORE: 1.0568042431514015                                                                                              \n",
      "SCORE: 0.6483350338327648                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.6356155825260832                                                                                              \n",
      "SCORE: 0.634204468111224                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.16trial/s, best loss: 0.6291744003870596]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3545273647894992, 'n_estimators': 229.0}\n",
      "Test Performance after first tuning round: 0.7029378484547038\n",
      "SCORE: 0.6040365340203263                                                                                              \n",
      "SCORE: 0.5777867071337375                                                                                              \n",
      "SCORE: 0.6058710261250619                                                                                              \n",
      "SCORE: 0.6010896614462828                                                                                              \n",
      "SCORE: 0.5944463906651931                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.65trial/s, best loss: 0.5777867071337375]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3545273647894992, 'n_estimators': 229.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 0.6057251566581584\n",
      "SCORE: 0.5908482606485297                                                                                              \n",
      "SCORE: 0.6077364860205287                                                                                              \n",
      "SCORE: 0.6159794154075786                                                                                              \n",
      "SCORE: 0.5940498410910999                                                                                              \n",
      "SCORE: 0.6178971503433128                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.80trial/s, best loss: 0.5908482606485297]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3545273647894992, 'n_estimators': 229.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.522708880764591, 'subsample': 0.5204157227038316}\n",
      "Test Performance after third tuning round: 0.5159171746487655\n",
      "SCORE: 0.9256222951945944                                                                                              \n",
      "SCORE: 0.6236412089587925                                                                                              \n",
      "SCORE: 0.8064615497129                                                                                                 \n",
      "SCORE: 0.8519798073739586                                                                                              \n",
      "SCORE: 0.6279180102799973                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.91trial/s, best loss: 0.6236412089587925]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3545273647894992, 'n_estimators': 229.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.522708880764591, 'subsample': 0.5204157227038316, 'gamma': 3.585988803298943, 'reg_alpha': 0.0, 'reg_lambda': 2.7767593459692286}\n",
      "Test Performance after last tuning round: 0.5291990405565046\n",
      "Preparing results for fold 4, subset=demo_only\n",
      "SCORE: 0.931666509195938                                                                                               \n",
      "SCORE: 0.9424521528998792                                                                                              \n",
      "SCORE: 0.9232497275690823                                                                                              \n",
      "SCORE: 0.9452113703874103                                                                                              \n",
      "SCORE: 0.926387135685985                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 16.23trial/s, best loss: 0.9232497275690823]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.8797398838070621}\n",
      "Default performance on Test: 0.9974222936190572\n",
      "SCORE: 0.9715599679994231                                                                                              \n",
      "SCORE: 0.9633012457783682                                                                                              \n",
      "SCORE: 0.9791858548393145                                                                                              \n",
      "SCORE: 0.969541677124847                                                                                               \n",
      "SCORE: 0.9681403665757886                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.46trial/s, best loss: 0.9633012457783682]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.20352169248594482, 'n_estimators': 447.0}\n",
      "Test Performance after first tuning round: 1.0631563753505517\n",
      "SCORE: 0.982065631803381                                                                                               \n",
      "SCORE: 0.9939613645981197                                                                                              \n",
      "SCORE: 0.9774061536896872                                                                                              \n",
      "SCORE: 0.9781761243207914                                                                                              \n",
      "SCORE: 0.9774049193767329                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.23trial/s, best loss: 0.9774049193767329]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.20352169248594482, 'n_estimators': 447.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 0.9235562926239433\n",
      "SCORE: 0.9803671099522901                                                                                              \n",
      "SCORE: 0.9590601133999161                                                                                              \n",
      "SCORE: 0.9652895027762913                                                                                              \n",
      "SCORE: 0.9654485026996577                                                                                              \n",
      "SCORE: 0.9944143686538895                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.10trial/s, best loss: 0.9590601133999161]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.20352169248594482, 'n_estimators': 447.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.878905956359602, 'subsample': 0.6267326505037099}\n",
      "Test Performance after third tuning round: 0.9261004663298716\n",
      "SCORE: 1.0627857165686068                                                                                              \n",
      "SCORE: 0.9909533822210956                                                                                              \n",
      "SCORE: 1.0656894396688394                                                                                              \n",
      "SCORE: 1.0852912590976946                                                                                              \n",
      "SCORE: 1.086452799536692                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.82trial/s, best loss: 0.9909533822210956]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.20352169248594482, 'n_estimators': 447.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.878905956359602, 'subsample': 0.6267326505037099, 'gamma': 1.3984152587862575, 'reg_alpha': 1.0, 'reg_lambda': 3.670045894753616}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance after last tuning round: 0.9227358543323549\n",
      "Preparing results for fold 4, subset=activity_only\n",
      "SCORE: 0.6187288972833442                                                                                              \n",
      "SCORE: 0.5917798073490517                                                                                              \n",
      "SCORE: 0.615468303603947                                                                                               \n",
      "SCORE: 0.5931141781017938                                                                                              \n",
      "SCORE: 0.5949930361664536                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 17.24trial/s, best loss: 0.5917798073490517]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.8385179995092874}\n",
      "Default performance on Test: 1.0076843199266508\n",
      "SCORE: 0.7244776855554361                                                                                              \n",
      "SCORE: 0.678133919940815                                                                                               \n",
      "SCORE: 0.730055837549158                                                                                               \n",
      "SCORE: 0.6937787971045267                                                                                              \n",
      "SCORE: 0.6793697889604068                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.55trial/s, best loss: 0.678133919940815]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.19717030690873574, 'n_estimators': 76.0}\n",
      "Test Performance after first tuning round: 0.8194348821012282\n",
      "SCORE: 0.6350083791418488                                                                                              \n",
      "SCORE: 0.619128753963502                                                                                               \n",
      "SCORE: 0.6398800905467262                                                                                              \n",
      "SCORE: 0.6876397892126541                                                                                              \n",
      "SCORE: 0.7672638920654434                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.35trial/s, best loss: 0.619128753963502]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.19717030690873574, 'n_estimators': 76.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 10.0}\n",
      "Test Performance after second tuning round: 0.6832024367487962\n",
      "SCORE: 0.621504643962917                                                                                               \n",
      "SCORE: 0.6343243183445749                                                                                              \n",
      "SCORE: 0.6284147678000339                                                                                              \n",
      "SCORE: 0.640389418218073                                                                                               \n",
      "SCORE: 0.7037309833169949                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.45trial/s, best loss: 0.621504643962917]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.19717030690873574, 'n_estimators': 76.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.9721736540420374, 'subsample': 0.9646163090038451}\n",
      "Test Performance after third tuning round: 0.678403567674117\n",
      "SCORE: 0.7561792585406505                                                                                              \n",
      "SCORE: 0.7437897398817371                                                                                              \n",
      "SCORE: 0.8520164450681973                                                                                              \n",
      "SCORE: 0.8964400881841119                                                                                              \n",
      "SCORE: 0.8047956068115167                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.37trial/s, best loss: 0.7437897398817371]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.19717030690873574, 'n_estimators': 76.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.9721736540420374, 'subsample': 0.9646163090038451, 'gamma': 5.063296579907067, 'reg_alpha': 3.0, 'reg_lambda': 1.2427016310370238}\n",
      "Test Performance after last tuning round: 0.7349591723045327\n",
      "Preparing results for fold 4, subset=activity_and_demo\n",
      "SCORE: 0.5686050037179637                                                                                              \n",
      "SCORE: 0.5687137629119826                                                                                              \n",
      "SCORE: 0.5760429965087416                                                                                              \n",
      "SCORE: 0.5929457362260716                                                                                              \n",
      "SCORE: 0.5835770211994744                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.65trial/s, best loss: 0.5686050037179637]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.8061317452406184}\n",
      "Default performance on Test: 0.8281393161900827\n",
      "SCORE: 0.6712907341585395                                                                                              \n",
      "SCORE: 1.0914821396909875                                                                                              \n",
      "SCORE: 0.9489798992481406                                                                                              \n",
      "SCORE: 0.7227250519101558                                                                                              \n",
      "SCORE: 0.6942206318544667                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.39trial/s, best loss: 0.6712907341585395]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.45841991593392495, 'n_estimators': 137.0}\n",
      "Test Performance after first tuning round: 0.901464764640104\n",
      "SCORE: 0.7286166110532099                                                                                              \n",
      "SCORE: 0.6362598908632389                                                                                              \n",
      "SCORE: 0.6646369908342089                                                                                              \n",
      "SCORE: 0.6367438764994138                                                                                              \n",
      "SCORE: 0.591638788324793                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.24trial/s, best loss: 0.591638788324793]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.45841991593392495, 'n_estimators': 137.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 0.7778919257944713\n",
      "SCORE: 0.5845173354337483                                                                                              \n",
      "SCORE: 0.5826218616398589                                                                                              \n",
      "SCORE: 0.6174177912801475                                                                                              \n",
      "SCORE: 0.6141453586094109                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.6000133933132923                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.69trial/s, best loss: 0.5826218616398589]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.45841991593392495, 'n_estimators': 137.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.6698051640038161, 'subsample': 0.5019866587213133}\n",
      "Test Performance after third tuning round: 0.7209656484855153\n",
      "SCORE: 0.7472486718198019                                                                                              \n",
      "SCORE: 0.8032155081232475                                                                                              \n",
      "SCORE: 0.7829031398645717                                                                                              \n",
      "SCORE: 0.8019236277941619                                                                                              \n",
      "SCORE: 0.8911164854218262                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.21trial/s, best loss: 0.7472486718198019]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.45841991593392495, 'n_estimators': 137.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.6698051640038161, 'subsample': 0.5019866587213133, 'gamma': 7.980250432074968, 'reg_alpha': 1.0, 'reg_lambda': 3.0295008576966516}\n",
      "Test Performance after last tuning round: 0.693192886691247\n",
      "Preparing results for fold 4, subset=all\n",
      "SCORE: 0.5675246655195354                                                                                              \n",
      "SCORE: 0.5692747380879382                                                                                              \n",
      "SCORE: 0.569832116472061                                                                                               \n",
      "SCORE: 0.571776688765047                                                                                               \n",
      "SCORE: 0.5701351908740019                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  8.01trial/s, best loss: 0.5675246655195354]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.41988919324648316}\n",
      "Default performance on Test: 0.6911163105365167\n",
      "SCORE: 0.6313559992011841                                                                                              \n",
      "SCORE: 0.6715135445998379                                                                                              \n",
      "SCORE: 0.6118344445883737                                                                                              \n",
      "SCORE: 0.6194530501274034                                                                                              \n",
      "SCORE: 0.8751043032407964                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.71trial/s, best loss: 0.6118344445883737]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.31303459082983137, 'n_estimators': 454.0}\n",
      "Test Performance after first tuning round: 0.7653172900100401\n",
      "SCORE: 0.6158627524433415                                                                                              \n",
      "SCORE: 0.5872122699200901                                                                                              \n",
      "SCORE: 0.5872122699200901                                                                                              \n",
      "SCORE: 0.5802263673631096                                                                                              \n",
      "SCORE: 0.6040847942245365                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.30trial/s, best loss: 0.5802263673631096]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.31303459082983137, 'n_estimators': 454.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.7317467137396282\n",
      "SCORE: 0.5831578729029628                                                                                              \n",
      "SCORE: 0.5983943751495581                                                                                              \n",
      "SCORE: 0.5759022368991655                                                                                              \n",
      "SCORE: 0.5978773516397543                                                                                              \n",
      "SCORE: 0.6132190173023495                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.27trial/s, best loss: 0.5759022368991655]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.31303459082983137, 'n_estimators': 454.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.9064895874694754, 'subsample': 0.7799044791809389}\n",
      "Test Performance after third tuning round: 0.7414605237482462\n",
      "SCORE: 0.715104123367569                                                                                               \n",
      "SCORE: 0.7752464571703455                                                                                              \n",
      "SCORE: 0.8101722143982114                                                                                              \n",
      "SCORE: 0.7736934584198929                                                                                              \n",
      "SCORE: 0.8826372772251692                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.49trial/s, best loss: 0.715104123367569]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.31303459082983137, 'n_estimators': 454.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.9064895874694754, 'subsample': 0.7799044791809389, 'gamma': 2.592140719415892, 'reg_alpha': 5.0, 'reg_lambda': 2.452934359030227}\n",
      "Test Performance after last tuning round: 0.6721278203780651\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_bdd91_row0_col0, #T_bdd91_row0_col1, #T_bdd91_row0_col2, #T_bdd91_row0_col3, #T_bdd91_row0_col4, #T_bdd91_row0_col5, #T_bdd91_row4_col0, #T_bdd91_row4_col1, #T_bdd91_row4_col2, #T_bdd91_row10_col2 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_bdd91\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_bdd91_level0_col0\" class=\"col_heading level0 col0\" >Accuracy Train</th>\n",
       "      <th id=\"T_bdd91_level0_col1\" class=\"col_heading level0 col1\" >F1 Train</th>\n",
       "      <th id=\"T_bdd91_level0_col2\" class=\"col_heading level0 col2\" >AUROC Train</th>\n",
       "      <th id=\"T_bdd91_level0_col3\" class=\"col_heading level0 col3\" >Accuracy Test</th>\n",
       "      <th id=\"T_bdd91_level0_col4\" class=\"col_heading level0 col4\" >F1 Test</th>\n",
       "      <th id=\"T_bdd91_level0_col5\" class=\"col_heading level0 col5\" >AUROC Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bdd91_level0_row0\" class=\"row_heading level0 row0\" >XGB_all</th>\n",
       "      <td id=\"T_bdd91_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_bdd91_row0_col1\" class=\"data row0 col1\" >1.000000</td>\n",
       "      <td id=\"T_bdd91_row0_col2\" class=\"data row0 col2\" >1.000000</td>\n",
       "      <td id=\"T_bdd91_row0_col3\" class=\"data row0 col3\" >0.781200</td>\n",
       "      <td id=\"T_bdd91_row0_col4\" class=\"data row0 col4\" >0.789200</td>\n",
       "      <td id=\"T_bdd91_row0_col5\" class=\"data row0 col5\" >0.908500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdd91_level0_row1\" class=\"row_heading level0 row1\" >LR_all_tuned</th>\n",
       "      <td id=\"T_bdd91_row1_col0\" class=\"data row1 col0\" >0.794300</td>\n",
       "      <td id=\"T_bdd91_row1_col1\" class=\"data row1 col1\" >0.796100</td>\n",
       "      <td id=\"T_bdd91_row1_col2\" class=\"data row1 col2\" >0.928500</td>\n",
       "      <td id=\"T_bdd91_row1_col3\" class=\"data row1 col3\" >0.729200</td>\n",
       "      <td id=\"T_bdd91_row1_col4\" class=\"data row1 col4\" >0.737200</td>\n",
       "      <td id=\"T_bdd91_row1_col5\" class=\"data row1 col5\" >0.884700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdd91_level0_row2\" class=\"row_heading level0 row2\" >LR_all</th>\n",
       "      <td id=\"T_bdd91_row2_col0\" class=\"data row2 col0\" >0.794300</td>\n",
       "      <td id=\"T_bdd91_row2_col1\" class=\"data row2 col1\" >0.796100</td>\n",
       "      <td id=\"T_bdd91_row2_col2\" class=\"data row2 col2\" >0.928600</td>\n",
       "      <td id=\"T_bdd91_row2_col3\" class=\"data row2 col3\" >0.729200</td>\n",
       "      <td id=\"T_bdd91_row2_col4\" class=\"data row2 col4\" >0.737200</td>\n",
       "      <td id=\"T_bdd91_row2_col5\" class=\"data row2 col5\" >0.884600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdd91_level0_row3\" class=\"row_heading level0 row3\" >XGB_all_tuned</th>\n",
       "      <td id=\"T_bdd91_row3_col0\" class=\"data row3 col0\" >0.830700</td>\n",
       "      <td id=\"T_bdd91_row3_col1\" class=\"data row3 col1\" >0.830500</td>\n",
       "      <td id=\"T_bdd91_row3_col2\" class=\"data row3 col2\" >0.938300</td>\n",
       "      <td id=\"T_bdd91_row3_col3\" class=\"data row3 col3\" >0.718800</td>\n",
       "      <td id=\"T_bdd91_row3_col4\" class=\"data row3 col4\" >0.730000</td>\n",
       "      <td id=\"T_bdd91_row3_col5\" class=\"data row3 col5\" >0.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdd91_level0_row4\" class=\"row_heading level0 row4\" >XGB_activity_and_demo</th>\n",
       "      <td id=\"T_bdd91_row4_col0\" class=\"data row4 col0\" >1.000000</td>\n",
       "      <td id=\"T_bdd91_row4_col1\" class=\"data row4 col1\" >1.000000</td>\n",
       "      <td id=\"T_bdd91_row4_col2\" class=\"data row4 col2\" >1.000000</td>\n",
       "      <td id=\"T_bdd91_row4_col3\" class=\"data row4 col3\" >0.708300</td>\n",
       "      <td id=\"T_bdd91_row4_col4\" class=\"data row4 col4\" >0.729800</td>\n",
       "      <td id=\"T_bdd91_row4_col5\" class=\"data row4 col5\" >0.875400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdd91_level0_row5\" class=\"row_heading level0 row5\" >LR_activity_and_demo_tuned</th>\n",
       "      <td id=\"T_bdd91_row5_col0\" class=\"data row5 col0\" >0.768200</td>\n",
       "      <td id=\"T_bdd91_row5_col1\" class=\"data row5 col1\" >0.769000</td>\n",
       "      <td id=\"T_bdd91_row5_col2\" class=\"data row5 col2\" >0.905600</td>\n",
       "      <td id=\"T_bdd91_row5_col3\" class=\"data row5 col3\" >0.708300</td>\n",
       "      <td id=\"T_bdd91_row5_col4\" class=\"data row5 col4\" >0.724600</td>\n",
       "      <td id=\"T_bdd91_row5_col5\" class=\"data row5 col5\" >0.889800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdd91_level0_row6\" class=\"row_heading level0 row6\" >LR_activity_only</th>\n",
       "      <td id=\"T_bdd91_row6_col0\" class=\"data row6 col0\" >0.726600</td>\n",
       "      <td id=\"T_bdd91_row6_col1\" class=\"data row6 col1\" >0.729200</td>\n",
       "      <td id=\"T_bdd91_row6_col2\" class=\"data row6 col2\" >0.883000</td>\n",
       "      <td id=\"T_bdd91_row6_col3\" class=\"data row6 col3\" >0.697900</td>\n",
       "      <td id=\"T_bdd91_row6_col4\" class=\"data row6 col4\" >0.718900</td>\n",
       "      <td id=\"T_bdd91_row6_col5\" class=\"data row6 col5\" >0.896900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdd91_level0_row7\" class=\"row_heading level0 row7\" >LR_activity_only_tuned</th>\n",
       "      <td id=\"T_bdd91_row7_col0\" class=\"data row7 col0\" >0.729200</td>\n",
       "      <td id=\"T_bdd91_row7_col1\" class=\"data row7 col1\" >0.731600</td>\n",
       "      <td id=\"T_bdd91_row7_col2\" class=\"data row7 col2\" >0.882700</td>\n",
       "      <td id=\"T_bdd91_row7_col3\" class=\"data row7 col3\" >0.697900</td>\n",
       "      <td id=\"T_bdd91_row7_col4\" class=\"data row7 col4\" >0.718900</td>\n",
       "      <td id=\"T_bdd91_row7_col5\" class=\"data row7 col5\" >0.896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdd91_level0_row8\" class=\"row_heading level0 row8\" >LR_activity_and_demo</th>\n",
       "      <td id=\"T_bdd91_row8_col0\" class=\"data row8 col0\" >0.768200</td>\n",
       "      <td id=\"T_bdd91_row8_col1\" class=\"data row8 col1\" >0.769600</td>\n",
       "      <td id=\"T_bdd91_row8_col2\" class=\"data row8 col2\" >0.907800</td>\n",
       "      <td id=\"T_bdd91_row8_col3\" class=\"data row8 col3\" >0.697900</td>\n",
       "      <td id=\"T_bdd91_row8_col4\" class=\"data row8 col4\" >0.715700</td>\n",
       "      <td id=\"T_bdd91_row8_col5\" class=\"data row8 col5\" >0.889300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdd91_level0_row9\" class=\"row_heading level0 row9\" >XGB_activity_and_demo_tuned</th>\n",
       "      <td id=\"T_bdd91_row9_col0\" class=\"data row9 col0\" >0.880200</td>\n",
       "      <td id=\"T_bdd91_row9_col1\" class=\"data row9 col1\" >0.880400</td>\n",
       "      <td id=\"T_bdd91_row9_col2\" class=\"data row9 col2\" >0.965000</td>\n",
       "      <td id=\"T_bdd91_row9_col3\" class=\"data row9 col3\" >0.677100</td>\n",
       "      <td id=\"T_bdd91_row9_col4\" class=\"data row9 col4\" >0.693500</td>\n",
       "      <td id=\"T_bdd91_row9_col5\" class=\"data row9 col5\" >0.876600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdd91_level0_row10\" class=\"row_heading level0 row10\" >XGB_activity_only</th>\n",
       "      <td id=\"T_bdd91_row10_col0\" class=\"data row10 col0\" >0.997400</td>\n",
       "      <td id=\"T_bdd91_row10_col1\" class=\"data row10 col1\" >0.997400</td>\n",
       "      <td id=\"T_bdd91_row10_col2\" class=\"data row10 col2\" >1.000000</td>\n",
       "      <td id=\"T_bdd91_row10_col3\" class=\"data row10 col3\" >0.645800</td>\n",
       "      <td id=\"T_bdd91_row10_col4\" class=\"data row10 col4\" >0.674400</td>\n",
       "      <td id=\"T_bdd91_row10_col5\" class=\"data row10 col5\" >0.863900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdd91_level0_row11\" class=\"row_heading level0 row11\" >XGB_activity_only_tuned</th>\n",
       "      <td id=\"T_bdd91_row11_col0\" class=\"data row11 col0\" >0.760400</td>\n",
       "      <td id=\"T_bdd91_row11_col1\" class=\"data row11 col1\" >0.745000</td>\n",
       "      <td id=\"T_bdd91_row11_col2\" class=\"data row11 col2\" >0.894700</td>\n",
       "      <td id=\"T_bdd91_row11_col3\" class=\"data row11 col3\" >0.604200</td>\n",
       "      <td id=\"T_bdd91_row11_col4\" class=\"data row11 col4\" >0.629500</td>\n",
       "      <td id=\"T_bdd91_row11_col5\" class=\"data row11 col5\" >0.863100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdd91_level0_row12\" class=\"row_heading level0 row12\" >XGB_demo_only</th>\n",
       "      <td id=\"T_bdd91_row12_col0\" class=\"data row12 col0\" >0.656200</td>\n",
       "      <td id=\"T_bdd91_row12_col1\" class=\"data row12 col1\" >0.650900</td>\n",
       "      <td id=\"T_bdd91_row12_col2\" class=\"data row12 col2\" >0.834500</td>\n",
       "      <td id=\"T_bdd91_row12_col3\" class=\"data row12 col3\" >0.510400</td>\n",
       "      <td id=\"T_bdd91_row12_col4\" class=\"data row12 col4\" >0.499100</td>\n",
       "      <td id=\"T_bdd91_row12_col5\" class=\"data row12 col5\" >0.694100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdd91_level0_row13\" class=\"row_heading level0 row13\" >XGB_demo_only_tuned</th>\n",
       "      <td id=\"T_bdd91_row13_col0\" class=\"data row13 col0\" >0.599000</td>\n",
       "      <td id=\"T_bdd91_row13_col1\" class=\"data row13 col1\" >0.596700</td>\n",
       "      <td id=\"T_bdd91_row13_col2\" class=\"data row13 col2\" >0.761900</td>\n",
       "      <td id=\"T_bdd91_row13_col3\" class=\"data row13 col3\" >0.500000</td>\n",
       "      <td id=\"T_bdd91_row13_col4\" class=\"data row13 col4\" >0.480700</td>\n",
       "      <td id=\"T_bdd91_row13_col5\" class=\"data row13 col5\" >0.685900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdd91_level0_row14\" class=\"row_heading level0 row14\" >LR_demo_only</th>\n",
       "      <td id=\"T_bdd91_row14_col0\" class=\"data row14 col0\" >0.578100</td>\n",
       "      <td id=\"T_bdd91_row14_col1\" class=\"data row14 col1\" >0.568000</td>\n",
       "      <td id=\"T_bdd91_row14_col2\" class=\"data row14 col2\" >0.737400</td>\n",
       "      <td id=\"T_bdd91_row14_col3\" class=\"data row14 col3\" >0.479200</td>\n",
       "      <td id=\"T_bdd91_row14_col4\" class=\"data row14 col4\" >0.466500</td>\n",
       "      <td id=\"T_bdd91_row14_col5\" class=\"data row14 col5\" >0.659900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdd91_level0_row15\" class=\"row_heading level0 row15\" >LR_demo_only_tuned</th>\n",
       "      <td id=\"T_bdd91_row15_col0\" class=\"data row15 col0\" >0.578100</td>\n",
       "      <td id=\"T_bdd91_row15_col1\" class=\"data row15 col1\" >0.568000</td>\n",
       "      <td id=\"T_bdd91_row15_col2\" class=\"data row15 col2\" >0.737100</td>\n",
       "      <td id=\"T_bdd91_row15_col3\" class=\"data row15 col3\" >0.479200</td>\n",
       "      <td id=\"T_bdd91_row15_col4\" class=\"data row15 col4\" >0.466500</td>\n",
       "      <td id=\"T_bdd91_row15_col5\" class=\"data row15 col5\" >0.661300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bdd91_level0_row16\" class=\"row_heading level0 row16\" >Baseline</th>\n",
       "      <td id=\"T_bdd91_row16_col0\" class=\"data row16 col0\" >0.455700</td>\n",
       "      <td id=\"T_bdd91_row16_col1\" class=\"data row16 col1\" >0.208700</td>\n",
       "      <td id=\"T_bdd91_row16_col2\" class=\"data row16 col2\" >0.500000</td>\n",
       "      <td id=\"T_bdd91_row16_col3\" class=\"data row16 col3\" >0.375000</td>\n",
       "      <td id=\"T_bdd91_row16_col4\" class=\"data row16 col4\" >0.181800</td>\n",
       "      <td id=\"T_bdd91_row16_col5\" class=\"data row16 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b80827d3a0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(f\"../results/{dataset_name}/{experiment_name}/results_subsets.pickle\"):\n",
    "\n",
    "    results_subsets = {}\n",
    "    results_subsets_feature_importances = {}\n",
    "\n",
    "    for fold in range(folds):\n",
    "        results_subsets[fold] = {}\n",
    "        results_subsets_feature_importances[fold] = {}\n",
    "        # Create baseline\n",
    "        y_train = data_dict[f\"y_train_{fold}\"]\n",
    "        y_val = data_dict[f\"y_val_{fold}\"]\n",
    "        y_test = data_dict[f\"y_test_{fold}\"]\n",
    "        y_train_val = np.concatenate([y_train,y_val])\n",
    "\n",
    "        u,c = np.unique(y_train_val,return_counts=True)\n",
    "        nb_classes = len(u)\n",
    "        baseline = np.argmax(c)\n",
    "\n",
    "        y_train_val_pred_base = np.ones(y_train_val.shape[0])*baseline\n",
    "        y_test_pred_base = np.ones(y_test.shape[0])*baseline\n",
    "\n",
    "        results_subsets[fold][\"Baseline\"] = {}\n",
    "        eval_res_train = get_metrics(get_one_hot(y_train_val, nb_classes), get_one_hot(y_train_val_pred_base.astype(int), nb_classes), target=target)\n",
    "        for metric in eval_res_train.keys():\n",
    "            results_subsets[fold][\"Baseline\"][metric + \" Train\"] = eval_res_train[metric]\n",
    "        eval_res_test = get_metrics(get_one_hot(y_test, nb_classes), get_one_hot(y_test_pred_base.astype(int), nb_classes), target=target)\n",
    "        for metric in eval_res_test.keys():\n",
    "            results_subsets[fold][\"Baseline\"][metric + \" Test\"] = eval_res_test[metric]\n",
    "\n",
    "\n",
    "        for subset_key in subsets:\n",
    "            print(f\"Preparing results for fold {fold}, subset={subset_key}\")\n",
    "            # Retrieve data\n",
    "            z_cols = data_dict[\"z_cols\"]\n",
    "\n",
    "            X_train = data_dict[f\"X_train_{fold}\"]\n",
    "            y_train = data_dict[f\"y_train_{fold}\"]\n",
    "\n",
    "            X_val = data_dict[f\"X_val_{fold}\"]\n",
    "            y_val = data_dict[f\"y_val_{fold}\"]\n",
    "\n",
    "            X_test = data_dict[f\"X_test_{fold}\"]\n",
    "            y_test = data_dict[f\"y_test_{fold}\"]\n",
    "        \n",
    "            y_train_val = np.concatenate([y_train,y_val])\n",
    "\n",
    "            # Define data subset for LR\n",
    "            z_glmm_encoded_train = data_dict[f\"z_glmm_encoded_train_{fold}\"] \n",
    "            z_glmm_encoded_val = data_dict[f\"z_glmm_encoded_val_{fold}\"] \n",
    "            z_glmm_encoded_test = data_dict[f\"z_glmm_encoded_test_{fold}\"] \n",
    "            X_train_lr = pd.concat([X_train,z_glmm_encoded_train],axis=1)\n",
    "            X_val_lr = pd.concat([X_val,z_glmm_encoded_val],axis=1)\n",
    "            X_test_lr = pd.concat([X_test,z_glmm_encoded_test],axis=1)      \n",
    "            X_train_val_lr = pd.concat([X_train_lr,X_val_lr])\n",
    "\n",
    "            # Define data subset for XGB\n",
    "            z_ordinal_encoded_train = data_dict[f\"z_ordinal_encoded_train_{fold}\"] \n",
    "            z_ordinal_encoded_val = data_dict[f\"z_ordinal_encoded_val_{fold}\"] \n",
    "            z_ordinal_encoded_test = data_dict[f\"z_ordinal_encoded_test_{fold}\"] \n",
    "            X_train_xgb = pd.concat([X_train,z_ordinal_encoded_train],axis=1)\n",
    "            X_val_xgb = pd.concat([X_val,z_ordinal_encoded_val],axis=1)\n",
    "            X_test_xgb = pd.concat([X_test,z_ordinal_encoded_test],axis=1)\n",
    "            X_train_val_xgb = pd.concat([X_train_xgb,X_val_xgb])\n",
    "\n",
    "\n",
    "            # Define data subset for evaluation\n",
    "            X_train_val_lr = X_train_val_lr[[i for i in X_train_val_lr.columns if any([j in i for j in subsets[subset_key]])]]\n",
    "            X_test_lr = X_test_lr[[i for i in X_test_lr.columns if any([j in i for j in subsets[subset_key]])]]\n",
    "            X_train_val_xgb = X_train_val_xgb[[i for i in X_train_val_xgb.columns if any([j in i for j in subsets[subset_key]])]]\n",
    "            X_test_xgb = X_test_xgb[[i for i in X_test_xgb.columns if any([j in i for j in subsets[subset_key]])]]\n",
    "\n",
    "\n",
    "            # Train base models\n",
    "            res, feats = evaluate_logreg(X_train_val_lr, y_train_val, X_test_lr, y_test, target=target,tune=False, seed=RS)\n",
    "            results_subsets[fold][\"LR_\"+subset_key] = res\n",
    "            results_subsets_feature_importances[fold][\"LR_\"+subset_key] = feats\n",
    "\n",
    "            res, feats = evaluate_xgb(X_train_val_xgb, y_train_val, X_test_xgb, y_test, target, tune=False, max_evals=max_evals, early_stopping_rounds=early_stopping_rounds, seed=RS)\n",
    "            results_subsets[fold][\"XGB_\"+subset_key] = res\n",
    "            results_subsets_feature_importances[fold][\"XGB_\"+subset_key] = feats\n",
    "\n",
    "            # Train tuned models\n",
    "            res, feats = evaluate_logreg(X_train_val_lr, y_train_val, X_test_lr, y_test, target=target, max_evals=max_evals, tune=True, seed=RS)\n",
    "            results_subsets[fold][\"LR_\"+subset_key+\"_tuned\"] = res\n",
    "            results_subsets_feature_importances[fold][\"LR_\"+subset_key+\"_tuned\"] = feats\n",
    "\n",
    "            res, feats = evaluate_xgb(X_train_val_xgb, y_train_val, X_test_xgb, y_test, target, tune=True, max_evals=max_evals, early_stopping_rounds=early_stopping_rounds, seed=RS)\n",
    "            results_subsets[fold][\"XGB_\"+subset_key+\"_tuned\"] = res\n",
    "            results_subsets_feature_importances[fold][\"XGB_\"+subset_key+\"_tuned\"] = feats\n",
    "    \n",
    "    if not os.path.exists(f\"../results/{dataset_name}/{experiment_name}\"):\n",
    "        os.makedirs(f\"../results/{dataset_name}/{experiment_name}\")\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_subsets.pickle\", 'wb') as handle:\n",
    "        pickle.dump(results_subsets, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_subsets_feature_importances.pickle\", 'wb') as handle:\n",
    "        pickle.dump(results_subsets_feature_importances, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "else:\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_subsets.pickle\", 'rb') as handle:\n",
    "        results_subsets = pickle.load(handle)\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_subsets_feature_importances.pickle\", 'rb') as handle:\n",
    "        results_subsets_feature_importances = pickle.load(handle)\n",
    "        \n",
    "        \n",
    "results_subsets_df = pd.DataFrame(results_subsets[0]).transpose().sort_values(\"F1 Test\",ascending=False).round(4)\n",
    "results_subsets_df[[\"Accuracy Train\", \"F1 Train\", \"AUROC Train\", \"Accuracy Test\", \"F1 Test\", \"AUROC Test\"]].style.highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb36703b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_de766_row0_col3, #T_de766_row0_col4, #T_de766_row1_col0, #T_de766_row1_col1, #T_de766_row1_col2, #T_de766_row3_col5, #T_de766_row4_col0, #T_de766_row4_col1, #T_de766_row4_col2, #T_de766_row7_col0, #T_de766_row7_col1, #T_de766_row7_col2 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_de766\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_de766_level0_col0\" class=\"col_heading level0 col0\" >Accuracy Train</th>\n",
       "      <th id=\"T_de766_level0_col1\" class=\"col_heading level0 col1\" >F1 Train</th>\n",
       "      <th id=\"T_de766_level0_col2\" class=\"col_heading level0 col2\" >AUROC Train</th>\n",
       "      <th id=\"T_de766_level0_col3\" class=\"col_heading level0 col3\" >Accuracy Test</th>\n",
       "      <th id=\"T_de766_level0_col4\" class=\"col_heading level0 col4\" >F1 Test</th>\n",
       "      <th id=\"T_de766_level0_col5\" class=\"col_heading level0 col5\" >AUROC Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_de766_level0_row0\" class=\"row_heading level0 row0\" >LR_all</th>\n",
       "      <td id=\"T_de766_row0_col0\" class=\"data row0 col0\" >0.781200</td>\n",
       "      <td id=\"T_de766_row0_col1\" class=\"data row0 col1\" >0.790200</td>\n",
       "      <td id=\"T_de766_row0_col2\" class=\"data row0 col2\" >0.918200</td>\n",
       "      <td id=\"T_de766_row0_col3\" class=\"data row0 col3\" >0.822900</td>\n",
       "      <td id=\"T_de766_row0_col4\" class=\"data row0 col4\" >0.824300</td>\n",
       "      <td id=\"T_de766_row0_col5\" class=\"data row0 col5\" >0.931300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_de766_level0_row1\" class=\"row_heading level0 row1\" >XGB_activity_and_demo</th>\n",
       "      <td id=\"T_de766_row1_col0\" class=\"data row1 col0\" >1.000000</td>\n",
       "      <td id=\"T_de766_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "      <td id=\"T_de766_row1_col2\" class=\"data row1 col2\" >1.000000</td>\n",
       "      <td id=\"T_de766_row1_col3\" class=\"data row1 col3\" >0.812500</td>\n",
       "      <td id=\"T_de766_row1_col4\" class=\"data row1 col4\" >0.813600</td>\n",
       "      <td id=\"T_de766_row1_col5\" class=\"data row1 col5\" >0.907800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_de766_level0_row2\" class=\"row_heading level0 row2\" >XGB_all_tuned</th>\n",
       "      <td id=\"T_de766_row2_col0\" class=\"data row2 col0\" >0.783900</td>\n",
       "      <td id=\"T_de766_row2_col1\" class=\"data row2 col1\" >0.790900</td>\n",
       "      <td id=\"T_de766_row2_col2\" class=\"data row2 col2\" >0.914300</td>\n",
       "      <td id=\"T_de766_row2_col3\" class=\"data row2 col3\" >0.802100</td>\n",
       "      <td id=\"T_de766_row2_col4\" class=\"data row2 col4\" >0.806100</td>\n",
       "      <td id=\"T_de766_row2_col5\" class=\"data row2 col5\" >0.927600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_de766_level0_row3\" class=\"row_heading level0 row3\" >LR_all_tuned</th>\n",
       "      <td id=\"T_de766_row3_col0\" class=\"data row3 col0\" >0.781200</td>\n",
       "      <td id=\"T_de766_row3_col1\" class=\"data row3 col1\" >0.790600</td>\n",
       "      <td id=\"T_de766_row3_col2\" class=\"data row3 col2\" >0.915900</td>\n",
       "      <td id=\"T_de766_row3_col3\" class=\"data row3 col3\" >0.802100</td>\n",
       "      <td id=\"T_de766_row3_col4\" class=\"data row3 col4\" >0.803300</td>\n",
       "      <td id=\"T_de766_row3_col5\" class=\"data row3 col5\" >0.933800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_de766_level0_row4\" class=\"row_heading level0 row4\" >XGB_all</th>\n",
       "      <td id=\"T_de766_row4_col0\" class=\"data row4 col0\" >1.000000</td>\n",
       "      <td id=\"T_de766_row4_col1\" class=\"data row4 col1\" >1.000000</td>\n",
       "      <td id=\"T_de766_row4_col2\" class=\"data row4 col2\" >1.000000</td>\n",
       "      <td id=\"T_de766_row4_col3\" class=\"data row4 col3\" >0.802100</td>\n",
       "      <td id=\"T_de766_row4_col4\" class=\"data row4 col4\" >0.802300</td>\n",
       "      <td id=\"T_de766_row4_col5\" class=\"data row4 col5\" >0.922300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_de766_level0_row5\" class=\"row_heading level0 row5\" >XGB_activity_and_demo_tuned</th>\n",
       "      <td id=\"T_de766_row5_col0\" class=\"data row5 col0\" >0.794300</td>\n",
       "      <td id=\"T_de766_row5_col1\" class=\"data row5 col1\" >0.801700</td>\n",
       "      <td id=\"T_de766_row5_col2\" class=\"data row5 col2\" >0.918700</td>\n",
       "      <td id=\"T_de766_row5_col3\" class=\"data row5 col3\" >0.791700</td>\n",
       "      <td id=\"T_de766_row5_col4\" class=\"data row5 col4\" >0.795600</td>\n",
       "      <td id=\"T_de766_row5_col5\" class=\"data row5 col5\" >0.928100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_de766_level0_row6\" class=\"row_heading level0 row6\" >LR_activity_only_tuned</th>\n",
       "      <td id=\"T_de766_row6_col0\" class=\"data row6 col0\" >0.718800</td>\n",
       "      <td id=\"T_de766_row6_col1\" class=\"data row6 col1\" >0.732600</td>\n",
       "      <td id=\"T_de766_row6_col2\" class=\"data row6 col2\" >0.879500</td>\n",
       "      <td id=\"T_de766_row6_col3\" class=\"data row6 col3\" >0.791700</td>\n",
       "      <td id=\"T_de766_row6_col4\" class=\"data row6 col4\" >0.794800</td>\n",
       "      <td id=\"T_de766_row6_col5\" class=\"data row6 col5\" >0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_de766_level0_row7\" class=\"row_heading level0 row7\" >XGB_activity_only</th>\n",
       "      <td id=\"T_de766_row7_col0\" class=\"data row7 col0\" >1.000000</td>\n",
       "      <td id=\"T_de766_row7_col1\" class=\"data row7 col1\" >1.000000</td>\n",
       "      <td id=\"T_de766_row7_col2\" class=\"data row7 col2\" >1.000000</td>\n",
       "      <td id=\"T_de766_row7_col3\" class=\"data row7 col3\" >0.791700</td>\n",
       "      <td id=\"T_de766_row7_col4\" class=\"data row7 col4\" >0.789100</td>\n",
       "      <td id=\"T_de766_row7_col5\" class=\"data row7 col5\" >0.907800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_de766_level0_row8\" class=\"row_heading level0 row8\" >XGB_activity_only_tuned</th>\n",
       "      <td id=\"T_de766_row8_col0\" class=\"data row8 col0\" >0.760400</td>\n",
       "      <td id=\"T_de766_row8_col1\" class=\"data row8 col1\" >0.772700</td>\n",
       "      <td id=\"T_de766_row8_col2\" class=\"data row8 col2\" >0.913000</td>\n",
       "      <td id=\"T_de766_row8_col3\" class=\"data row8 col3\" >0.781200</td>\n",
       "      <td id=\"T_de766_row8_col4\" class=\"data row8 col4\" >0.786000</td>\n",
       "      <td id=\"T_de766_row8_col5\" class=\"data row8 col5\" >0.913600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_de766_level0_row9\" class=\"row_heading level0 row9\" >LR_activity_only</th>\n",
       "      <td id=\"T_de766_row9_col0\" class=\"data row9 col0\" >0.724000</td>\n",
       "      <td id=\"T_de766_row9_col1\" class=\"data row9 col1\" >0.737400</td>\n",
       "      <td id=\"T_de766_row9_col2\" class=\"data row9 col2\" >0.879600</td>\n",
       "      <td id=\"T_de766_row9_col3\" class=\"data row9 col3\" >0.781200</td>\n",
       "      <td id=\"T_de766_row9_col4\" class=\"data row9 col4\" >0.784400</td>\n",
       "      <td id=\"T_de766_row9_col5\" class=\"data row9 col5\" >0.910300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_de766_level0_row10\" class=\"row_heading level0 row10\" >LR_activity_and_demo</th>\n",
       "      <td id=\"T_de766_row10_col0\" class=\"data row10 col0\" >0.765600</td>\n",
       "      <td id=\"T_de766_row10_col1\" class=\"data row10 col1\" >0.775000</td>\n",
       "      <td id=\"T_de766_row10_col2\" class=\"data row10 col2\" >0.899400</td>\n",
       "      <td id=\"T_de766_row10_col3\" class=\"data row10 col3\" >0.750000</td>\n",
       "      <td id=\"T_de766_row10_col4\" class=\"data row10 col4\" >0.753000</td>\n",
       "      <td id=\"T_de766_row10_col5\" class=\"data row10 col5\" >0.922900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_de766_level0_row11\" class=\"row_heading level0 row11\" >LR_activity_and_demo_tuned</th>\n",
       "      <td id=\"T_de766_row11_col0\" class=\"data row11 col0\" >0.765600</td>\n",
       "      <td id=\"T_de766_row11_col1\" class=\"data row11 col1\" >0.775000</td>\n",
       "      <td id=\"T_de766_row11_col2\" class=\"data row11 col2\" >0.899300</td>\n",
       "      <td id=\"T_de766_row11_col3\" class=\"data row11 col3\" >0.750000</td>\n",
       "      <td id=\"T_de766_row11_col4\" class=\"data row11 col4\" >0.753000</td>\n",
       "      <td id=\"T_de766_row11_col5\" class=\"data row11 col5\" >0.923400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_de766_level0_row12\" class=\"row_heading level0 row12\" >XGB_demo_only</th>\n",
       "      <td id=\"T_de766_row12_col0\" class=\"data row12 col0\" >0.630200</td>\n",
       "      <td id=\"T_de766_row12_col1\" class=\"data row12 col1\" >0.627500</td>\n",
       "      <td id=\"T_de766_row12_col2\" class=\"data row12 col2\" >0.815400</td>\n",
       "      <td id=\"T_de766_row12_col3\" class=\"data row12 col3\" >0.604200</td>\n",
       "      <td id=\"T_de766_row12_col4\" class=\"data row12 col4\" >0.596400</td>\n",
       "      <td id=\"T_de766_row12_col5\" class=\"data row12 col5\" >0.745800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_de766_level0_row13\" class=\"row_heading level0 row13\" >LR_demo_only</th>\n",
       "      <td id=\"T_de766_row13_col0\" class=\"data row13 col0\" >0.533900</td>\n",
       "      <td id=\"T_de766_row13_col1\" class=\"data row13 col1\" >0.527600</td>\n",
       "      <td id=\"T_de766_row13_col2\" class=\"data row13 col2\" >0.727600</td>\n",
       "      <td id=\"T_de766_row13_col3\" class=\"data row13 col3\" >0.562500</td>\n",
       "      <td id=\"T_de766_row13_col4\" class=\"data row13 col4\" >0.559300</td>\n",
       "      <td id=\"T_de766_row13_col5\" class=\"data row13 col5\" >0.756300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_de766_level0_row14\" class=\"row_heading level0 row14\" >LR_demo_only_tuned</th>\n",
       "      <td id=\"T_de766_row14_col0\" class=\"data row14 col0\" >0.533900</td>\n",
       "      <td id=\"T_de766_row14_col1\" class=\"data row14 col1\" >0.527600</td>\n",
       "      <td id=\"T_de766_row14_col2\" class=\"data row14 col2\" >0.727500</td>\n",
       "      <td id=\"T_de766_row14_col3\" class=\"data row14 col3\" >0.562500</td>\n",
       "      <td id=\"T_de766_row14_col4\" class=\"data row14 col4\" >0.559300</td>\n",
       "      <td id=\"T_de766_row14_col5\" class=\"data row14 col5\" >0.756300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_de766_level0_row15\" class=\"row_heading level0 row15\" >XGB_demo_only_tuned</th>\n",
       "      <td id=\"T_de766_row15_col0\" class=\"data row15 col0\" >0.513000</td>\n",
       "      <td id=\"T_de766_row15_col1\" class=\"data row15 col1\" >0.380400</td>\n",
       "      <td id=\"T_de766_row15_col2\" class=\"data row15 col2\" >0.656400</td>\n",
       "      <td id=\"T_de766_row15_col3\" class=\"data row15 col3\" >0.531200</td>\n",
       "      <td id=\"T_de766_row15_col4\" class=\"data row15 col4\" >0.413100</td>\n",
       "      <td id=\"T_de766_row15_col5\" class=\"data row15 col5\" >0.681800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_de766_level0_row16\" class=\"row_heading level0 row16\" >Baseline</th>\n",
       "      <td id=\"T_de766_row16_col0\" class=\"data row16 col0\" >0.437500</td>\n",
       "      <td id=\"T_de766_row16_col1\" class=\"data row16 col1\" >0.202900</td>\n",
       "      <td id=\"T_de766_row16_col2\" class=\"data row16 col2\" >0.500000</td>\n",
       "      <td id=\"T_de766_row16_col3\" class=\"data row16 col3\" >0.447900</td>\n",
       "      <td id=\"T_de766_row16_col4\" class=\"data row16 col4\" >0.206200</td>\n",
       "      <td id=\"T_de766_row16_col5\" class=\"data row16 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b808402cd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_subsets_df = pd.DataFrame(results_subsets[1]).transpose().sort_values(\"F1 Test\",ascending=False).round(4)\n",
    "results_subsets_df[[\"Accuracy Train\", \"F1 Train\", \"AUROC Train\", \"Accuracy Test\", \"F1 Test\", \"AUROC Test\"]].style.highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b731e1b0",
   "metadata": {},
   "source": [
    "### Effectiveness of Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "513536a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e33f7_row0_col0, #T_e33f7_row0_col1, #T_e33f7_row1_col0, #T_e33f7_row1_col1, #T_e33f7_row2_col1, #T_e33f7_row3_col0, #T_e33f7_row3_col1, #T_e33f7_row4_col1, #T_e33f7_row5_col0, #T_e33f7_row6_col0, #T_e33f7_row7_col0, #T_e33f7_row8_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e33f7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e33f7_level0_col0\" class=\"col_heading level0 col0\" >Untuned</th>\n",
       "      <th id=\"T_e33f7_level0_col1\" class=\"col_heading level0 col1\" >Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e33f7_level0_row0\" class=\"row_heading level0 row0\" >Baseline</th>\n",
       "      <td id=\"T_e33f7_row0_col0\" class=\"data row0 col0\" >0.200000</td>\n",
       "      <td id=\"T_e33f7_row0_col1\" class=\"data row0 col1\" >0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e33f7_level0_row1\" class=\"row_heading level0 row1\" >LR_activity_and_demo</th>\n",
       "      <td id=\"T_e33f7_row1_col0\" class=\"data row1 col0\" >0.720000</td>\n",
       "      <td id=\"T_e33f7_row1_col1\" class=\"data row1 col1\" >0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e33f7_level0_row2\" class=\"row_heading level0 row2\" >LR_activity_only</th>\n",
       "      <td id=\"T_e33f7_row2_col0\" class=\"data row2 col0\" >0.720000</td>\n",
       "      <td id=\"T_e33f7_row2_col1\" class=\"data row2 col1\" >0.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e33f7_level0_row3\" class=\"row_heading level0 row3\" >LR_all</th>\n",
       "      <td id=\"T_e33f7_row3_col0\" class=\"data row3 col0\" >0.750000</td>\n",
       "      <td id=\"T_e33f7_row3_col1\" class=\"data row3 col1\" >0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e33f7_level0_row4\" class=\"row_heading level0 row4\" >LR_demo_only</th>\n",
       "      <td id=\"T_e33f7_row4_col0\" class=\"data row4 col0\" >0.500000</td>\n",
       "      <td id=\"T_e33f7_row4_col1\" class=\"data row4 col1\" >0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e33f7_level0_row5\" class=\"row_heading level0 row5\" >XGB_activity_and_demo</th>\n",
       "      <td id=\"T_e33f7_row5_col0\" class=\"data row5 col0\" >0.760000</td>\n",
       "      <td id=\"T_e33f7_row5_col1\" class=\"data row5 col1\" >0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e33f7_level0_row6\" class=\"row_heading level0 row6\" >XGB_activity_only</th>\n",
       "      <td id=\"T_e33f7_row6_col0\" class=\"data row6 col0\" >0.720000</td>\n",
       "      <td id=\"T_e33f7_row6_col1\" class=\"data row6 col1\" >0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e33f7_level0_row7\" class=\"row_heading level0 row7\" >XGB_all</th>\n",
       "      <td id=\"T_e33f7_row7_col0\" class=\"data row7 col0\" >0.780000</td>\n",
       "      <td id=\"T_e33f7_row7_col1\" class=\"data row7 col1\" >0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e33f7_level0_row8\" class=\"row_heading level0 row8\" >XGB_demo_only</th>\n",
       "      <td id=\"T_e33f7_row8_col0\" class=\"data row8 col0\" >0.540000</td>\n",
       "      <td id=\"T_e33f7_row8_col1\" class=\"data row8 col1\" >0.490000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b85c39f520>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = results_subsets[0].keys()\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "subsets_folds_df = pd.DataFrame([pd.DataFrame(results_subsets[fold_num]).loc[metric,models] for fold_num in results_subsets.keys()],index=results_subsets.keys())\n",
    "subsets_mean_df = subsets_folds_df.mean(axis=0)\n",
    "subsets_std_df = subsets_folds_df.std(axis=0)\n",
    "\n",
    "methods = sorted(list(subsets_mean_df.index))[1:]\n",
    "not_tuned = [\"Baseline\"]+methods[::2]\n",
    "tuned = [\"Baseline\"]+methods[1::2]\n",
    "\n",
    "res_df_tune_comp_mean = pd.DataFrame([subsets_mean_df.loc[not_tuned].values,subsets_mean_df.loc[tuned]],index=[\"Untuned\",\"Tuned\"],columns=not_tuned).transpose()\n",
    "res_df_tune_comp_std = pd.DataFrame([subsets_std_df.loc[not_tuned].values,subsets_std_df.loc[tuned]],index=[\"Untuned\",\"Tuned\"],columns=not_tuned).transpose()\n",
    "\n",
    "res_df_tune_comp_mean.round(2).style.highlight_max(color = 'lightgreen', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8370957b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Untuned</th>\n",
       "      <th>Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.2 (0.016)</td>\n",
       "      <td>0.2 (0.016)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_activity_and_demo</th>\n",
       "      <td>0.72 (0.022)</td>\n",
       "      <td>0.72 (0.022)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_activity_only</th>\n",
       "      <td>0.72 (0.037)</td>\n",
       "      <td>0.73 (0.04)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_all</th>\n",
       "      <td>0.75 (0.047)</td>\n",
       "      <td>0.75 (0.045)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_demo_only</th>\n",
       "      <td>0.5 (0.047)</td>\n",
       "      <td>0.51 (0.041)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_activity_and_demo</th>\n",
       "      <td>0.76 (0.036)</td>\n",
       "      <td>0.72 (0.047)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_activity_only</th>\n",
       "      <td>0.72 (0.048)</td>\n",
       "      <td>0.7 (0.057)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_all</th>\n",
       "      <td>0.78 (0.015)</td>\n",
       "      <td>0.75 (0.035)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_demo_only</th>\n",
       "      <td>0.54 (0.048)</td>\n",
       "      <td>0.49 (0.052)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Untuned         Tuned\n",
       "Baseline                0.2 (0.016)   0.2 (0.016)\n",
       "LR_activity_and_demo   0.72 (0.022)  0.72 (0.022)\n",
       "LR_activity_only       0.72 (0.037)   0.73 (0.04)\n",
       "LR_all                 0.75 (0.047)  0.75 (0.045)\n",
       "LR_demo_only            0.5 (0.047)  0.51 (0.041)\n",
       "XGB_activity_and_demo  0.76 (0.036)  0.72 (0.047)\n",
       "XGB_activity_only      0.72 (0.048)   0.7 (0.057)\n",
       "XGB_all                0.78 (0.015)  0.75 (0.035)\n",
       "XGB_demo_only          0.54 (0.048)  0.49 (0.052)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_df = res_df_tune_comp_mean.round(2).astype(str) + \" (\" +  res_df_tune_comp_std.round(3).astype(str) + \")\"\n",
    "latex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ba86b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_activity_and_demo</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_activity_only</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_all</th>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_demo_only</th>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_activity_and_demo</th>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_activity_only</th>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_all</th>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_demo_only</th>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Tuned\n",
       "Baseline                0.00\n",
       "LR_activity_and_demo    0.00\n",
       "LR_activity_only        0.00\n",
       "LR_all                 -0.01\n",
       "LR_demo_only            0.01\n",
       "XGB_activity_and_demo  -0.05\n",
       "XGB_activity_only      -0.01\n",
       "XGB_all                -0.03\n",
       "XGB_demo_only          -0.05"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df_tune_comp_diff = res_df_tune_comp_mean[[\"Tuned\"]]-res_df_tune_comp_mean[[\"Untuned\"]].values\n",
    "res_df_tune_comp_diff.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b7f5278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_and_demo</th>\n",
       "      <td>-0.047662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_only</th>\n",
       "      <td>-0.013039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>-0.033293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>demo_only</th>\n",
       "      <td>-0.045561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Tuned\n",
       "Baseline           0.000000\n",
       "activity_and_demo -0.047662\n",
       "activity_only     -0.013039\n",
       "all               -0.033293\n",
       "demo_only         -0.045561"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df_tune_comp_diff_lr = res_df_tune_comp_diff.loc[[i for i in res_df_tune_comp_diff.index if (i == \"Baseline\" or \"LR\" in i)]]\n",
    "res_df_tune_comp_diff_xgb = res_df_tune_comp_diff.loc[[i for i in res_df_tune_comp_diff.index if (i == \"Baseline\" or \"XGB\" in i)]]\n",
    "\n",
    "res_df_tune_comp_diff_lr.index = [i[3:] if i!=\"Baseline\" else \"Baseline\" for i in res_df_tune_comp_diff_lr.index]\n",
    "res_df_tune_comp_diff_xgb.index = [i[4:] if i!=\"Baseline\" else \"Baseline\" for i in res_df_tune_comp_diff_xgb.index]\n",
    "res_df_tune_comp_diff_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c17cb615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>XGB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_and_demo</th>\n",
       "      <td>0.003645</td>\n",
       "      <td>-0.047662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_only</th>\n",
       "      <td>0.004012</td>\n",
       "      <td>-0.013039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>-0.005774</td>\n",
       "      <td>-0.033293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>demo_only</th>\n",
       "      <td>0.006388</td>\n",
       "      <td>-0.045561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         LR       XGB\n",
       "Baseline           0.000000  0.000000\n",
       "activity_and_demo  0.003645 -0.047662\n",
       "activity_only      0.004012 -0.013039\n",
       "all               -0.005774 -0.033293\n",
       "demo_only          0.006388 -0.045561"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_df_diff = pd.concat([res_df_tune_comp_diff_lr,res_df_tune_comp_diff_xgb],axis=1)\n",
    "latex_df_diff.columns = [\"LR\", \"XGB\"]\n",
    "latex_df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f679ecbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "{} &    LR &   XGB \\\\\n",
      "\\midrule\n",
      "Baseline          &  0.00 &  0.00 \\\\\n",
      "activity\\_and\\_demo &  0.00 & -0.05 \\\\\n",
      "activity\\_only     &  0.00 & -0.01 \\\\\n",
      "all               & -0.01 & -0.03 \\\\\n",
      "demo\\_only         &  0.01 & -0.05 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_df_diff.round(2).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492a1f62",
   "metadata": {},
   "source": [
    "### Performance Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff6a58a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b426d_row0_col2, #T_b426d_row0_col3, #T_b426d_row0_col4 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b426d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b426d_level0_col0\" class=\"col_heading level0 col0\" >Baseline</th>\n",
       "      <th id=\"T_b426d_level0_col1\" class=\"col_heading level0 col1\" >LR_demo_only_tuned</th>\n",
       "      <th id=\"T_b426d_level0_col2\" class=\"col_heading level0 col2\" >LR_activity_only_tuned</th>\n",
       "      <th id=\"T_b426d_level0_col3\" class=\"col_heading level0 col3\" >LR_activity_and_demo_tuned</th>\n",
       "      <th id=\"T_b426d_level0_col4\" class=\"col_heading level0 col4\" >LR_all_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b426d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b426d_row0_col0\" class=\"data row0 col0\" >0.203 (0.016)</td>\n",
       "      <td id=\"T_b426d_row0_col1\" class=\"data row0 col1\" >0.508 (0.041)</td>\n",
       "      <td id=\"T_b426d_row0_col2\" class=\"data row0 col2\" >0.726 (0.04)</td>\n",
       "      <td id=\"T_b426d_row0_col3\" class=\"data row0 col3\" >0.721 (0.022)</td>\n",
       "      <td id=\"T_b426d_row0_col4\" class=\"data row0 col4\" >0.746 (0.045)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b808367670>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For LR\n",
    "models = [\"Baseline\"]+[i for i in results_subsets[0].keys() if (\"tuned\" in i and \"LR\" in i)]\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "use_df = pd.DataFrame([pd.DataFrame(results_subsets[fold_num]).loc[metric,models] for fold_num in results_subsets.keys()],index=results_subsets.keys())\n",
    "\n",
    "df_mean = pd.DataFrame((use_df).mean(axis=0).round(3).astype(str) + \" (\" + use_df.std(axis=0).round(3).astype(str) + \")\").transpose()\n",
    "model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "\n",
    "best_model = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "t_test_res = np.array([stats.ttest_rel(use_df[best_model].values, use_df[model].values)[1] for model in models]).round(3)\n",
    "t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    \n",
    "res_df_lr = pd.DataFrame([model_dict])\n",
    "\n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_res[i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "res_df_lr.style.apply(negative_bold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd8406e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e2b73_row0_col2, #T_e2b73_row0_col4 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e2b73\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e2b73_level0_col0\" class=\"col_heading level0 col0\" >Baseline</th>\n",
       "      <th id=\"T_e2b73_level0_col1\" class=\"col_heading level0 col1\" >XGB_demo_only_tuned</th>\n",
       "      <th id=\"T_e2b73_level0_col2\" class=\"col_heading level0 col2\" >XGB_activity_only_tuned</th>\n",
       "      <th id=\"T_e2b73_level0_col3\" class=\"col_heading level0 col3\" >XGB_activity_and_demo_tuned</th>\n",
       "      <th id=\"T_e2b73_level0_col4\" class=\"col_heading level0 col4\" >XGB_all_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e2b73_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e2b73_row0_col0\" class=\"data row0 col0\" >0.203 (0.016)</td>\n",
       "      <td id=\"T_e2b73_row0_col1\" class=\"data row0 col1\" >0.491 (0.052)</td>\n",
       "      <td id=\"T_e2b73_row0_col2\" class=\"data row0 col2\" >0.703 (0.057)</td>\n",
       "      <td id=\"T_e2b73_row0_col3\" class=\"data row0 col3\" >0.716 (0.047)</td>\n",
       "      <td id=\"T_e2b73_row0_col4\" class=\"data row0 col4\" >0.747 (0.035)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b8568dc280>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For XGB\n",
    "models = [\"Baseline\"]+[i for i in results_subsets[0].keys() if (\"tuned\" in i and \"XGB\" in i)]\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "use_df = pd.DataFrame([pd.DataFrame(results_subsets[fold_num]).loc[metric,models] for fold_num in results_subsets.keys()],index=results_subsets.keys())\n",
    "\n",
    "df_mean = pd.DataFrame((use_df).mean(axis=0).round(3).astype(str) + \" (\" + use_df.std(axis=0).round(3).astype(str) + \")\").transpose()\n",
    "model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "\n",
    "best_model = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "t_test_res = np.array([stats.ttest_rel(use_df[best_model].values, use_df[model].values)[1] for model in models]).round(3)\n",
    "t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    \n",
    "res_df_xgb = pd.DataFrame([model_dict])\n",
    "\n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_res[i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "res_df_xgb.style.apply(negative_bold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccd45db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline</th>\n",
       "      <th>demo_only</th>\n",
       "      <th>activity_only</th>\n",
       "      <th>activity_and_demo</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.203 (0.016)</td>\n",
       "      <td>0.508 (0.041)</td>\n",
       "      <td>0.726 (0.04)</td>\n",
       "      <td>0.721 (0.022)</td>\n",
       "      <td>0.746 (0.045)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>0.203 (0.016)</td>\n",
       "      <td>0.491 (0.052)</td>\n",
       "      <td>0.703 (0.057)</td>\n",
       "      <td>0.716 (0.047)</td>\n",
       "      <td>0.747 (0.035)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Baseline      demo_only  activity_only activity_and_demo  \\\n",
       "LR   0.203 (0.016)  0.508 (0.041)   0.726 (0.04)     0.721 (0.022)   \n",
       "XGB  0.203 (0.016)  0.491 (0.052)  0.703 (0.057)     0.716 (0.047)   \n",
       "\n",
       "               all  \n",
       "LR   0.746 (0.045)  \n",
       "XGB  0.747 (0.035)  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df_lr.columns = [i[3:-6] if i != \"Baseline\" else \"Baseline\" for i in res_df_lr.columns]    \n",
    "res_df_xgb.columns = [i[4:-6] if i != \"Baseline\" else \"Baseline\" for i in res_df_xgb.columns]    \n",
    "\n",
    "latex_df_subsets = pd.concat([res_df_lr,res_df_xgb],axis=0)\n",
    "latex_df_subsets.index = [\"LR\", \"XGB\"]\n",
    "latex_df_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35ad05f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lll}\n",
      "\\toprule\n",
      "{} &             LR &            XGB \\\\\n",
      "\\midrule\n",
      "Baseline          &  0.203 (0.016) &  0.203 (0.016) \\\\\n",
      "demo\\_only         &  0.508 (0.041) &  0.491 (0.052) \\\\\n",
      "activity\\_only     &   0.726 (0.04) &  0.703 (0.057) \\\\\n",
      "activity\\_and\\_demo &  0.721 (0.022) &  0.716 (0.047) \\\\\n",
      "all               &  0.746 (0.045) &  0.747 (0.035) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_df_subsets.round(2).transpose().to_latex())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7144e34",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21dedee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_10_importances = {}\n",
    "\n",
    "# for model in list(results_subsets_feature_importances[fold].keys()):\n",
    "#     imp_df = pd.concat([results_subsets_feature_importances[fold][model] for fold in range(folds)],axis=1)\n",
    "\n",
    "#     if \"LR\" in model:\n",
    "#         direction = imp_df.apply(lambda x: np.sign(x))\n",
    "#         imp_df = imp_df.abs()\n",
    "\n",
    "#     imp_df = imp_df/imp_df.sum(axis=0)\n",
    "\n",
    "#     mean_imp_df = imp_df.mean(axis=1)\n",
    "#     std_imp_df = imp_df.std(axis=1)\n",
    "\n",
    "#     mean_imp_df = mean_imp_df.sort_values(ascending=False)\n",
    "#     std_imp_df = std_imp_df.loc[mean_imp_df.index]\n",
    "#     final_imps = mean_imp_df[:10]\n",
    "#     final_imps[\"Rest\"] = sum(mean_imp_df[10:])\n",
    "#     top_5_importances[model] = np.array([final_imps.index.values, final_imps.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4282b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_importances = {}\n",
    "demo_importances_stds = {}\n",
    "\n",
    "for model in list(results_subsets_feature_importances[fold].keys()):\n",
    "    if \"demo\" in model or \"all\" in model:\n",
    "        imp_df_all = pd.concat([results_subsets_feature_importances[fold][model] for fold in range(folds)],axis=1)\n",
    "        \n",
    "        if \"LR\" in model:\n",
    "            direction = imp_df_all.apply(lambda x: np.sign(x))\n",
    "            imp_df_all = imp_df_all.abs()\n",
    "        if imp_df_all.sum().sum()!=0:\n",
    "            imp_df = imp_df_all/imp_df_all.sum(axis=0)\n",
    "        imp_df = imp_df.fillna(1/imp_df.shape[0])\n",
    "#         imp_df = imp_df.loc[demographic_cols]\n",
    "\n",
    "#         mean_imp_df = imp_df.mean(axis=1)\n",
    "#         std_imp_df = imp_df.std(axis=1)\n",
    "\n",
    "#         mean_imp_df = mean_imp_df.sort_values(ascending=False)\n",
    "#         std_imp_df = std_imp_df.loc[mean_imp_df.index]\n",
    "#         final_imps = mean_imp_df#[:10]\n",
    "#         final_imps[\"Rest\"] = sum(mean_imp_df[10:])\n",
    "#         final_imps[\"Total\"] = sum(mean_imp_df)\n",
    "        demo_importances[model] = np.round(np.mean(imp_df.loc[[i for i in imp_df.index if any([j in i for j in demographic_cols])]].sum(axis=0)),2)#final_imps.values\n",
    "        demo_importances_stds[model] = np.round(np.std(imp_df.loc[[i for i in imp_df.index if any([j in i for j in demographic_cols])]].sum(axis=0)),2)#final_imps.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e8c23830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demo_only</th>\n",
       "      <th>activity_and_demo</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>1.0 (0.0)</td>\n",
       "      <td>0.47 (0.2)</td>\n",
       "      <td>0.29 (0.1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>1.0 (0.0)</td>\n",
       "      <td>0.2 (0.06)</td>\n",
       "      <td>0.18 (0.05)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     demo_only activity_and_demo          all\n",
       "LR   1.0 (0.0)        0.47 (0.2)   0.29 (0.1)\n",
       "XGB  1.0 (0.0)        0.2 (0.06)  0.18 (0.05)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_demo_imp = pd.Series({i: demo_importances[i] for i in demo_importances if \"LR\" in i and \"tuned\" in i})\n",
    "xgb_demo_imp = pd.Series({i: demo_importances[i] for i in demo_importances if \"XGB\" in i and \"tuned\" in i})\n",
    "lr_demo_imp.index = [i[3:-6] for i in lr_demo_imp.index]    \n",
    "xgb_demo_imp.index = [i[4:-6] for i in xgb_demo_imp.index]    \n",
    "\n",
    "lr_demo_imp_stds = pd.Series({i: demo_importances_stds[i] for i in demo_importances_stds if \"LR\" in i and \"tuned\" in i})\n",
    "xgb_demo_imp_stds = pd.Series({i: demo_importances_stds[i] for i in demo_importances_stds if \"XGB\" in i and \"tuned\" in i})\n",
    "lr_demo_imp_stds.index = [i[3:-6] for i in lr_demo_imp_stds.index]    \n",
    "xgb_demo_imp_stds.index = [i[4:-6] for i in xgb_demo_imp_stds.index]    \n",
    "\n",
    "\n",
    "latex_df_imp = pd.DataFrame([lr_demo_imp.astype(str) + \" (\" + lr_demo_imp_stds.astype(str) + \")\",\n",
    "                             xgb_demo_imp.astype(str) + \" (\" + xgb_demo_imp_stds.astype(str) + \")\"])\n",
    "latex_df_imp.index = [\"LR\", \"XGB\"]\n",
    "latex_df_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbeb673d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "{} &       Baseline &      demo\\_only &  activity\\_only & activity\\_and\\_demo &            all \\\\\n",
      "\\midrule\n",
      "LR  &  0.203 (0.016) &  0.508 (0.041) &   0.726 (0.04) &     0.721 (0.022) &  0.746 (0.045) \\\\\n",
      "XGB &  0.203 (0.016) &  0.491 (0.052) &  0.703 (0.057) &     0.716 (0.047) &  0.747 (0.035) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_df_subsets.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09568d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced0355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc07112e7ae1e8e28a0232207620ff002934c05692de8df42430404c766a0a8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
