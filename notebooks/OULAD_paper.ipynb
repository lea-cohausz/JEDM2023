{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955f54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils.evaluation import *\n",
    "from utils.utils import *\n",
    "\n",
    "from data import dataset_preprocessing\n",
    "\n",
    "from utils.evaluation import get_metrics\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6dd2461",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"OULAD\"\n",
    "mode=\"cv\"\n",
    "RS=42\n",
    "hct=10\n",
    "test_ratio=0.2\n",
    "val_ratio=0.1\n",
    "folds=5\n",
    "target = \"binary\"\n",
    "experiment_name = \"5CV_paper_final\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae51df",
   "metadata": {},
   "source": [
    "### Describe raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "620449bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assessments_df = pd.read_csv(f'../data/raw/{dataset_name}/assessments.csv')\n",
    "courses_df = pd.read_csv(f'../data/raw/{dataset_name}/courses.csv')\n",
    "studentAssessment_df = pd.read_csv(f'../data/raw/{dataset_name}/studentAssessment.csv')\n",
    "studentInfo_df = pd.read_csv(f'../data/raw/{dataset_name}/studentInfo.csv')\n",
    "studentRegistration_df = pd.read_csv(f'../data/raw/{dataset_name}/studentRegistration.csv')\n",
    "studentVle_df = pd.read_csv(f'../data/raw/{dataset_name}/studentVle.csv')\n",
    "vle_df = pd.read_csv(f'../data/raw/{dataset_name}/vle.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "982e518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########  Preprocessing #################################\n",
    "if not os.path.exists(f\"../data/prepared/{dataset_name}/df_prepared.pickle\"):\n",
    "\n",
    "    assessments_df = pd.read_csv(f'../data/raw/{dataset_name}/assessments.csv')\n",
    "    courses_df = pd.read_csv(f'../data/raw/{dataset_name}/courses.csv')\n",
    "    studentAssessment_df = pd.read_csv(f'../data/raw/{dataset_name}/studentAssessment.csv')\n",
    "    studentInfo_df = pd.read_csv(f'../data/raw/{dataset_name}/studentInfo.csv')\n",
    "    studentRegistration_df = pd.read_csv(f'../data/raw/{dataset_name}/studentRegistration.csv')\n",
    "    studentVle_df = pd.read_csv(f'../data/raw/{dataset_name}/studentVle.csv')\n",
    "    vle_df = pd.read_csv(f'../data/raw/{dataset_name}/vle.csv')\n",
    "\n",
    "\n",
    "    # Remove all withdrawn\n",
    "    studentInfo_df = studentInfo_df.loc[studentInfo_df.final_result!=\"Withdrawn\"]\n",
    "    studentInfo_df.shape\n",
    "\n",
    "    # Assessment performance features\n",
    "    merged_assessments_df = pd.merge(studentAssessment_df,assessments_df,on=\"id_assessment\")\n",
    "\n",
    "    avg_tma = [merged_assessments_df.loc[np.logical_and(merged_assessments_df.id_student==i,merged_assessments_df.assessment_type==\"TMA\"),\"score\"].mean() for i in studentInfo_df.id_student.values]\n",
    "    avg_cma = [merged_assessments_df.loc[np.logical_and(merged_assessments_df.id_student==i,merged_assessments_df.assessment_type==\"CMA\"),\"score\"].mean() for i in studentInfo_df.id_student.values]\n",
    "    avg_exam = [merged_assessments_df.loc[np.logical_and(merged_assessments_df.id_student==i,merged_assessments_df.assessment_type==\"Exam\"),\"score\"].mean() for i in studentInfo_df.id_student.values]\n",
    "\n",
    "    studentInfo_df[\"avg_tma\"] = avg_tma\n",
    "    studentInfo_df[\"avg_cma\"] = avg_cma\n",
    "    studentInfo_df[\"avg_exam\"] = avg_exam\n",
    "\n",
    "    # Get VLE features\n",
    "    vle_merged = pd.merge(studentVle_df,vle_df,on=[\"code_module\", \"code_presentation\", \"id_site\"])\n",
    "    for activity_type in vle_df.activity_type.unique():\n",
    "        agg = vle_merged.loc[vle_merged.activity_type==activity_type].groupby(\"id_student\")\n",
    "        count_click_dict = dict(agg.count()[\"sum_click\"])\n",
    "        sum_click_dict = dict(agg.sum()[\"sum_click\"])\n",
    "        studentInfo_df[f\"Count_Visits_{activity_type}\"] = studentInfo_df[\"id_student\"].apply(lambda x: sum_click_dict[x] if x in count_click_dict.keys() else 0)\n",
    "        studentInfo_df[f\"Sum_Clicks_{activity_type}\"] = studentInfo_df[\"id_student\"].apply(lambda x: sum_click_dict[x] if x in sum_click_dict.keys() else 0)\n",
    "\n",
    "    with open(f\"../data/prepared/{dataset_name}/df_prepared.pickle\", 'wb') as handle:\n",
    "        pickle.dump(studentInfo_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    df = studentInfo_df\n",
    "\n",
    "else:    \n",
    "    with open(f\"../data/prepared/{dataset_name}/df_prepared.pickle\", 'rb') as handle:\n",
    "        df = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2680e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove all withdrawn\n",
    "# studentInfo_df = studentInfo_df.loc[studentInfo_df.final_result!=\"Withdrawn\"]\n",
    "# studentInfo_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2c79166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get final score\n",
    "# merged_assessments_df = pd.merge(studentAssessment_df,assessments_df,on=\"id_assessment\")\n",
    "# final_results = [merged_assessments_df.loc[merged_assessments_df.id_student==i,\"score\"].dot(merged_assessments_df.loc[merged_assessments_df.id_student==i,\"weight\"]/100) for i in studentInfo_df.id_student.values]\n",
    "# studentInfo_df[\"final_score\"] = final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "302d13ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude those with 0 final score who did not fail (three from differen scores as error, and code module GGG as 0 is normal for it)\n",
    "# studentInfo_df = studentInfo_df.loc[~np.logical_and(studentInfo_df[\"final_score\"]<1, studentInfo_df[\"final_result\"]!=\"Fail\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "888956c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Avg_TMA_Score, Avg_CMA_Score, Exam_Score\n",
    "# avg_tma = [merged_assessments_df.loc[np.logical_and(merged_assessments_df.id_student==i,merged_assessments_df.assessment_type==\"TMA\"),\"score\"].dot(merged_assessments_df.loc[np.logical_and(merged_assessments_df.id_student==i,merged_assessments_df.assessment_type==\"TMA\"),\"weight\"]/100) for i in studentInfo_df.id_student.values[:100]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c3b5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assessment performance features\n",
    "# merged_assessments_df = pd.merge(studentAssessment_df,assessments_df,on=\"id_assessment\")\n",
    "\n",
    "# avg_tma = [merged_assessments_df.loc[np.logical_and(merged_assessments_df.id_student==i,merged_assessments_df.assessment_type==\"TMA\"),\"score\"].mean() for i in studentInfo_df.id_student.values]\n",
    "# avg_cma = [merged_assessments_df.loc[np.logical_and(merged_assessments_df.id_student==i,merged_assessments_df.assessment_type==\"CMA\"),\"score\"].mean() for i in studentInfo_df.id_student.values]\n",
    "# avg_exam = [merged_assessments_df.loc[np.logical_and(merged_assessments_df.id_student==i,merged_assessments_df.assessment_type==\"Exam\"),\"score\"].mean() for i in studentInfo_df.id_student.values]\n",
    "\n",
    "# studentInfo_df[\"avg_tma\"] = avg_tma\n",
    "# studentInfo_df[\"avg_cma\"] = avg_cma\n",
    "# studentInfo_df[\"avg_exam\"] = avg_exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04c30922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get VLE features\n",
    "# vle_merged = pd.merge(studentVle_df,vle_df,on=[\"code_module\", \"code_presentation\", \"id_site\"])\n",
    "# for activity_type in vle_df.activity_type.unique():\n",
    "#     agg = vle_merged.loc[vle_merged.activity_type==activity_type].groupby(\"id_student\")\n",
    "#     count_click_dict = dict(agg.count()[\"sum_click\"])\n",
    "#     sum_click_dict = dict(agg.sum()[\"sum_click\"])\n",
    "#     studentInfo_df[f\"Count_Visits_{activity_type}\"] = studentInfo_df[\"id_student\"].apply(lambda x: sum_click_dict[x] if x in count_click_dict.keys() else 0)\n",
    "#     studentInfo_df[f\"Sum_Clicks_{activity_type}\"] = studentInfo_df[\"id_student\"].apply(lambda x: sum_click_dict[x] if x in sum_click_dict.keys() else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "004f8295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = studentInfo_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fdc599c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop([\"id_student\", \"code_module\", \"code_presentation\", \"avg_exam\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1ff71e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"final_result\"] = df[\"final_result\"].apply(lambda x: 0 if x==\"Fail\" else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea7782da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_col = \"final_result\"\n",
    "demographic_cols = ['gender', 'region', 'imd_band', 'age_band', 'disability', 'highest_education']\n",
    "perf_cols = [\"avg_cma\", \"avg_tma\", 'num_of_prev_attempts', 'studied_credits']\n",
    "activity_cols = [i for i in df.columns if \"Sum_Clicks\" in i] + [i for i in df.columns if \"Count_Visits\" in i]\n",
    "other_cols = []\n",
    "\n",
    "set(df.columns)-set([y_col]+demographic_cols+perf_cols+activity_cols+other_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "529261c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No. of samples</th>\n",
       "      <th>No. of features</th>\n",
       "      <th>Performance features</th>\n",
       "      <th>Demographic features</th>\n",
       "      <th>Activity features</th>\n",
       "      <th>Other features</th>\n",
       "      <th>Categorical features</th>\n",
       "      <th>Total cardinality</th>\n",
       "      <th>% NA</th>\n",
       "      <th>Target $\\textbf{y} \\in$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cortez</th>\n",
       "      <td>22437</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>0.480034</td>\n",
       "      <td>[1..2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        No. of samples  No. of features  Performance features  \\\n",
       "cortez           22437               51                     4   \n",
       "\n",
       "        Demographic features  Activity features  Other features  \\\n",
       "cortez                     6                 40               0   \n",
       "\n",
       "        Categorical features  Total cardinality      % NA  \\\n",
       "cortez                     4                 31  0.480034   \n",
       "\n",
       "       Target $\\textbf{y} \\in$  \n",
       "cortez                  [1..2]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_df_dict = {\"No. of samples\": df.shape[0],\n",
    "           \"No. of features\": df.shape[1],\n",
    "           \"Performance features\": len(perf_cols),\n",
    "           \"Demographic features\": len(demographic_cols),\n",
    "           \"Activity features\": len(activity_cols),\n",
    "           \"Other features\": len(other_cols),\n",
    "           \"Categorical features\": len(df.columns[list(np.logical_and(df.nunique() > 2, df.dtypes == \"object\"))]),     \n",
    "           \"Total cardinality\": df[df.columns[list(np.logical_and(df.nunique() > 2, df.dtypes == \"object\"))]].nunique().sum(),     \n",
    "           \"% NA\": df.isna().sum().sum()/sum(df.shape),\n",
    "           \"Target $\\textbf{y} \\in$\": f\"[1..{df[y_col].nunique()}]\",\n",
    "#            \"High cardinality levels\":  list(df.loc[:,list(df.columns[list(np.logical_and(df.nunique() >= 10, df.dtypes == \"object\"))])].nunique().sort_values().values),\n",
    "          \n",
    "}\n",
    "desc_df = pd.DataFrame([desc_df_dict],index=[\"cortez\"])\n",
    "desc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0e1463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{ll}\n",
      "\\toprule\n",
      "{} &    cortez \\\\\n",
      "\\midrule\n",
      "No. of samples          &     22437 \\\\\n",
      "No. of features         &        51 \\\\\n",
      "Performance features    &         4 \\\\\n",
      "Demographic features    &         6 \\\\\n",
      "Activity features       &        40 \\\\\n",
      "Other features          &         0 \\\\\n",
      "Categorical features    &         4 \\\\\n",
      "Total cardinality       &        31 \\\\\n",
      "\\% NA                    &  0.480034 \\\\\n",
      "Target \\$\\textbackslash textbf\\{y\\} \\textbackslash in\\$ &    [1..2] \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(desc_df.transpose().to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b7644d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6390a689",
   "metadata": {},
   "source": [
    "### Preprocessing and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5cc6a91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = f\"{mode}_RS{RS}_hct{hct}\"\n",
    "if mode == \"cv\":\n",
    "    data_path += f\"_{folds}folds\"\n",
    "elif mode == \"train_test\":\n",
    "    data_path += f\"_split{1-test_ratio*100}-{test_ratio*100}\"\n",
    "elif mode == \"train_val_test\":\n",
    "    data_path += f\"_split{round(100-(test_ratio+val_ratio)*100)}-{round(test_ratio*100)}-{round(val_ratio*100)}\"\n",
    "\n",
    "\n",
    "# If no data_dict for the configuration exists, run preprocessing, else load data_dict\n",
    "if not os.path.exists(f\"../data/prepared/{dataset_name}/\"+data_path+\"/data_dict.pickle\"):\n",
    "    dataset_preprocessing.process_dataset(dataset_name, target, mode, RS, hct, test_ratio, val_ratio, folds)\n",
    "with open(f\"../data/prepared/{dataset_name}/{data_path}/data_dict.pickle\", 'rb') as handle:\n",
    "        data_dict = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b9f710",
   "metadata": {},
   "source": [
    "## Evaluation of categorical data treatment methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "348226c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\"ignore\", \"ohe\", \"target\", \"ordinal\", \"catboost\", \"glmm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2853513",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_rounds = 10\n",
    "max_evals = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7625f75f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing results for fold 0, condition=ignore\n",
      "SCORE: 0.6236650205401243                                                                                              \n",
      "SCORE: 0.6236651971377892                                                                                              \n",
      "SCORE: 0.6236653496493902                                                                                              \n",
      "SCORE: 0.6236628137576451                                                                                              \n",
      "SCORE: 0.6236653495420327                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 16.13trial/s, best loss: 0.6236628137576451]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.07433411394132879}\n",
      "Default performance on Test: 0.6141019933315486\n",
      "SCORE: 0.6476721883883189                                                                                              \n",
      "SCORE: 0.6599544603399445                                                                                              \n",
      "SCORE: 0.6597374172408201                                                                                              \n",
      "SCORE: 0.6738994655096485                                                                                              \n",
      "SCORE: 0.6688825551828039                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.38trial/s, best loss: 0.6476721883883189]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4241158840493566, 'n_estimators': 403.0}\n",
      "Test Performance after first tuning round: 0.6141020045859099\n",
      "SCORE: 0.6476721883883189                                                                                              \n",
      "SCORE: 0.6476721883883189                                                                                              \n",
      "SCORE: 0.6476721883883189                                                                                              \n",
      "SCORE: 0.6476721883883189                                                                                              \n",
      "SCORE: 0.6476721883883189                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.30trial/s, best loss: 0.6476721883883189]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4241158840493566, 'n_estimators': 403.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 0.6141020045859099\n",
      "SCORE: 0.6311199054786564                                                                                              \n",
      "SCORE: 0.6311364121586337                                                                                              \n",
      "SCORE: 0.6310646588919093                                                                                              \n",
      "SCORE: 0.6311225086164539                                                                                              \n",
      "SCORE: 0.6311371973906518                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.45trial/s, best loss: 0.6310646588919093]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4241158840493566, 'n_estimators': 403.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.7303131539893615, 'subsample': 0.9593023699134816}\n",
      "Test Performance after third tuning round: 0.6140413410401788\n",
      "SCORE: 0.6242945140577968                                                                                              \n",
      "SCORE: 0.6243640673073325                                                                                              \n",
      "SCORE: 0.624293042381034                                                                                               \n",
      "SCORE: 0.6261793630782373                                                                                              \n",
      "SCORE: 0.6242919034842245                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.89trial/s, best loss: 0.6242919034842245]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4241158840493566, 'n_estimators': 403.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.7303131539893615, 'subsample': 0.9593023699134816, 'gamma': 5.1697660499921065, 'reg_alpha': 5.0, 'reg_lambda': 1.8451897412848688}\n",
      "Test Performance after last tuning round: 0.6140008276910901\n",
      "Preparing results for fold 0, condition=ohe\n",
      "SCORE: 0.599080251463237                                                                                               \n",
      "SCORE: 0.5990816494112207                                                                                              \n",
      "SCORE: 0.5990417747050778                                                                                              \n",
      "SCORE: 0.5990582616622225                                                                                              \n",
      "SCORE: 0.5990845280410556                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.57trial/s, best loss: 0.5990417747050778]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.16689949107602478}\n",
      "Default performance on Test: 0.6105402429426053\n",
      "SCORE: 0.6031465179386332                                                                                              \n",
      "SCORE: 0.6384158420636533                                                                                              \n",
      "SCORE: 0.6027897076324067                                                                                              \n",
      "SCORE: 0.6028380130716144                                                                                              \n",
      "SCORE: 0.6046699086081997                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:06<00:00,  1.23s/trial, best loss: 0.6027897076324067]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.23270078553187346, 'n_estimators': 459.0}\n",
      "Test Performance after first tuning round: 0.6316663162540688\n",
      "SCORE: 0.6059049831777852                                                                                              \n",
      "SCORE: 0.6083126213295168                                                                                              \n",
      "SCORE: 0.6074563995449596                                                                                              \n",
      "SCORE: 0.5998916207776601                                                                                              \n",
      "SCORE: 0.599651923983138                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:06<00:00,  1.26s/trial, best loss: 0.599651923983138]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.23270078553187346, 'n_estimators': 459.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.6015349917874516\n",
      "SCORE: 0.6006705348382628                                                                                              \n",
      "SCORE: 0.5998611329377004                                                                                              \n",
      "SCORE: 0.600890489587828                                                                                               \n",
      "SCORE: 0.5997258979213226                                                                                              \n",
      "SCORE: 0.599658001737366                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.18s/trial, best loss: 0.599658001737366]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.23270078553187346, 'n_estimators': 459.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.7352852497514042, 'subsample': 0.6386337481887971}\n",
      "Test Performance after third tuning round: 0.6063132838297732\n",
      "SCORE: 0.6024875039292996                                                                                              \n",
      "SCORE: 0.600648704907919                                                                                               \n",
      "SCORE: 0.6014805410581645                                                                                              \n",
      "SCORE: 0.6015721827374974                                                                                              \n",
      "SCORE: 0.6021853481218435                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:06<00:00,  1.22s/trial, best loss: 0.600648704907919]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.23270078553187346, 'n_estimators': 459.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.7352852497514042, 'subsample': 0.6386337481887971, 'gamma': 4.0695741315032805, 'reg_alpha': 8.0, 'reg_lambda': 1.5946997203337854}\n",
      "Test Performance after last tuning round: 0.5902412198466493\n",
      "Preparing results for fold 0, condition=target\n",
      "SCORE: 0.5997672112590127                                                                                              \n",
      "SCORE: 0.5987434194599418                                                                                              \n",
      "SCORE: 0.5998381579487191                                                                                              \n",
      "SCORE: 0.5988839878614798                                                                                              \n",
      "SCORE: 0.6074332018611301                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.91trial/s, best loss: 0.5987434194599418]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.869207637039547}\n",
      "Default performance on Test: 0.6114684792007351\n",
      "SCORE: 0.601623964460552                                                                                               \n",
      "SCORE: 0.6035902988527262                                                                                              \n",
      "SCORE: 0.6197604585915958                                                                                              \n",
      "SCORE: 0.6145914776736775                                                                                              \n",
      "SCORE: 0.6042381954358359                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.70trial/s, best loss: 0.601623964460552]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.48635529194972754, 'n_estimators': 284.0}\n",
      "Test Performance after first tuning round: 0.6406105590150662\n",
      "SCORE: 0.6130764922794403                                                                                              \n",
      "SCORE: 0.59908077691807                                                                                                \n",
      "SCORE: 0.5999125342544076                                                                                              \n",
      "SCORE: 0.6191567820651913                                                                                              \n",
      "SCORE: 0.59908077691807                                                                                                \n",
      "100%|████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.64trial/s, best loss: 0.59908077691807]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.48635529194972754, 'n_estimators': 284.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 0.590754334922382\n",
      "SCORE: 0.5991222080866782                                                                                              \n",
      "SCORE: 0.5994915023370317                                                                                              \n",
      "SCORE: 0.5993135583258898                                                                                              \n",
      "SCORE: 0.5991356961831334                                                                                              \n",
      "SCORE: 0.5990872141323101                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.21trial/s, best loss: 0.5990872141323101]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.48635529194972754, 'n_estimators': 284.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.8974461539347816, 'subsample': 0.5338515153810182}\n",
      "Test Performance after third tuning round: 0.5906336623116527\n",
      "SCORE: 0.6001348984801457                                                                                              \n",
      "SCORE: 0.598963578374928                                                                                               \n",
      "SCORE: 0.6006880472914168                                                                                              \n",
      "SCORE: 0.5997436883422597                                                                                              \n",
      "SCORE: 0.5996822195546276                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.43trial/s, best loss: 0.598963578374928]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.48635529194972754, 'n_estimators': 284.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.8974461539347816, 'subsample': 0.5338515153810182, 'gamma': 0.7817354945381682, 'reg_alpha': 5.0, 'reg_lambda': 3.566020514804488}\n",
      "Test Performance after last tuning round: 0.5908812105460345\n",
      "Preparing results for fold 0, condition=ordinal\n",
      "SCORE: 0.6203354866835484                                                                                              \n",
      "SCORE: 0.6203352913474205                                                                                              \n",
      "SCORE: 0.6203353588388124                                                                                              \n",
      "SCORE: 0.620334799431892                                                                                               \n",
      "SCORE: 0.6203343091167557                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.21trial/s, best loss: 0.6203343091167557]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.3088008725461635}\n",
      "Default performance on Test: 0.6131218793938061\n",
      "SCORE: 0.603784341864368                                                                                               \n",
      "SCORE: 0.6039376178490408                                                                                              \n",
      "SCORE: 0.604223525702716                                                                                               \n",
      "SCORE: 0.6038458620818022                                                                                              \n",
      "SCORE: 0.6043716573432352                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.33trial/s, best loss: 0.603784341864368]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.36680921045468584, 'n_estimators': 268.0}\n",
      "Test Performance after first tuning round: 0.6350014528703835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.6257396740400203                                                                                              \n",
      "SCORE: 0.604482932656958                                                                                               \n",
      "SCORE: 0.6034237539794147                                                                                              \n",
      "SCORE: 0.6372387800116414                                                                                              \n",
      "SCORE: 0.6006459402163772                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.52trial/s, best loss: 0.6006459402163772]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.36680921045468584, 'n_estimators': 268.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 0.6049853436292145\n",
      "SCORE: 0.5999695149287458                                                                                              \n",
      "SCORE: 0.600562044171631                                                                                               \n",
      "SCORE: 0.6007150499443299                                                                                              \n",
      "SCORE: 0.5996074840291987                                                                                              \n",
      "SCORE: 0.6000008324259408                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.16trial/s, best loss: 0.5996074840291987]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.36680921045468584, 'n_estimators': 268.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.608725921719949, 'subsample': 0.8060509377505832}\n",
      "Test Performance after third tuning round: 0.5998157157010414\n",
      "SCORE: 0.6000217213801702                                                                                              \n",
      "SCORE: 0.6016619475247535                                                                                              \n",
      "SCORE: 0.6021443287254324                                                                                              \n",
      "SCORE: 0.6009694094171841                                                                                              \n",
      "SCORE: 0.5998187754489651                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.12trial/s, best loss: 0.5998187754489651]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.36680921045468584, 'n_estimators': 268.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.608725921719949, 'subsample': 0.8060509377505832, 'gamma': 1.1987024761329987, 'reg_alpha': 2.0, 'reg_lambda': 1.6946408645755784}\n",
      "Test Performance after last tuning round: 0.5934142097644328\n",
      "Preparing results for fold 0, condition=catboost\n",
      "SCORE: 0.6044039617572879                                                                                              \n",
      "SCORE: 0.6044112575866571                                                                                              \n",
      "SCORE: 0.6067369156070994                                                                                              \n",
      "SCORE: 0.604468261225703                                                                                               \n",
      "SCORE: 0.6055692902538692                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.79trial/s, best loss: 0.6044039617572879]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.980014714669304}\n",
      "Default performance on Test: 0.6473131725852648\n",
      "SCORE: 0.6101295007330434                                                                                              \n",
      "SCORE: 0.609429190606353                                                                                               \n",
      "SCORE: 0.6143918702698321                                                                                              \n",
      "SCORE: 0.610412093296971                                                                                               \n",
      "SCORE: 0.6111118815287385                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.01s/trial, best loss: 0.609429190606353]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.45923954152441404, 'n_estimators': 325.0}\n",
      "Test Performance after first tuning round: 0.7464055706733771\n",
      "SCORE: 0.677735555886662                                                                                               \n",
      "SCORE: 0.6298057710707285                                                                                              \n",
      "SCORE: 0.6086987940482874                                                                                              \n",
      "SCORE: 0.6152961879440108                                                                                              \n",
      "SCORE: 0.6608434863537855                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:08<00:00,  1.71s/trial, best loss: 0.6086987940482874]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.45923954152441404, 'n_estimators': 325.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 0.746740495394641\n",
      "SCORE: 0.6158885295044395                                                                                              \n",
      "SCORE: 0.6207203523475883                                                                                              \n",
      "SCORE: 0.6130564679430628                                                                                              \n",
      "SCORE: 0.6097367569508733                                                                                              \n",
      "SCORE: 0.610478000171178                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.26trial/s, best loss: 0.6097367569508733]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.45923954152441404, 'n_estimators': 325.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.7978748388254575, 'subsample': 0.6647219711995768}\n",
      "Test Performance after third tuning round: 0.81126890168273\n",
      "SCORE: 0.6040299178224002                                                                                              \n",
      "SCORE: 0.612870344391642                                                                                               \n",
      "SCORE: 0.6065409526298431                                                                                              \n",
      "SCORE: 0.6059715408909604                                                                                              \n",
      "SCORE: 0.6037189551290499                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.12trial/s, best loss: 0.6037189551290499]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.45923954152441404, 'n_estimators': 325.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.7978748388254575, 'subsample': 0.6647219711995768, 'gamma': 5.02130421643029, 'reg_alpha': 8.0, 'reg_lambda': 1.4309744304778533}\n",
      "Test Performance after last tuning round: 0.6070921563714157\n",
      "Preparing results for fold 0, condition=glmm\n",
      "SCORE: 0.5997691702597747                                                                                              \n",
      "SCORE: 0.5997529508469788                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.5997530759292096                                                                                              \n",
      "SCORE: 0.5997536503107777                                                                                              \n",
      "SCORE: 0.6013928622595787                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.78trial/s, best loss: 0.5997529508469788]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.9873092146842414}\n",
      "Default performance on Test: 0.63158157263424\n",
      "SCORE: 0.6204294106552372                                                                                              \n",
      "SCORE: 0.6315750337382251                                                                                              \n",
      "SCORE: 0.6115764431004491                                                                                              \n",
      "SCORE: 0.6323261827220732                                                                                              \n",
      "SCORE: 0.6139574352431663                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.67trial/s, best loss: 0.6115764431004491]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.2817999344678683, 'n_estimators': 437.0}\n",
      "Test Performance after first tuning round: 0.7173609123703626\n",
      "SCORE: 0.6173264960782988                                                                                              \n",
      "SCORE: 0.6136285307099232                                                                                              \n",
      "SCORE: 0.6461529402383288                                                                                              \n",
      "SCORE: 0.6025998603391229                                                                                              \n",
      "SCORE: 0.6531829617431735                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.18trial/s, best loss: 0.6025998603391229]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.2817999344678683, 'n_estimators': 437.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 9.0}\n",
      "Test Performance after second tuning round: 0.6518336312339507\n",
      "SCORE: 0.6154135321934238                                                                                              \n",
      "SCORE: 0.6087371216184657                                                                                              \n",
      "SCORE: 0.6088188531561385                                                                                              \n",
      "SCORE: 0.6115431144648679                                                                                              \n",
      "SCORE: 0.605043662569664                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.41trial/s, best loss: 0.605043662569664]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.2817999344678683, 'n_estimators': 437.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.8539512130548601, 'subsample': 0.6299658882333483}\n",
      "Test Performance after third tuning round: 0.6679643415099624\n",
      "SCORE: 0.6012743249787091                                                                                              \n",
      "SCORE: 0.600959766289712                                                                                               \n",
      "SCORE: 0.6004759572542081                                                                                              \n",
      "SCORE: 0.6005159682999701                                                                                              \n",
      "SCORE: 0.6001858001436382                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.02s/trial, best loss: 0.6001858001436382]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.2817999344678683, 'n_estimators': 437.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.8539512130548601, 'subsample': 0.6299658882333483, 'gamma': 6.148513604575651, 'reg_alpha': 4.0, 'reg_lambda': 2.4779056900613776}\n",
      "Test Performance after last tuning round: 0.5917568598409287\n",
      "Preparing results for fold 1, condition=ignore\n",
      "SCORE: 0.6205738548624612                                                                                              \n",
      "SCORE: 0.6205945295898075                                                                                              \n",
      "SCORE: 0.6205564533806986                                                                                              \n",
      "SCORE: 0.6205556336706228                                                                                              \n",
      "SCORE: 0.6205560268950225                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 18.14trial/s, best loss: 0.6205556336706228]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.16013336523201235}\n",
      "Default performance on Test: 0.6258953611342296\n",
      "SCORE: 0.6376836794267162                                                                                              \n",
      "SCORE: 0.6657279464372445                                                                                              \n",
      "SCORE: 0.6671783789006264                                                                                              \n",
      "SCORE: 0.6395864251271106                                                                                              \n",
      "SCORE: 0.6860559243149082                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.44trial/s, best loss: 0.6376836794267162]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4797345657644125, 'n_estimators': 409.0}\n",
      "Test Performance after first tuning round: 0.6258952063194122\n",
      "SCORE: 0.6376836794267162                                                                                              \n",
      "SCORE: 0.6376836794267162                                                                                              \n",
      "SCORE: 0.6376836794267162                                                                                              \n",
      "SCORE: 0.6376836794267162                                                                                              \n",
      "SCORE: 0.6376836794267162                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.53trial/s, best loss: 0.6376836794267162]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4797345657644125, 'n_estimators': 409.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 0.6258952063194122\n",
      "SCORE: 0.6262310534505959                                                                                              \n",
      "SCORE: 0.6262232883209423                                                                                              \n",
      "SCORE: 0.6254410263737585                                                                                              \n",
      "SCORE: 0.6254432805594051                                                                                              \n",
      "SCORE: 0.626160583146434                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.64trial/s, best loss: 0.6254410263737585]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4797345657644125, 'n_estimators': 409.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.6394105907473081, 'subsample': 0.676882772727635}\n",
      "Test Performance after third tuning round: 0.6260881062351659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.6241185740687458                                                                                              \n",
      "SCORE: 0.6211326671126494                                                                                              \n",
      "SCORE: 0.6211154953834385                                                                                              \n",
      "SCORE: 0.6241003152697718                                                                                              \n",
      "SCORE: 0.6253903228585348                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.27trial/s, best loss: 0.6211154953834385]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4797345657644125, 'n_estimators': 409.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.6394105907473081, 'subsample': 0.676882772727635, 'gamma': 2.546426874549293, 'reg_alpha': 4.0, 'reg_lambda': 1.597617040045428}\n",
      "Test Performance after last tuning round: 0.6259986920074242\n",
      "Preparing results for fold 1, condition=ohe\n",
      "SCORE: 0.5972434799953845                                                                                              \n",
      "SCORE: 0.5972275528648489                                                                                              \n",
      "SCORE: 0.5972348582376784                                                                                              \n",
      "SCORE: 0.5972339056008199                                                                                              \n",
      "SCORE: 0.5972451037641482                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.74trial/s, best loss: 0.5972275528648489]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.32331483101289027}\n",
      "Default performance on Test: 0.6192105486377875\n",
      "SCORE: 0.6004302746606229                                                                                              \n",
      "SCORE: 0.6001574775224764                                                                                              \n",
      "SCORE: 0.6003468995269996                                                                                              \n",
      "SCORE: 0.6000995516855895                                                                                              \n",
      "SCORE: 0.5999676709353419                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:07<00:00,  1.55s/trial, best loss: 0.5999676709353419]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.11548679974684832, 'n_estimators': 202.0}\n",
      "Test Performance after first tuning round: 0.6127722111896148\n",
      "SCORE: 0.5972006145446194                                                                                              \n",
      "SCORE: 0.61194466014578                                                                                                \n",
      "SCORE: 0.5975577326030587                                                                                              \n",
      "SCORE: 0.5989579678707593                                                                                              \n",
      "SCORE: 0.601686935141801                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.97s/trial, best loss: 0.5972006145446194]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.11548679974684832, 'n_estimators': 202.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 0.5980577851737916\n",
      "SCORE: 0.5982759491850091                                                                                              \n",
      "SCORE: 0.5986147857226791                                                                                              \n",
      "SCORE: 0.5976623961857642                                                                                              \n",
      "SCORE: 0.5975824617996346                                                                                              \n",
      "SCORE: 0.5981953171355402                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.13s/trial, best loss: 0.5975824617996346]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.11548679974684832, 'n_estimators': 202.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.878987680955218, 'subsample': 0.6273569071077583}\n",
      "Test Performance after third tuning round: 0.597737258524862\n",
      "SCORE: 0.59795198413902                                                                                                \n",
      "SCORE: 0.5986563434475292                                                                                              \n",
      "SCORE: 0.5974729765922813                                                                                              \n",
      "SCORE: 0.5975322580259769                                                                                              \n",
      "SCORE: 0.5977759613380158                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:12<00:00,  2.48s/trial, best loss: 0.5974729765922813]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.11548679974684832, 'n_estimators': 202.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.878987680955218, 'subsample': 0.6273569071077583, 'gamma': 1.795307146476329, 'reg_alpha': 7.0, 'reg_lambda': 1.1696524151717476}\n",
      "Test Performance after last tuning round: 0.5975992666456938\n",
      "Preparing results for fold 1, condition=target\n",
      "SCORE: 0.6077362968114498                                                                                              \n",
      "SCORE: 0.5975357179338613                                                                                              \n",
      "SCORE: 0.5970803686989129                                                                                              \n",
      "SCORE: 0.5968555038648444                                                                                              \n",
      "SCORE: 0.5984483448400214                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.34trial/s, best loss: 0.5968555038648444]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.7468036741844497}\n",
      "Default performance on Test: 0.6171935540238213\n",
      "SCORE: 0.6032264827083561                                                                                              \n",
      "SCORE: 0.6049409238512029                                                                                              \n",
      "SCORE: 0.6062838324075162                                                                                              \n",
      "SCORE: 0.6031832422708547                                                                                              \n",
      "SCORE: 0.6036530821474348                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.01trial/s, best loss: 0.6031832422708547]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.41961302087937313, 'n_estimators': 258.0}\n",
      "Test Performance after first tuning round: 0.6375992425117243\n",
      "SCORE: 0.6230404921724043                                                                                              \n",
      "SCORE: 0.597576157837393                                                                                               \n",
      "SCORE: 0.6326309702879573                                                                                              \n",
      "SCORE: 0.5970871616138507                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.5971498947913453                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.72trial/s, best loss: 0.5970871616138507]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.41961302087937313, 'n_estimators': 258.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 0.5992537105793219\n",
      "SCORE: 0.5973804525821982                                                                                              \n",
      "SCORE: 0.5972153628323296                                                                                              \n",
      "SCORE: 0.5971736674597418                                                                                              \n",
      "SCORE: 0.5968314225764858                                                                                              \n",
      "SCORE: 0.597485346121298                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.20trial/s, best loss: 0.5968314225764858]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.41961302087937313, 'n_estimators': 258.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.9054173971032551, 'subsample': 0.5722834457558137}\n",
      "Test Performance after third tuning round: 0.5987951087141983\n",
      "SCORE: 0.5980757339934615                                                                                              \n",
      "SCORE: 0.5970524806456435                                                                                              \n",
      "SCORE: 0.5976983274400058                                                                                              \n",
      "SCORE: 0.5979811254428968                                                                                              \n",
      "SCORE: 0.5974634301123561                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.30trial/s, best loss: 0.5970524806456435]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.41961302087937313, 'n_estimators': 258.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.9054173971032551, 'subsample': 0.5722834457558137, 'gamma': 0.6395691767958028, 'reg_alpha': 1.0, 'reg_lambda': 1.2064739783410574}\n",
      "Test Performance after last tuning round: 0.5979891040572509\n",
      "Preparing results for fold 1, condition=ordinal\n",
      "SCORE: 0.6108498291926181                                                                                              \n",
      "SCORE: 0.6108492824820272                                                                                              \n",
      "SCORE: 0.6108493817443171                                                                                              \n",
      "SCORE: 0.6108499028405209                                                                                              \n",
      "SCORE: 0.6108498638217579                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.43trial/s, best loss: 0.6108492824820272]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.3113474480486701}\n",
      "Default performance on Test: 0.6170210120888957\n",
      "SCORE: 0.6002943003988169                                                                                              \n",
      "SCORE: 0.599980228224649                                                                                               \n",
      "SCORE: 0.6004627311985473                                                                                              \n",
      "SCORE: 0.6006280125439696                                                                                              \n",
      "SCORE: 0.6002941961462598                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.56trial/s, best loss: 0.599980228224649]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.13342336035991123, 'n_estimators': 270.0}\n",
      "Test Performance after first tuning round: 0.6153586780726427\n",
      "SCORE: 0.5982391692350781                                                                                              \n",
      "SCORE: 0.6340236536339553                                                                                              \n",
      "SCORE: 0.6109733156630376                                                                                              \n",
      "SCORE: 0.6173797873265313                                                                                              \n",
      "SCORE: 0.6150907621944836                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.08trial/s, best loss: 0.5982391692350781]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.13342336035991123, 'n_estimators': 270.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 0.5987305189554873\n",
      "SCORE: 0.5974510503752131                                                                                              \n",
      "SCORE: 0.5980054980215534                                                                                              \n",
      "SCORE: 0.5976625663531474                                                                                              \n",
      "SCORE: 0.5977325868072818                                                                                              \n",
      "SCORE: 0.5972620601392424                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:08<00:00,  1.69s/trial, best loss: 0.5972620601392424]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.13342336035991123, 'n_estimators': 270.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.5222121641305251, 'subsample': 0.8706333037359387}\n",
      "Test Performance after third tuning round: 0.5975650164623528\n",
      "SCORE: 0.5987387528870071                                                                                              \n",
      "SCORE: 0.597689160370078                                                                                               \n",
      "SCORE: 0.5975728320084162                                                                                              \n",
      "SCORE: 0.5997510696232842                                                                                              \n",
      "SCORE: 0.601891043341182                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:07<00:00,  1.55s/trial, best loss: 0.5975728320084162]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.13342336035991123, 'n_estimators': 270.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.5222121641305251, 'subsample': 0.8706333037359387, 'gamma': 0.66517761008509, 'reg_alpha': 6.0, 'reg_lambda': 1.7450219317778255}\n",
      "Test Performance after last tuning round: 0.5969339331411918\n",
      "Preparing results for fold 1, condition=catboost\n",
      "SCORE: 0.6015995419657149                                                                                              \n",
      "SCORE: 0.6030388701813425                                                                                              \n",
      "SCORE: 0.601512183518311                                                                                               \n",
      "SCORE: 0.6026662420719717                                                                                              \n",
      "SCORE: 0.6032729778246922                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.75trial/s, best loss: 0.601512183518311]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.8325497333383343}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default performance on Test: 0.670421304237992\n",
      "SCORE: 0.6047188011852482                                                                                              \n",
      "SCORE: 0.6077634522161428                                                                                              \n",
      "SCORE: 0.6070325326678049                                                                                              \n",
      "SCORE: 0.6672717688768388                                                                                              \n",
      "SCORE: 0.6076723971364386                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:06<00:00,  1.28s/trial, best loss: 0.6047188011852482]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4006467595797892, 'n_estimators': 287.0}\n",
      "Test Performance after first tuning round: 0.786586746057795\n",
      "SCORE: 0.6273934941951655                                                                                              \n",
      "SCORE: 0.6038289551174163                                                                                              \n",
      "SCORE: 0.6190475630614485                                                                                              \n",
      "SCORE: 0.604307661528704                                                                                               \n",
      "SCORE: 0.6461585934023188                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:06<00:00,  1.32s/trial, best loss: 0.6038289551174163]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4006467595797892, 'n_estimators': 287.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 4.0}\n",
      "Test Performance after second tuning round: 0.7453816399002987\n",
      "SCORE: 0.6057474741649                                                                                                 \n",
      "SCORE: 0.6077527405339096                                                                                              \n",
      "SCORE: 0.6059926392461692                                                                                              \n",
      "SCORE: 0.6039898512259338                                                                                              \n",
      "SCORE: 0.6044605117008544                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.25trial/s, best loss: 0.6039898512259338]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4006467595797892, 'n_estimators': 287.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.8446925890892015, 'subsample': 0.692766059522667}\n",
      "Test Performance after third tuning round: 0.7875884057351373\n",
      "SCORE: 0.6066187447012654                                                                                              \n",
      "SCORE: 0.60220758107586                                                                                                \n",
      "SCORE: 0.6014508337872755                                                                                              \n",
      "SCORE: 0.6014503607854724                                                                                              \n",
      "SCORE: 0.6008902190502056                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.08s/trial, best loss: 0.6008902190502056]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4006467595797892, 'n_estimators': 287.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.8446925890892015, 'subsample': 0.692766059522667, 'gamma': 7.773257136566302, 'reg_alpha': 5.0, 'reg_lambda': 3.0927735997973365}\n",
      "Test Performance after last tuning round: 0.6114996018491854\n",
      "Preparing results for fold 1, condition=glmm\n",
      "SCORE: 0.5979871866301979                                                                                              \n",
      "SCORE: 0.5979151243379611                                                                                              \n",
      "SCORE: 0.5979123709527714                                                                                              \n",
      "SCORE: 0.5983548235948211                                                                                              \n",
      "SCORE: 0.5979119599166969                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.21trial/s, best loss: 0.5979119599166969]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.9677523499636155}\n",
      "Default performance on Test: 0.6361748213630788\n",
      "SCORE: 0.6106344277648553                                                                                              \n",
      "SCORE: 0.6022419987501706                                                                                              \n",
      "SCORE: 0.6076723404541113                                                                                              \n",
      "SCORE: 0.6085120703823822                                                                                              \n",
      "SCORE: 0.6088936270598003                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.34trial/s, best loss: 0.6022419987501706]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4846263956609598, 'n_estimators': 306.0}\n",
      "Test Performance after first tuning round: 0.7614359349926378\n",
      "SCORE: 0.5983352749063113                                                                                              \n",
      "SCORE: 0.5979260820549145                                                                                              \n",
      "SCORE: 0.61609599197848                                                                                                \n",
      "SCORE: 0.6207228149925965                                                                                              \n",
      "SCORE: 0.6185839364122268                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.49trial/s, best loss: 0.5979260820549145]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4846263956609598, 'n_estimators': 306.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 10.0}\n",
      "Test Performance after second tuning round: 0.6183395236929388\n",
      "SCORE: 0.598096412138746                                                                                               \n",
      "SCORE: 0.5993713458171053                                                                                              \n",
      "SCORE: 0.5988568120271298                                                                                              \n",
      "SCORE: 0.5995662095700143                                                                                              \n",
      "SCORE: 0.5982298079100936                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.58trial/s, best loss: 0.598096412138746]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4846263956609598, 'n_estimators': 306.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.8784945687347432, 'subsample': 0.6892322378662046}\n",
      "Test Performance after third tuning round: 0.6194060114524965\n",
      "SCORE: 0.5984304849840425                                                                                              \n",
      "SCORE: 0.598679855381443                                                                                               \n",
      "SCORE: 0.5979107961792268                                                                                              \n",
      "SCORE: 0.5992508941858828                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.5977891241143676                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.48trial/s, best loss: 0.5977891241143676]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4846263956609598, 'n_estimators': 306.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.8784945687347432, 'subsample': 0.6892322378662046, 'gamma': 4.723901282944907, 'reg_alpha': 4.0, 'reg_lambda': 2.924422725086774}\n",
      "Test Performance after last tuning round: 0.6019868991058321\n",
      "Preparing results for fold 2, condition=ignore\n",
      "SCORE: 0.6209029419246056                                                                                              \n",
      "SCORE: 0.6209031633594427                                                                                              \n",
      "SCORE: 0.6209034534815443                                                                                              \n",
      "SCORE: 0.620903196139827                                                                                               \n",
      "SCORE: 0.6209030555476174                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 19.84trial/s, best loss: 0.6209029419246056]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.32618228226851587}\n",
      "Default performance on Test: 0.624705213241029\n",
      "SCORE: 0.6812033235571867                                                                                              \n",
      "SCORE: 0.6449682179983164                                                                                              \n",
      "SCORE: 0.6422914352066804                                                                                              \n",
      "SCORE: 0.641555421922722                                                                                               \n",
      "SCORE: 0.6794153994241036                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.78trial/s, best loss: 0.641555421922722]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4802511013880511, 'n_estimators': 78.0}\n",
      "Test Performance after first tuning round: 0.6247051785185286\n",
      "SCORE: 0.641555421922722                                                                                               \n",
      "SCORE: 0.641555421922722                                                                                               \n",
      "SCORE: 0.641555421922722                                                                                               \n",
      "SCORE: 0.641555421922722                                                                                               \n",
      "SCORE: 0.641555421922722                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.89trial/s, best loss: 0.641555421922722]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4802511013880511, 'n_estimators': 78.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 2.0}\n",
      "Test Performance after second tuning round: 0.6247051785185286\n",
      "SCORE: 0.6227642247208728                                                                                              \n",
      "SCORE: 0.6227282803442526                                                                                              \n",
      "SCORE: 0.6227695385975988                                                                                              \n",
      "SCORE: 0.6227826120692684                                                                                              \n",
      "SCORE: 0.6228040099452912                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.58trial/s, best loss: 0.6227282803442526]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4802511013880511, 'n_estimators': 78.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.6719912327819505, 'subsample': 0.9150998246303976}\n",
      "Test Performance after third tuning round: 0.6244587057554203\n",
      "SCORE: 0.6230065735078051                                                                                              \n",
      "SCORE: 0.6219632205634805                                                                                              \n",
      "SCORE: 0.6214948633384573                                                                                              \n",
      "SCORE: 0.6220535140995923                                                                                              \n",
      "SCORE: 0.6216332442015976                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.53trial/s, best loss: 0.6214948633384573]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4802511013880511, 'n_estimators': 78.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.6719912327819505, 'subsample': 0.9150998246303976, 'gamma': 3.5574230255362433, 'reg_alpha': 4.0, 'reg_lambda': 1.4442888687304185}\n",
      "Test Performance after last tuning round: 0.624693057402855\n",
      "Preparing results for fold 2, condition=ohe\n",
      "SCORE: 0.5967147886478675                                                                                              \n",
      "SCORE: 0.5967130355321306                                                                                              \n",
      "SCORE: 0.5966887376644164                                                                                              \n",
      "SCORE: 0.5967055909448498                                                                                              \n",
      "SCORE: 0.5966953294847248                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.85trial/s, best loss: 0.5966887376644164]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.16563397652969894}\n",
      "Default performance on Test: 0.6227381299290058\n",
      "SCORE: 0.5991423389143102                                                                                              \n",
      "SCORE: 0.6003242723140463                                                                                              \n",
      "SCORE: 0.5994560124703827                                                                                              \n",
      "SCORE: 0.599465553539579                                                                                               \n",
      "SCORE: 0.6273370877582483                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:07<00:00,  1.49s/trial, best loss: 0.5991423389143102]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.18670046111081837, 'n_estimators': 224.0}\n",
      "Test Performance after first tuning round: 0.625357823102651\n",
      "SCORE: 0.6151719456289527                                                                                              \n",
      "SCORE: 0.6300256491903549                                                                                              \n",
      "SCORE: 0.6034808753353754                                                                                              \n",
      "SCORE: 0.5991620358967664                                                                                              \n",
      "SCORE: 0.614752180472849                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:08<00:00,  1.71s/trial, best loss: 0.5991620358967664]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.18670046111081837, 'n_estimators': 224.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.6189702577803007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.598637661037736                                                                                               \n",
      "SCORE: 0.60116243131516                                                                                                \n",
      "SCORE: 0.600853058189801                                                                                               \n",
      "SCORE: 0.5985456226489344                                                                                              \n",
      "SCORE: 0.5982842435414092                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:07<00:00,  1.41s/trial, best loss: 0.5982842435414092]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.18670046111081837, 'n_estimators': 224.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.6007855372762598, 'subsample': 0.8964099788096584}\n",
      "Test Performance after third tuning round: 0.6190582035458669\n",
      "SCORE: 0.5973434713058066                                                                                              \n",
      "SCORE: 0.5989022436679456                                                                                              \n",
      "SCORE: 0.5967694812079373                                                                                              \n",
      "SCORE: 0.5991692773564221                                                                                              \n",
      "SCORE: 0.5973494916112385                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:08<00:00,  1.78s/trial, best loss: 0.5967694812079373]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.18670046111081837, 'n_estimators': 224.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.6007855372762598, 'subsample': 0.8964099788096584, 'gamma': 0.8270526191828508, 'reg_alpha': 9.0, 'reg_lambda': 1.8352486397194214}\n",
      "Test Performance after last tuning round: 0.6006464234331277\n",
      "Preparing results for fold 2, condition=target\n",
      "SCORE: 0.5966455883583358                                                                                              \n",
      "SCORE: 0.5995559377264887                                                                                              \n",
      "SCORE: 0.5990473833503682                                                                                              \n",
      "SCORE: 0.5974555126142956                                                                                              \n",
      "SCORE: 0.6064138787253855                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.04trial/s, best loss: 0.5966455883583358]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.6450429248929184}\n",
      "Default performance on Test: 0.6229978712709887\n",
      "SCORE: 0.6011300694174094                                                                                              \n",
      "SCORE: 0.5998833137233925                                                                                              \n",
      "SCORE: 0.6600670290168048                                                                                              \n",
      "SCORE: 0.599634106241636                                                                                               \n",
      "SCORE: 0.6652781497777263                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.22trial/s, best loss: 0.599634106241636]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.1467633080725568, 'n_estimators': 408.0}\n",
      "Test Performance after first tuning round: 0.6292782573794364\n",
      "SCORE: 0.6380981023371733                                                                                              \n",
      "SCORE: 0.6462158792782261                                                                                              \n",
      "SCORE: 0.6382935352262175                                                                                              \n",
      "SCORE: 0.5966956474877685                                                                                              \n",
      "SCORE: 0.5971114564845527                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.04trial/s, best loss: 0.5966956474877685]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.1467633080725568, 'n_estimators': 408.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 2.0}\n",
      "Test Performance after second tuning round: 0.6051209602974464\n",
      "SCORE: 0.5999677317998116                                                                                              \n",
      "SCORE: 0.5969377658522872                                                                                              \n",
      "SCORE: 0.5977311268694384                                                                                              \n",
      "SCORE: 0.597043150833535                                                                                               \n",
      "SCORE: 0.6004700304492129                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.05s/trial, best loss: 0.5969377658522872]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.1467633080725568, 'n_estimators': 408.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.701066840980388, 'subsample': 0.7300392450576828}\n",
      "Test Performance after third tuning round: 0.6063927038336955\n",
      "SCORE: 0.5966108494629865                                                                                              \n",
      "SCORE: 0.596927526632089                                                                                               \n",
      "SCORE: 0.5971712768026487                                                                                              \n",
      "SCORE: 0.5964081803811302                                                                                              \n",
      "SCORE: 0.5976322654252233                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:06<00:00,  1.32s/trial, best loss: 0.5964081803811302]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.1467633080725568, 'n_estimators': 408.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.701066840980388, 'subsample': 0.7300392450576828, 'gamma': 2.9659763467248554, 'reg_alpha': 3.0, 'reg_lambda': 3.0061878954947137}\n",
      "Test Performance after last tuning round: 0.5991914977761146\n",
      "Preparing results for fold 2, condition=ordinal\n",
      "SCORE: 0.6162985886983102                                                                                              \n",
      "SCORE: 0.616298638993851                                                                                               \n",
      "SCORE: 0.6162975787427021                                                                                              \n",
      "SCORE: 0.6162981666412619                                                                                              \n",
      "SCORE: 0.6162980466942037                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.06trial/s, best loss: 0.6162975787427021]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.13322345396080015}\n",
      "Default performance on Test: 0.6225026696653517\n",
      "SCORE: 0.6030566481527337                                                                                              \n",
      "SCORE: 0.6002158998594987                                                                                              \n",
      "SCORE: 0.6017486685217582                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.6002866165136777                                                                                              \n",
      "SCORE: 0.6009320217613853                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.18trial/s, best loss: 0.6002158998594987]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.14314694824574486, 'n_estimators': 306.0}\n",
      "Test Performance after first tuning round: 0.6255798394490263\n",
      "SCORE: 0.6026633858533417                                                                                              \n",
      "SCORE: 0.6175623437011378                                                                                              \n",
      "SCORE: 0.6129833374825019                                                                                              \n",
      "SCORE: 0.6146812830753723                                                                                              \n",
      "SCORE: 0.5975946414872925                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.16s/trial, best loss: 0.5975946414872925]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.14314694824574486, 'n_estimators': 306.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 0.6033725777249116\n",
      "SCORE: 0.5976511577261364                                                                                              \n",
      "SCORE: 0.5976312728349886                                                                                              \n",
      "SCORE: 0.5974551478666886                                                                                              \n",
      "SCORE: 0.5974644641679836                                                                                              \n",
      "SCORE: 0.5977803649622355                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:08<00:00,  1.65s/trial, best loss: 0.5974551478666886]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.14314694824574486, 'n_estimators': 306.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.6185726383580579, 'subsample': 0.8484431056569293}\n",
      "Test Performance after third tuning round: 0.6031811846269278\n",
      "SCORE: 0.6020023370833214                                                                                              \n",
      "SCORE: 0.6002127340462009                                                                                              \n",
      "SCORE: 0.5989964891740345                                                                                              \n",
      "SCORE: 0.5994519564789702                                                                                              \n",
      "SCORE: 0.6027316920133315                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:06<00:00,  1.33s/trial, best loss: 0.5989964891740345]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.14314694824574486, 'n_estimators': 306.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.6185726383580579, 'subsample': 0.8484431056569293, 'gamma': 5.1349657712040395, 'reg_alpha': 5.0, 'reg_lambda': 3.7037765643832095}\n",
      "Test Performance after last tuning round: 0.6014982265824762\n",
      "Preparing results for fold 2, condition=catboost\n",
      "SCORE: 0.6031034997146139                                                                                              \n",
      "SCORE: 0.6016520535964407                                                                                              \n",
      "SCORE: 0.6016613858873695                                                                                              \n",
      "SCORE: 0.6021351727212492                                                                                              \n",
      "SCORE: 0.6017202435925476                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.27trial/s, best loss: 0.6016520535964407]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.9272239410941996}\n",
      "Default performance on Test: 0.6734488634118835\n",
      "SCORE: 0.6112461380874352                                                                                              \n",
      "SCORE: 0.6144651454183201                                                                                              \n",
      "SCORE: 0.6092297871874183                                                                                              \n",
      "SCORE: 0.6079479474223491                                                                                              \n",
      "SCORE: 0.6106368992404756                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.18s/trial, best loss: 0.6079479474223491]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.11030377013140984, 'n_estimators': 191.0}\n",
      "Test Performance after first tuning round: 0.6434529252122594\n",
      "SCORE: 0.6077968355365964                                                                                              \n",
      "SCORE: 0.6076663221763232                                                                                              \n",
      "SCORE: 0.6098312591706575                                                                                              \n",
      "SCORE: 0.600640524971543                                                                                               \n",
      "SCORE: 0.6238933851326601                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.80s/trial, best loss: 0.600640524971543]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.11030377013140984, 'n_estimators': 191.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 9.0}\n",
      "Test Performance after second tuning round: 0.6090572626022511\n",
      "SCORE: 0.6035842359770303                                                                                              \n",
      "SCORE: 0.6038156899067983                                                                                              \n",
      "SCORE: 0.6014614871766981                                                                                              \n",
      "SCORE: 0.6037604713469797                                                                                              \n",
      "SCORE: 0.6009124413375794                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:07<00:00,  1.45s/trial, best loss: 0.6009124413375794]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.11030377013140984, 'n_estimators': 191.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.8616126243298677, 'subsample': 0.8438859136059469}\n",
      "Test Performance after third tuning round: 0.6106690729576936\n",
      "SCORE: 0.6012181682480371                                                                                              \n",
      "SCORE: 0.6009961787330805                                                                                              \n",
      "SCORE: 0.6013704015592166                                                                                              \n",
      "SCORE: 0.6006581451161793                                                                                              \n",
      "SCORE: 0.6004043171498459                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:07<00:00,  1.50s/trial, best loss: 0.6004043171498459]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.11030377013140984, 'n_estimators': 191.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.8616126243298677, 'subsample': 0.8438859136059469, 'gamma': 0.5692747589885863, 'reg_alpha': 1.0, 'reg_lambda': 2.775257412678767}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance after last tuning round: 0.6080777958004696\n",
      "Preparing results for fold 2, condition=glmm\n",
      "SCORE: 0.5979075717550503                                                                                              \n",
      "SCORE: 0.5979202620320436                                                                                              \n",
      "SCORE: 0.5980841000322579                                                                                              \n",
      "SCORE: 0.597922017631328                                                                                               \n",
      "SCORE: 0.5979399377379615                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.06trial/s, best loss: 0.5979075717550503]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.7939575926526773}\n",
      "Default performance on Test: 0.6459813238019704\n",
      "SCORE: 0.6061119652178169                                                                                              \n",
      "SCORE: 0.6030607573207242                                                                                              \n",
      "SCORE: 0.6108572371138112                                                                                              \n",
      "SCORE: 0.6060588235681917                                                                                              \n",
      "SCORE: 0.6232502497670338                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.42trial/s, best loss: 0.6030607573207242]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.272722641927611, 'n_estimators': 178.0}\n",
      "Test Performance after first tuning round: 0.6589848033619833\n",
      "SCORE: 0.6239816266134801                                                                                              \n",
      "SCORE: 0.6158760221228413                                                                                              \n",
      "SCORE: 0.6172360931037966                                                                                              \n",
      "SCORE: 0.6159838570656866                                                                                              \n",
      "SCORE: 0.6004205520843027                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.40trial/s, best loss: 0.6004205520843027]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.272722641927611, 'n_estimators': 178.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.61946579211028\n",
      "SCORE: 0.6026288347735358                                                                                              \n",
      "SCORE: 0.5990625685103627                                                                                              \n",
      "SCORE: 0.6015414486987178                                                                                              \n",
      "SCORE: 0.6004709344564745                                                                                              \n",
      "SCORE: 0.6007957563136406                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.56trial/s, best loss: 0.5990625685103627]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.272722641927611, 'n_estimators': 178.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.6962609310529019, 'subsample': 0.7332788769860425}\n",
      "Test Performance after third tuning round: 0.6200435851306835\n",
      "SCORE: 0.5986241797065207                                                                                              \n",
      "SCORE: 0.5992496130014305                                                                                              \n",
      "SCORE: 0.5987004047555116                                                                                              \n",
      "SCORE: 0.5990819747678664                                                                                              \n",
      "SCORE: 0.5979491580529147                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.36trial/s, best loss: 0.5979491580529147]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.272722641927611, 'n_estimators': 178.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.6962609310529019, 'subsample': 0.7332788769860425, 'gamma': 4.364174613492789, 'reg_alpha': 1.0, 'reg_lambda': 2.689696947891718}\n",
      "Test Performance after last tuning round: 0.6045770556444431\n",
      "Preparing results for fold 3, condition=ignore\n",
      "SCORE: 0.6206228050417615                                                                                              \n",
      "SCORE: 0.6206237047809569                                                                                              \n",
      "SCORE: 0.620622886574571                                                                                               \n",
      "SCORE: 0.6206233153468087                                                                                              \n",
      "SCORE: 0.6206196896739262                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 19.23trial/s, best loss: 0.6206196896739262]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.11629273380551582}\n",
      "Default performance on Test: 0.6263899934824161\n",
      "SCORE: 0.6745232568434033                                                                                              \n",
      "SCORE: 0.6488432120959367                                                                                              \n",
      "SCORE: 0.679497021911983                                                                                               \n",
      "SCORE: 0.6915346742861334                                                                                              \n",
      "SCORE: 0.6878568811227628                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.76trial/s, best loss: 0.6488432120959367]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3868202319427809, 'n_estimators': 142.0}\n",
      "Test Performance after first tuning round: 0.6263900038557413\n",
      "SCORE: 0.6488432120959367                                                                                              \n",
      "SCORE: 0.6488432120959367                                                                                              \n",
      "SCORE: 0.6488432120959367                                                                                              \n",
      "SCORE: 0.6488432120959367                                                                                              \n",
      "SCORE: 0.6488432120959367                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.65trial/s, best loss: 0.6488432120959367]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3868202319427809, 'n_estimators': 142.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 8.0}\n",
      "Test Performance after second tuning round: 0.6263900038557413\n",
      "SCORE: 0.6251583360315994                                                                                              \n",
      "SCORE: 0.6250953411507025                                                                                              \n",
      "SCORE: 0.62506405369184                                                                                                \n",
      "SCORE: 0.6251112699337131                                                                                              \n",
      "SCORE: 0.6250805473600864                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.46trial/s, best loss: 0.62506405369184]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3868202319427809, 'n_estimators': 142.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.7168878608517066, 'subsample': 0.9252073969955656}\n",
      "Test Performance after third tuning round: 0.6261359057572918\n",
      "SCORE: 0.6207954336920123                                                                                              \n",
      "SCORE: 0.6207621016537429                                                                                              \n",
      "SCORE: 0.6253379485937073                                                                                              \n",
      "SCORE: 0.6207624977840794                                                                                              \n",
      "SCORE: 0.6207438804881142                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.15trial/s, best loss: 0.6207438804881142]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3868202319427809, 'n_estimators': 142.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.7168878608517066, 'subsample': 0.9252073969955656, 'gamma': 5.4500246269716515, 'reg_alpha': 3.0, 'reg_lambda': 1.615013445476031}\n",
      "Test Performance after last tuning round: 0.6263009832618184\n",
      "Preparing results for fold 3, condition=ohe\n",
      "SCORE: 0.5964560885835597                                                                                              \n",
      "SCORE: 0.5965233848439597                                                                                              \n",
      "SCORE: 0.5965262054147485                                                                                              \n",
      "SCORE: 0.5965162682716625                                                                                              \n",
      "SCORE: 0.5964424668234154                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.80trial/s, best loss: 0.5964424668234154]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.07263180237962319}\n",
      "Default performance on Test: 0.6214916081673847\n",
      "SCORE: 0.6027628345031811                                                                                              \n",
      "SCORE: 0.5997979452969052                                                                                              \n",
      "SCORE: 0.6005020254781861                                                                                              \n",
      "SCORE: 0.6001120511512668                                                                                              \n",
      "SCORE: 0.5995896975177312                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.17s/trial, best loss: 0.5995896975177312]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3904727362543373, 'n_estimators': 419.0}\n",
      "Test Performance after first tuning round: 0.6608676323936946\n",
      "SCORE: 0.5976994273024905                                                                                              \n",
      "SCORE: 0.6054226707266739                                                                                              \n",
      "SCORE: 0.5996294958206749                                                                                              \n",
      "SCORE: 0.59783517610323                                                                                                \n",
      "SCORE: 0.6038424111909189                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.12trial/s, best loss: 0.5976994273024905]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3904727362543373, 'n_estimators': 419.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.6159327744323937\n",
      "SCORE: 0.5977632044600927                                                                                              \n",
      "SCORE: 0.5969403431428271                                                                                              \n",
      "SCORE: 0.5977416927343151                                                                                              \n",
      "SCORE: 0.5972059441267396                                                                                              \n",
      "SCORE: 0.5973584022444418                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.07trial/s, best loss: 0.5969403431428271]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3904727362543373, 'n_estimators': 419.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.9536745298828677, 'subsample': 0.5516480224647353}\n",
      "Test Performance after third tuning round: 0.6211866685477373\n",
      "SCORE: 0.5970204446676883                                                                                              \n",
      "SCORE: 0.5964818393045688                                                                                              \n",
      "SCORE: 0.5965549904201509                                                                                              \n",
      "SCORE: 0.5993693105430473                                                                                              \n",
      "SCORE: 0.5984596575264597                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.17s/trial, best loss: 0.5964818393045688]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3904727362543373, 'n_estimators': 419.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.9536745298828677, 'subsample': 0.5516480224647353, 'gamma': 4.108254437107845, 'reg_alpha': 5.0, 'reg_lambda': 2.5859465514613853}\n",
      "Test Performance after last tuning round: 0.6009657046168922\n",
      "Preparing results for fold 3, condition=target\n",
      "SCORE: 0.5983988670081924                                                                                              \n",
      "SCORE: 0.5968375113629824                                                                                              \n",
      "SCORE: 0.6199968204535053                                                                                              \n",
      "SCORE: 0.5961670005797658                                                                                              \n",
      "SCORE: 0.5961414350610635                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.63trial/s, best loss: 0.5961414350610635]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.9973633653463561}\n",
      "Default performance on Test: 0.6268056102250082\n",
      "SCORE: 0.6039800822979884                                                                                              \n",
      "SCORE: 0.6239328011829615                                                                                              \n",
      "SCORE: 0.6148810325052372                                                                                              \n",
      "SCORE: 0.6163392933327075                                                                                              \n",
      "SCORE: 0.6239484226768011                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.04trial/s, best loss: 0.6039800822979884]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.31577934507360356, 'n_estimators': 146.0}\n",
      "Test Performance after first tuning round: 0.6341252471798301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.6291912489202215                                                                                              \n",
      "SCORE: 0.6060073098361691                                                                                              \n",
      "SCORE: 0.6344365692833567                                                                                              \n",
      "SCORE: 0.6373371625994922                                                                                              \n",
      "SCORE: 0.6353294251417373                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.51trial/s, best loss: 0.6060073098361691]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.31577934507360356, 'n_estimators': 146.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 10.0}\n",
      "Test Performance after second tuning round: 0.6388450153281484\n",
      "SCORE: 0.6081394209794461                                                                                              \n",
      "SCORE: 0.6028523992605113                                                                                              \n",
      "SCORE: 0.6144168299829083                                                                                              \n",
      "SCORE: 0.6048571570957124                                                                                              \n",
      "SCORE: 0.6011957510104327                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.61trial/s, best loss: 0.6011957510104327]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.31577934507360356, 'n_estimators': 146.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.6465162964028128, 'subsample': 0.7026467458332144}\n",
      "Test Performance after third tuning round: 0.616955553359276\n",
      "SCORE: 0.5977804411563385                                                                                              \n",
      "SCORE: 0.5977437285211569                                                                                              \n",
      "SCORE: 0.5968486824467594                                                                                              \n",
      "SCORE: 0.5968604928893255                                                                                              \n",
      "SCORE: 0.5966403888527773                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.11trial/s, best loss: 0.5966403888527773]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.31577934507360356, 'n_estimators': 146.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.6465162964028128, 'subsample': 0.7026467458332144, 'gamma': 2.1797164540980947, 'reg_alpha': 7.0, 'reg_lambda': 2.7357738743627396}\n",
      "Test Performance after last tuning round: 0.6012223439558354\n",
      "Preparing results for fold 3, condition=ordinal\n",
      "SCORE: 0.618658148293477                                                                                               \n",
      "SCORE: 0.6186615052348372                                                                                              \n",
      "SCORE: 0.6186599677903365                                                                                              \n",
      "SCORE: 0.6186614280969759                                                                                              \n",
      "SCORE: 0.6186577680914234                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.05trial/s, best loss: 0.6186577680914234]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.14957302762161634}\n",
      "Default performance on Test: 0.623185253225928\n",
      "SCORE: 0.6026021969959964                                                                                              \n",
      "SCORE: 0.6010124097166922                                                                                              \n",
      "SCORE: 0.5993518886253877                                                                                              \n",
      "SCORE: 0.602065067686115                                                                                               \n",
      "SCORE: 0.6006838535956532                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.40trial/s, best loss: 0.5993518886253877]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.47848442217729703, 'n_estimators': 205.0}\n",
      "Test Performance after first tuning round: 0.6480855767138906\n",
      "SCORE: 0.615272449968932                                                                                               \n",
      "SCORE: 0.6208504486703458                                                                                              \n",
      "SCORE: 0.5970921362861992                                                                                              \n",
      "SCORE: 0.6075144930067873                                                                                              \n",
      "SCORE: 0.6268189979817472                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.94trial/s, best loss: 0.5970921362861992]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.47848442217729703, 'n_estimators': 205.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 2.0}\n",
      "Test Performance after second tuning round: 0.6043091229022101\n",
      "SCORE: 0.5991734628941695                                                                                              \n",
      "SCORE: 0.5987328620636415                                                                                              \n",
      "SCORE: 0.596885943388169                                                                                               \n",
      "SCORE: 0.5969493339588674                                                                                              \n",
      "SCORE: 0.5966971330447483                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.39trial/s, best loss: 0.5966971330447483]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.47848442217729703, 'n_estimators': 205.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.5877944099394088, 'subsample': 0.8806173986745853}\n",
      "Test Performance after third tuning round: 0.603392667313955\n",
      "SCORE: 0.5969673936981614                                                                                              \n",
      "SCORE: 0.5990660533075847                                                                                              \n",
      "SCORE: 0.5980247592340476                                                                                              \n",
      "SCORE: 0.5979651080508452                                                                                              \n",
      "SCORE: 0.598441801902412                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.48trial/s, best loss: 0.5969673936981614]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.47848442217729703, 'n_estimators': 205.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.5877944099394088, 'subsample': 0.8806173986745853, 'gamma': 1.0210027699178912, 'reg_alpha': 5.0, 'reg_lambda': 1.1414695451890526}\n",
      "Test Performance after last tuning round: 0.6003622036096079\n",
      "Preparing results for fold 3, condition=catboost\n",
      "SCORE: 0.6154528680399478                                                                                              \n",
      "SCORE: 0.6006232253206267                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.6007163788256636                                                                                              \n",
      "SCORE: 0.6013585327255604                                                                                              \n",
      "SCORE: 0.6077814683864606                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.30trial/s, best loss: 0.6006232253206267]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.7965388938340073}\n",
      "Default performance on Test: 0.6511875789150704\n",
      "SCORE: 0.6230011430085965                                                                                              \n",
      "SCORE: 0.6156372830646257                                                                                              \n",
      "SCORE: 0.6363697564504015                                                                                              \n",
      "SCORE: 0.617911240295317                                                                                               \n",
      "SCORE: 0.6177037681291934                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.11s/trial, best loss: 0.6156372830646257]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.1313115662397814, 'n_estimators': 182.0}\n",
      "Test Performance after first tuning round: 0.6399508903630257\n",
      "SCORE: 0.620074376981717                                                                                               \n",
      "SCORE: 0.624233600014181                                                                                               \n",
      "SCORE: 0.6100034337356264                                                                                              \n",
      "SCORE: 0.614408175848172                                                                                               \n",
      "SCORE: 0.6165065313780529                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:09<00:00,  2.00s/trial, best loss: 0.6100034337356264]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.1313115662397814, 'n_estimators': 182.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 4.0}\n",
      "Test Performance after second tuning round: 0.6428071338985305\n",
      "SCORE: 0.6063745807741316                                                                                              \n",
      "SCORE: 0.6104000167399098                                                                                              \n",
      "SCORE: 0.6120607129468735                                                                                              \n",
      "SCORE: 0.6158521285597509                                                                                              \n",
      "SCORE: 0.6168244294260403                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:07<00:00,  1.51s/trial, best loss: 0.6063745807741316]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.1313115662397814, 'n_estimators': 182.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.7218658315501577, 'subsample': 0.687981108294763}\n",
      "Test Performance after third tuning round: 0.6484094166181513\n",
      "SCORE: 0.6060770153726731                                                                                              \n",
      "SCORE: 0.6115827346762545                                                                                              \n",
      "SCORE: 0.6022843492889957                                                                                              \n",
      "SCORE: 0.608122103478457                                                                                               \n",
      "SCORE: 0.5995637981440981                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:10<00:00,  2.16s/trial, best loss: 0.5995637981440981]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.1313115662397814, 'n_estimators': 182.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.7218658315501577, 'subsample': 0.687981108294763, 'gamma': 4.305255714644743, 'reg_alpha': 8.0, 'reg_lambda': 2.575895918291187}\n",
      "Test Performance after last tuning round: 0.6116191339558671\n",
      "Preparing results for fold 3, condition=glmm\n",
      "SCORE: 0.5979050521189664                                                                                              \n",
      "SCORE: 0.5975431706532811                                                                                              \n",
      "SCORE: 0.5975444446648196                                                                                              \n",
      "SCORE: 0.5975549097035496                                                                                              \n",
      "SCORE: 0.597542570077521                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.87trial/s, best loss: 0.597542570077521]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.7253712723288611}\n",
      "Default performance on Test: 0.6426858501527244\n",
      "SCORE: 0.6088389484698838                                                                                              \n",
      "SCORE: 0.6071136908316447                                                                                              \n",
      "SCORE: 0.6095053711924574                                                                                              \n",
      "SCORE: 0.6094305160319148                                                                                              \n",
      "SCORE: 0.6151420061717946                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.68trial/s, best loss: 0.6071136908316447]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3251950373433666, 'n_estimators': 197.0}\n",
      "Test Performance after first tuning round: 0.6754152430994962\n",
      "SCORE: 0.6429087171565427                                                                                              \n",
      "SCORE: 0.6146751411738495                                                                                              \n",
      "SCORE: 0.6071181556408138                                                                                              \n",
      "SCORE: 0.6161694903309793                                                                                              \n",
      "SCORE: 0.596958388260162                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.39trial/s, best loss: 0.596958388260162]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3251950373433666, 'n_estimators': 197.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 0.604422303300525\n",
      "SCORE: 0.5976865740993909                                                                                              \n",
      "SCORE: 0.5975242856465922                                                                                              \n",
      "SCORE: 0.5976155376859275                                                                                              \n",
      "SCORE: 0.5972765707745606                                                                                              \n",
      "SCORE: 0.5969686381273934                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.32trial/s, best loss: 0.5969686381273934]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3251950373433666, 'n_estimators': 197.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.5608440465303586, 'subsample': 0.682997879073356}\n",
      "Test Performance after third tuning round: 0.6053109575348813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.5971789368910974                                                                                              \n",
      "SCORE: 0.5974459680726518                                                                                              \n",
      "SCORE: 0.5973272071381414                                                                                              \n",
      "SCORE: 0.5974890676271228                                                                                              \n",
      "SCORE: 0.597631443351963                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.29trial/s, best loss: 0.5971789368910974]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3251950373433666, 'n_estimators': 197.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.5608440465303586, 'subsample': 0.682997879073356, 'gamma': 4.339278940523746, 'reg_alpha': 0.0, 'reg_lambda': 1.253128925045932}\n",
      "Test Performance after last tuning round: 0.6027020579989858\n",
      "Preparing results for fold 4, condition=ignore\n",
      "SCORE: 0.6226187141778807                                                                                              \n",
      "SCORE: 0.622620876180114                                                                                               \n",
      "SCORE: 0.6226208913272681                                                                                              \n",
      "SCORE: 0.6226209059380826                                                                                              \n",
      "SCORE: 0.6226208623644982                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 19.45trial/s, best loss: 0.6226187141778807]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.09300078579245266}\n",
      "Default performance on Test: 0.6177367790651125\n",
      "SCORE: 0.6454370945877084                                                                                              \n",
      "SCORE: 0.6924162827456816                                                                                              \n",
      "SCORE: 0.6843456159537296                                                                                              \n",
      "SCORE: 0.6463615023085738                                                                                              \n",
      "SCORE: 0.6788709664323215                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.50trial/s, best loss: 0.6454370945877084]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.44620156762323404, 'n_estimators': 334.0}\n",
      "Test Performance after first tuning round: 0.6177367047517318\n",
      "SCORE: 0.6454370945877084                                                                                              \n",
      "SCORE: 0.6454370945877084                                                                                              \n",
      "SCORE: 0.6454370945877084                                                                                              \n",
      "SCORE: 0.6454370945877084                                                                                              \n",
      "SCORE: 0.6454370945877084                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.79trial/s, best loss: 0.6454370945877084]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.44620156762323404, 'n_estimators': 334.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 10.0}\n",
      "Test Performance after second tuning round: 0.6177367047517318\n",
      "SCORE: 0.6291540017058561                                                                                              \n",
      "SCORE: 0.6291036816943921                                                                                              \n",
      "SCORE: 0.629083530522386                                                                                               \n",
      "SCORE: 0.6291250124350924                                                                                              \n",
      "SCORE: 0.6291400432017665                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.64trial/s, best loss: 0.629083530522386]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.44620156762323404, 'n_estimators': 334.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.9880275257373872, 'subsample': 0.547448948366116}\n",
      "Test Performance after third tuning round: 0.6177559782345339\n",
      "SCORE: 0.62329427779686                                                                                                \n",
      "SCORE: 0.6292103414314785                                                                                              \n",
      "SCORE: 0.623775633297866                                                                                               \n",
      "SCORE: 0.6275414115836419                                                                                              \n",
      "SCORE: 0.6237702189212498                                                                                              \n",
      "100%|████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.11trial/s, best loss: 0.62329427779686]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.44620156762323404, 'n_estimators': 334.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.9880275257373872, 'subsample': 0.547448948366116, 'gamma': 2.590789540860436, 'reg_alpha': 10.0, 'reg_lambda': 3.5804850921039844}\n",
      "Test Performance after last tuning round: 0.6178506669845572\n",
      "Preparing results for fold 4, condition=ohe\n",
      "SCORE: 0.5977219412386872                                                                                              \n",
      "SCORE: 0.5977488793020116                                                                                              \n",
      "SCORE: 0.5977613583441792                                                                                              \n",
      "SCORE: 0.5977229019170582                                                                                              \n",
      "SCORE: 0.5977190934565446                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.83trial/s, best loss: 0.5977190934565446]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.21082852398873136}\n",
      "Default performance on Test: 0.6162708661804789\n",
      "SCORE: 0.603250279793324                                                                                               \n",
      "SCORE: 0.6036258180382711                                                                                              \n",
      "SCORE: 0.6016878293145618                                                                                              \n",
      "SCORE: 0.6017791906025532                                                                                              \n",
      "SCORE: 0.6016974791316475                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:06<00:00,  1.38s/trial, best loss: 0.6016878293145618]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.05453663646373698, 'n_estimators': 92.0}\n",
      "Test Performance after first tuning round: 0.5981088353754183\n",
      "SCORE: 0.6207488376369769                                                                                              \n",
      "SCORE: 0.6058829805701657                                                                                              \n",
      "SCORE: 0.6394670690609675                                                                                              \n",
      "SCORE: 0.6027323462265183                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.6525158143422415                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:13<00:00,  2.74s/trial, best loss: 0.6027323462265183]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.05453663646373698, 'n_estimators': 92.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 10.0}\n",
      "Test Performance after second tuning round: 0.6022300941487789\n",
      "SCORE: 0.6143935909840421                                                                                              \n",
      "SCORE: 0.6127552675726959                                                                                              \n",
      "SCORE: 0.6213931848348881                                                                                              \n",
      "SCORE: 0.6252742300442466                                                                                              \n",
      "SCORE: 0.639421880846931                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:09<00:00,  1.81s/trial, best loss: 0.6127552675726959]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.05453663646373698, 'n_estimators': 92.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.9997586508325147, 'subsample': 0.5248475974324212}\n",
      "Test Performance after third tuning round: 0.59879802345865\n",
      "SCORE: 0.6010915583019332                                                                                              \n",
      "SCORE: 0.5986381640443575                                                                                              \n",
      "SCORE: 0.5992263480673542                                                                                              \n",
      "SCORE: 0.6025101116130911                                                                                              \n",
      "SCORE: 0.5988652655140915                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:22<00:00,  4.42s/trial, best loss: 0.5986381640443575]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.05453663646373698, 'n_estimators': 92.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.9997586508325147, 'subsample': 0.5248475974324212, 'gamma': 3.5645662705999746, 'reg_alpha': 2.0, 'reg_lambda': 2.3544276408445417}\n",
      "Test Performance after last tuning round: 0.5955394201204797\n",
      "Preparing results for fold 4, condition=target\n",
      "SCORE: 0.6018959754999155                                                                                              \n",
      "SCORE: 0.5980543004430123                                                                                              \n",
      "SCORE: 0.5979839475334038                                                                                              \n",
      "SCORE: 0.6064515036112377                                                                                              \n",
      "SCORE: 0.5974413448036489                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.18trial/s, best loss: 0.5974413448036489]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.9486694317804015}\n",
      "Default performance on Test: 0.6137332342398921\n",
      "SCORE: 0.6030005910661156                                                                                              \n",
      "SCORE: 0.6068871506747806                                                                                              \n",
      "SCORE: 0.6104807360086477                                                                                              \n",
      "SCORE: 0.6045460026844063                                                                                              \n",
      "SCORE: 0.6069952081265351                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.95trial/s, best loss: 0.6030005910661156]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3208892315875432, 'n_estimators': 65.0}\n",
      "Test Performance after first tuning round: 0.6096771512098168\n",
      "SCORE: 0.63456966788091                                                                                                \n",
      "SCORE: 0.6033099164616709                                                                                              \n",
      "SCORE: 0.599733603300227                                                                                               \n",
      "SCORE: 0.6035521238893871                                                                                              \n",
      "SCORE: 0.5977607568337342                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.69trial/s, best loss: 0.5977607568337342]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3208892315875432, 'n_estimators': 65.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.5976804943535787\n",
      "SCORE: 0.5975041011366929                                                                                              \n",
      "SCORE: 0.5975546000063316                                                                                              \n",
      "SCORE: 0.597873556676633                                                                                               \n",
      "SCORE: 0.5972102200141889                                                                                              \n",
      "SCORE: 0.5973488943931169                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.20trial/s, best loss: 0.5972102200141889]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3208892315875432, 'n_estimators': 65.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.9210466915186994, 'subsample': 0.9360451414236821}\n",
      "Test Performance after third tuning round: 0.5974118442565775\n",
      "SCORE: 0.5974987745541469                                                                                              \n",
      "SCORE: 0.5985998092030463                                                                                              \n",
      "SCORE: 0.5986525951704348                                                                                              \n",
      "SCORE: 0.5988857251559107                                                                                              \n",
      "SCORE: 0.5992156791264637                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.51trial/s, best loss: 0.5974987745541469]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3208892315875432, 'n_estimators': 65.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.9210466915186994, 'subsample': 0.9360451414236821, 'gamma': 0.6261286900244253, 'reg_alpha': 2.0, 'reg_lambda': 1.9048773419514808}\n",
      "Test Performance after last tuning round: 0.59658617388082\n",
      "Preparing results for fold 4, condition=ordinal\n",
      "SCORE: 0.6119437960081978                                                                                              \n",
      "SCORE: 0.6119450548357906                                                                                              \n",
      "SCORE: 0.6119417529659995                                                                                              \n",
      "SCORE: 0.6119513853633645                                                                                              \n",
      "SCORE: 0.6119457363229113                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.53trial/s, best loss: 0.6119417529659995]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.05911404165730402}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default performance on Test: 0.6178878935579406\n",
      "SCORE: 0.6033682178101658                                                                                              \n",
      "SCORE: 0.6212524804231888                                                                                              \n",
      "SCORE: 0.6021776168226325                                                                                              \n",
      "SCORE: 0.6695310329967695                                                                                              \n",
      "SCORE: 0.6054339829733932                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:07<00:00,  1.44s/trial, best loss: 0.6021776168226325]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.26243517658424165, 'n_estimators': 389.0}\n",
      "Test Performance after first tuning round: 0.6365859267071927\n",
      "SCORE: 0.6002202950888103                                                                                              \n",
      "SCORE: 0.6021776168226325                                                                                              \n",
      "SCORE: 0.6021336760249942                                                                                              \n",
      "SCORE: 0.6311735282078093                                                                                              \n",
      "SCORE: 0.6156898509129586                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:06<00:00,  1.24s/trial, best loss: 0.6002202950888103]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.26243517658424165, 'n_estimators': 389.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.5965140002244\n",
      "SCORE: 0.6031258576109007                                                                                              \n",
      "SCORE: 0.6004353137582289                                                                                              \n",
      "SCORE: 0.6017038302625458                                                                                              \n",
      "SCORE: 0.6020420982400827                                                                                              \n",
      "SCORE: 0.603657385715153                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:12<00:00,  2.41s/trial, best loss: 0.6004353137582289]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.26243517658424165, 'n_estimators': 389.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.9721935822659415, 'subsample': 0.7542501098513581}\n",
      "Test Performance after third tuning round: 0.5971335126205928\n",
      "SCORE: 0.6073065348422891                                                                                              \n",
      "SCORE: 0.599712459250109                                                                                               \n",
      "SCORE: 0.6035451840789957                                                                                              \n",
      "SCORE: 0.6046786145078162                                                                                              \n",
      "SCORE: 0.6079268237787058                                                                                              \n",
      "100%|███████████████████████████████████████████████████| 5/5 [00:08<00:00,  1.64s/trial, best loss: 0.599712459250109]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.26243517658424165, 'n_estimators': 389.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.9721935822659415, 'subsample': 0.7542501098513581, 'gamma': 1.4155167152537842, 'reg_alpha': 0.0, 'reg_lambda': 3.612509836421694}\n",
      "Test Performance after last tuning round: 0.5970591867907387\n",
      "Preparing results for fold 4, condition=catboost\n",
      "SCORE: 0.6029247070848845                                                                                              \n",
      "SCORE: 0.6025641744216569                                                                                              \n",
      "SCORE: 0.6022564788773643                                                                                              \n",
      "SCORE: 0.6021789793068842                                                                                              \n",
      "SCORE: 0.602212064992209                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.58trial/s, best loss: 0.6021789793068842]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.651979754426125}\n",
      "Default performance on Test: 0.6719882062396147\n",
      "SCORE: 0.6082127121149243                                                                                              \n",
      "SCORE: 0.6083965529060479                                                                                              \n",
      "SCORE: 0.607097293680428                                                                                               \n",
      "SCORE: 0.6074602667324521                                                                                              \n",
      "SCORE: 0.6063641697329313                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.09trial/s, best loss: 0.6063641697329313]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3453067995543792, 'n_estimators': 190.0}\n",
      "Test Performance after first tuning round: 0.7106432066495412\n",
      "SCORE: 0.621745406943844                                                                                               \n",
      "SCORE: 0.6010300371016358                                                                                              \n",
      "SCORE: 0.6185710166060614                                                                                              \n",
      "SCORE: 0.6189315580681374                                                                                              \n",
      "SCORE: 0.609836197145811                                                                                               \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.20s/trial, best loss: 0.6010300371016358]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3453067995543792, 'n_estimators': 190.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 0.6039727547981536\n",
      "SCORE: 0.6010957797606096                                                                                              \n",
      "SCORE: 0.6007925860568023                                                                                              \n",
      "SCORE: 0.6007188634814334                                                                                              \n",
      "SCORE: 0.600966515941008                                                                                               \n",
      "SCORE: 0.6008336855577936                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.01s/trial, best loss: 0.6007188634814334]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3453067995543792, 'n_estimators': 190.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.8161299501447582, 'subsample': 0.9108247493188844}\n",
      "Test Performance after third tuning round: 0.6070999620245593\n",
      "SCORE: 0.6011051116081505                                                                                              \n",
      "SCORE: 0.601692567758912                                                                                               \n",
      "SCORE: 0.6014049373711727                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.6008890133054804                                                                                              \n",
      "SCORE: 0.6015546042291552                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.12trial/s, best loss: 0.6008890133054804]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3453067995543792, 'n_estimators': 190.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.8161299501447582, 'subsample': 0.9108247493188844, 'gamma': 1.7351540675723383, 'reg_alpha': 9.0, 'reg_lambda': 1.2725650194696203}\n",
      "Test Performance after last tuning round: 0.6007589863075742\n",
      "Preparing results for fold 4, condition=glmm\n",
      "SCORE: 0.5992873061595962                                                                                              \n",
      "SCORE: 0.5987683195379809                                                                                              \n",
      "SCORE: 0.5987241568603278                                                                                              \n",
      "SCORE: 0.5987242529864345                                                                                              \n",
      "SCORE: 0.5987245240932664                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 11.21trial/s, best loss: 0.5987241568603278]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.7355463284265689}\n",
      "Default performance on Test: 0.6356456387102021\n",
      "SCORE: 0.6043919827751456                                                                                              \n",
      "SCORE: 0.6215698035622156                                                                                              \n",
      "SCORE: 0.6052845376974931                                                                                              \n",
      "SCORE: 0.6029280428433087                                                                                              \n",
      "SCORE: 0.6045599797481047                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.73trial/s, best loss: 0.6029280428433087]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4510088483056008, 'n_estimators': 395.0}\n",
      "Test Performance after first tuning round: 0.7596398756878376\n",
      "SCORE: 0.6123191226375206                                                                                              \n",
      "SCORE: 0.5981166376549347                                                                                              \n",
      "SCORE: 0.6136919177336388                                                                                              \n",
      "SCORE: 0.6100835221718744                                                                                              \n",
      "SCORE: 0.6039024385865341                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.85trial/s, best loss: 0.5981166376549347]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4510088483056008, 'n_estimators': 395.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 0.6048381397735377\n",
      "SCORE: 0.5984742525294224                                                                                              \n",
      "SCORE: 0.5988067000232721                                                                                              \n",
      "SCORE: 0.5988690496256269                                                                                              \n",
      "SCORE: 0.5983005032601312                                                                                              \n",
      "SCORE: 0.5983791435146495                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.54trial/s, best loss: 0.5983005032601312]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4510088483056008, 'n_estimators': 395.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.8161053006867462, 'subsample': 0.9041717730351315}\n",
      "Test Performance after third tuning round: 0.6070288287901199\n",
      "SCORE: 0.5998015648635235                                                                                              \n",
      "SCORE: 0.5991380718158756                                                                                              \n",
      "SCORE: 0.5989597066323816                                                                                              \n",
      "SCORE: 0.5992857187842703                                                                                              \n",
      "SCORE: 0.5991212097527205                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.44trial/s, best loss: 0.5989597066323816]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4510088483056008, 'n_estimators': 395.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.8161053006867462, 'subsample': 0.9041717730351315, 'gamma': 4.979905587648055, 'reg_alpha': 3.0, 'reg_lambda': 2.385299223759667}\n",
      "Test Performance after last tuning round: 0.5976533448299074\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_41f9c_row0_col4, #T_41f9c_row1_col4, #T_41f9c_row2_col4, #T_41f9c_row3_col4, #T_41f9c_row4_col4, #T_41f9c_row5_col4, #T_41f9c_row9_col3, #T_41f9c_row11_col5, #T_41f9c_row12_col5, #T_41f9c_row13_col5, #T_41f9c_row14_col5, #T_41f9c_row23_col0, #T_41f9c_row23_col1, #T_41f9c_row23_col2 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_41f9c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_41f9c_level0_col0\" class=\"col_heading level0 col0\" >Accuracy Train</th>\n",
       "      <th id=\"T_41f9c_level0_col1\" class=\"col_heading level0 col1\" >F1 Train</th>\n",
       "      <th id=\"T_41f9c_level0_col2\" class=\"col_heading level0 col2\" >AUROC Train</th>\n",
       "      <th id=\"T_41f9c_level0_col3\" class=\"col_heading level0 col3\" >Accuracy Test</th>\n",
       "      <th id=\"T_41f9c_level0_col4\" class=\"col_heading level0 col4\" >F1 Test</th>\n",
       "      <th id=\"T_41f9c_level0_col5\" class=\"col_heading level0 col5\" >AUROC Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row0\" class=\"row_heading level0 row0\" >LR_ignore</th>\n",
       "      <td id=\"T_41f9c_row0_col0\" class=\"data row0 col0\" >0.683000</td>\n",
       "      <td id=\"T_41f9c_row0_col1\" class=\"data row0 col1\" >0.811700</td>\n",
       "      <td id=\"T_41f9c_row0_col2\" class=\"data row0 col2\" >0.521200</td>\n",
       "      <td id=\"T_41f9c_row0_col3\" class=\"data row0 col3\" >0.696300</td>\n",
       "      <td id=\"T_41f9c_row0_col4\" class=\"data row0 col4\" >0.821000</td>\n",
       "      <td id=\"T_41f9c_row0_col5\" class=\"data row0 col5\" >0.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row1\" class=\"row_heading level0 row1\" >XGB_ignore</th>\n",
       "      <td id=\"T_41f9c_row1_col0\" class=\"data row1 col0\" >0.683000</td>\n",
       "      <td id=\"T_41f9c_row1_col1\" class=\"data row1 col1\" >0.811700</td>\n",
       "      <td id=\"T_41f9c_row1_col2\" class=\"data row1 col2\" >0.521200</td>\n",
       "      <td id=\"T_41f9c_row1_col3\" class=\"data row1 col3\" >0.696300</td>\n",
       "      <td id=\"T_41f9c_row1_col4\" class=\"data row1 col4\" >0.821000</td>\n",
       "      <td id=\"T_41f9c_row1_col5\" class=\"data row1 col5\" >0.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row2\" class=\"row_heading level0 row2\" >LR_ordinal_tuned</th>\n",
       "      <td id=\"T_41f9c_row2_col0\" class=\"data row2 col0\" >0.683000</td>\n",
       "      <td id=\"T_41f9c_row2_col1\" class=\"data row2 col1\" >0.811700</td>\n",
       "      <td id=\"T_41f9c_row2_col2\" class=\"data row2 col2\" >0.560000</td>\n",
       "      <td id=\"T_41f9c_row2_col3\" class=\"data row2 col3\" >0.696300</td>\n",
       "      <td id=\"T_41f9c_row2_col4\" class=\"data row2 col4\" >0.821000</td>\n",
       "      <td id=\"T_41f9c_row2_col5\" class=\"data row2 col5\" >0.546100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row3\" class=\"row_heading level0 row3\" >LR_ordinal</th>\n",
       "      <td id=\"T_41f9c_row3_col0\" class=\"data row3 col0\" >0.683000</td>\n",
       "      <td id=\"T_41f9c_row3_col1\" class=\"data row3 col1\" >0.811700</td>\n",
       "      <td id=\"T_41f9c_row3_col2\" class=\"data row3 col2\" >0.560000</td>\n",
       "      <td id=\"T_41f9c_row3_col3\" class=\"data row3 col3\" >0.696300</td>\n",
       "      <td id=\"T_41f9c_row3_col4\" class=\"data row3 col4\" >0.821000</td>\n",
       "      <td id=\"T_41f9c_row3_col5\" class=\"data row3 col5\" >0.546100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row4\" class=\"row_heading level0 row4\" >LR_ignore_tuned</th>\n",
       "      <td id=\"T_41f9c_row4_col0\" class=\"data row4 col0\" >0.683000</td>\n",
       "      <td id=\"T_41f9c_row4_col1\" class=\"data row4 col1\" >0.811700</td>\n",
       "      <td id=\"T_41f9c_row4_col2\" class=\"data row4 col2\" >0.521200</td>\n",
       "      <td id=\"T_41f9c_row4_col3\" class=\"data row4 col3\" >0.696300</td>\n",
       "      <td id=\"T_41f9c_row4_col4\" class=\"data row4 col4\" >0.821000</td>\n",
       "      <td id=\"T_41f9c_row4_col5\" class=\"data row4 col5\" >0.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row5\" class=\"row_heading level0 row5\" >XGB_ignore_tuned</th>\n",
       "      <td id=\"T_41f9c_row5_col0\" class=\"data row5 col0\" >0.683000</td>\n",
       "      <td id=\"T_41f9c_row5_col1\" class=\"data row5 col1\" >0.811700</td>\n",
       "      <td id=\"T_41f9c_row5_col2\" class=\"data row5 col2\" >0.521200</td>\n",
       "      <td id=\"T_41f9c_row5_col3\" class=\"data row5 col3\" >0.696300</td>\n",
       "      <td id=\"T_41f9c_row5_col4\" class=\"data row5 col4\" >0.821000</td>\n",
       "      <td id=\"T_41f9c_row5_col5\" class=\"data row5 col5\" >0.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row6\" class=\"row_heading level0 row6\" >LR_catboost_tuned</th>\n",
       "      <td id=\"T_41f9c_row6_col0\" class=\"data row6 col0\" >0.684500</td>\n",
       "      <td id=\"T_41f9c_row6_col1\" class=\"data row6 col1\" >0.809400</td>\n",
       "      <td id=\"T_41f9c_row6_col2\" class=\"data row6 col2\" >0.625800</td>\n",
       "      <td id=\"T_41f9c_row6_col3\" class=\"data row6 col3\" >0.695400</td>\n",
       "      <td id=\"T_41f9c_row6_col4\" class=\"data row6 col4\" >0.818100</td>\n",
       "      <td id=\"T_41f9c_row6_col5\" class=\"data row6 col5\" >0.613800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row7\" class=\"row_heading level0 row7\" >LR_catboost</th>\n",
       "      <td id=\"T_41f9c_row7_col0\" class=\"data row7 col0\" >0.684600</td>\n",
       "      <td id=\"T_41f9c_row7_col1\" class=\"data row7 col1\" >0.809400</td>\n",
       "      <td id=\"T_41f9c_row7_col2\" class=\"data row7 col2\" >0.625800</td>\n",
       "      <td id=\"T_41f9c_row7_col3\" class=\"data row7 col3\" >0.695400</td>\n",
       "      <td id=\"T_41f9c_row7_col4\" class=\"data row7 col4\" >0.818100</td>\n",
       "      <td id=\"T_41f9c_row7_col5\" class=\"data row7 col5\" >0.613800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row8\" class=\"row_heading level0 row8\" >XGB_ohe_tuned</th>\n",
       "      <td id=\"T_41f9c_row8_col0\" class=\"data row8 col0\" >0.689700</td>\n",
       "      <td id=\"T_41f9c_row8_col1\" class=\"data row8 col1\" >0.809000</td>\n",
       "      <td id=\"T_41f9c_row8_col2\" class=\"data row8 col2\" >0.645200</td>\n",
       "      <td id=\"T_41f9c_row8_col3\" class=\"data row8 col3\" >0.697900</td>\n",
       "      <td id=\"T_41f9c_row8_col4\" class=\"data row8 col4\" >0.816100</td>\n",
       "      <td id=\"T_41f9c_row8_col5\" class=\"data row8 col5\" >0.633800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row9\" class=\"row_heading level0 row9\" >LR_ohe</th>\n",
       "      <td id=\"T_41f9c_row9_col0\" class=\"data row9 col0\" >0.689800</td>\n",
       "      <td id=\"T_41f9c_row9_col1\" class=\"data row9 col1\" >0.808100</td>\n",
       "      <td id=\"T_41f9c_row9_col2\" class=\"data row9 col2\" >0.641300</td>\n",
       "      <td id=\"T_41f9c_row9_col3\" class=\"data row9 col3\" >0.699600</td>\n",
       "      <td id=\"T_41f9c_row9_col4\" class=\"data row9 col4\" >0.815900</td>\n",
       "      <td id=\"T_41f9c_row9_col5\" class=\"data row9 col5\" >0.633100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row10\" class=\"row_heading level0 row10\" >LR_ohe_tuned</th>\n",
       "      <td id=\"T_41f9c_row10_col0\" class=\"data row10 col0\" >0.689600</td>\n",
       "      <td id=\"T_41f9c_row10_col1\" class=\"data row10 col1\" >0.808200</td>\n",
       "      <td id=\"T_41f9c_row10_col2\" class=\"data row10 col2\" >0.641200</td>\n",
       "      <td id=\"T_41f9c_row10_col3\" class=\"data row10 col3\" >0.698800</td>\n",
       "      <td id=\"T_41f9c_row10_col4\" class=\"data row10 col4\" >0.815600</td>\n",
       "      <td id=\"T_41f9c_row10_col5\" class=\"data row10 col5\" >0.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row11\" class=\"row_heading level0 row11\" >LR_target</th>\n",
       "      <td id=\"T_41f9c_row11_col0\" class=\"data row11 col0\" >0.687600</td>\n",
       "      <td id=\"T_41f9c_row11_col1\" class=\"data row11 col1\" >0.808800</td>\n",
       "      <td id=\"T_41f9c_row11_col2\" class=\"data row11 col2\" >0.639100</td>\n",
       "      <td id=\"T_41f9c_row11_col3\" class=\"data row11 col3\" >0.695000</td>\n",
       "      <td id=\"T_41f9c_row11_col4\" class=\"data row11 col4\" >0.814600</td>\n",
       "      <td id=\"T_41f9c_row11_col5\" class=\"data row11 col5\" >0.635400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row12\" class=\"row_heading level0 row12\" >LR_target_tuned</th>\n",
       "      <td id=\"T_41f9c_row12_col0\" class=\"data row12 col0\" >0.687400</td>\n",
       "      <td id=\"T_41f9c_row12_col1\" class=\"data row12 col1\" >0.808800</td>\n",
       "      <td id=\"T_41f9c_row12_col2\" class=\"data row12 col2\" >0.639100</td>\n",
       "      <td id=\"T_41f9c_row12_col3\" class=\"data row12 col3\" >0.694700</td>\n",
       "      <td id=\"T_41f9c_row12_col4\" class=\"data row12 col4\" >0.814500</td>\n",
       "      <td id=\"T_41f9c_row12_col5\" class=\"data row12 col5\" >0.635400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row13\" class=\"row_heading level0 row13\" >LR_glmm</th>\n",
       "      <td id=\"T_41f9c_row13_col0\" class=\"data row13 col0\" >0.687600</td>\n",
       "      <td id=\"T_41f9c_row13_col1\" class=\"data row13 col1\" >0.808100</td>\n",
       "      <td id=\"T_41f9c_row13_col2\" class=\"data row13 col2\" >0.636600</td>\n",
       "      <td id=\"T_41f9c_row13_col3\" class=\"data row13 col3\" >0.695000</td>\n",
       "      <td id=\"T_41f9c_row13_col4\" class=\"data row13 col4\" >0.813500</td>\n",
       "      <td id=\"T_41f9c_row13_col5\" class=\"data row13 col5\" >0.635400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row14\" class=\"row_heading level0 row14\" >LR_glmm_tuned</th>\n",
       "      <td id=\"T_41f9c_row14_col0\" class=\"data row14 col0\" >0.687600</td>\n",
       "      <td id=\"T_41f9c_row14_col1\" class=\"data row14 col1\" >0.808100</td>\n",
       "      <td id=\"T_41f9c_row14_col2\" class=\"data row14 col2\" >0.636600</td>\n",
       "      <td id=\"T_41f9c_row14_col3\" class=\"data row14 col3\" >0.695000</td>\n",
       "      <td id=\"T_41f9c_row14_col4\" class=\"data row14 col4\" >0.813500</td>\n",
       "      <td id=\"T_41f9c_row14_col5\" class=\"data row14 col5\" >0.635400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row15\" class=\"row_heading level0 row15\" >XGB_ordinal_tuned</th>\n",
       "      <td id=\"T_41f9c_row15_col0\" class=\"data row15 col0\" >0.692200</td>\n",
       "      <td id=\"T_41f9c_row15_col1\" class=\"data row15 col1\" >0.808700</td>\n",
       "      <td id=\"T_41f9c_row15_col2\" class=\"data row15 col2\" >0.660100</td>\n",
       "      <td id=\"T_41f9c_row15_col3\" class=\"data row15 col3\" >0.697200</td>\n",
       "      <td id=\"T_41f9c_row15_col4\" class=\"data row15 col4\" >0.813500</td>\n",
       "      <td id=\"T_41f9c_row15_col5\" class=\"data row15 col5\" >0.629200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row16\" class=\"row_heading level0 row16\" >XGB_target_tuned</th>\n",
       "      <td id=\"T_41f9c_row16_col0\" class=\"data row16 col0\" >0.690300</td>\n",
       "      <td id=\"T_41f9c_row16_col1\" class=\"data row16 col1\" >0.807200</td>\n",
       "      <td id=\"T_41f9c_row16_col2\" class=\"data row16 col2\" >0.641100</td>\n",
       "      <td id=\"T_41f9c_row16_col3\" class=\"data row16 col3\" >0.697200</td>\n",
       "      <td id=\"T_41f9c_row16_col4\" class=\"data row16 col4\" >0.813300</td>\n",
       "      <td id=\"T_41f9c_row16_col5\" class=\"data row16 col5\" >0.633800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row17\" class=\"row_heading level0 row17\" >XGB_catboost_tuned</th>\n",
       "      <td id=\"T_41f9c_row17_col0\" class=\"data row17 col0\" >0.699600</td>\n",
       "      <td id=\"T_41f9c_row17_col1\" class=\"data row17 col1\" >0.812300</td>\n",
       "      <td id=\"T_41f9c_row17_col2\" class=\"data row17 col2\" >0.679900</td>\n",
       "      <td id=\"T_41f9c_row17_col3\" class=\"data row17 col3\" >0.690500</td>\n",
       "      <td id=\"T_41f9c_row17_col4\" class=\"data row17 col4\" >0.812800</td>\n",
       "      <td id=\"T_41f9c_row17_col5\" class=\"data row17 col5\" >0.606100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row18\" class=\"row_heading level0 row18\" >XGB_glmm_tuned</th>\n",
       "      <td id=\"T_41f9c_row18_col0\" class=\"data row18 col0\" >0.694500</td>\n",
       "      <td id=\"T_41f9c_row18_col1\" class=\"data row18 col1\" >0.810400</td>\n",
       "      <td id=\"T_41f9c_row18_col2\" class=\"data row18 col2\" >0.657000</td>\n",
       "      <td id=\"T_41f9c_row18_col3\" class=\"data row18 col3\" >0.695000</td>\n",
       "      <td id=\"T_41f9c_row18_col4\" class=\"data row18 col4\" >0.812000</td>\n",
       "      <td id=\"T_41f9c_row18_col5\" class=\"data row18 col5\" >0.628900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row19\" class=\"row_heading level0 row19\" >XGB_target</th>\n",
       "      <td id=\"T_41f9c_row19_col0\" class=\"data row19 col0\" >0.705400</td>\n",
       "      <td id=\"T_41f9c_row19_col1\" class=\"data row19 col1\" >0.814200</td>\n",
       "      <td id=\"T_41f9c_row19_col2\" class=\"data row19 col2\" >0.703800</td>\n",
       "      <td id=\"T_41f9c_row19_col3\" class=\"data row19 col3\" >0.685400</td>\n",
       "      <td id=\"T_41f9c_row19_col4\" class=\"data row19 col4\" >0.803600</td>\n",
       "      <td id=\"T_41f9c_row19_col5\" class=\"data row19 col5\" >0.604800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row20\" class=\"row_heading level0 row20\" >XGB_ohe</th>\n",
       "      <td id=\"T_41f9c_row20_col0\" class=\"data row20 col0\" >0.707500</td>\n",
       "      <td id=\"T_41f9c_row20_col1\" class=\"data row20 col1\" >0.814100</td>\n",
       "      <td id=\"T_41f9c_row20_col2\" class=\"data row20 col2\" >0.710000</td>\n",
       "      <td id=\"T_41f9c_row20_col3\" class=\"data row20 col3\" >0.684900</td>\n",
       "      <td id=\"T_41f9c_row20_col4\" class=\"data row20 col4\" >0.802100</td>\n",
       "      <td id=\"T_41f9c_row20_col5\" class=\"data row20 col5\" >0.605500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row21\" class=\"row_heading level0 row21\" >XGB_ordinal</th>\n",
       "      <td id=\"T_41f9c_row21_col0\" class=\"data row21 col0\" >0.706800</td>\n",
       "      <td id=\"T_41f9c_row21_col1\" class=\"data row21 col1\" >0.813800</td>\n",
       "      <td id=\"T_41f9c_row21_col2\" class=\"data row21 col2\" >0.704300</td>\n",
       "      <td id=\"T_41f9c_row21_col3\" class=\"data row21 col3\" >0.681600</td>\n",
       "      <td id=\"T_41f9c_row21_col4\" class=\"data row21 col4\" >0.800000</td>\n",
       "      <td id=\"T_41f9c_row21_col5\" class=\"data row21 col5\" >0.607500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row22\" class=\"row_heading level0 row22\" >XGB_glmm</th>\n",
       "      <td id=\"T_41f9c_row22_col0\" class=\"data row22 col0\" >0.730900</td>\n",
       "      <td id=\"T_41f9c_row22_col1\" class=\"data row22 col1\" >0.828400</td>\n",
       "      <td id=\"T_41f9c_row22_col2\" class=\"data row22 col2\" >0.764500</td>\n",
       "      <td id=\"T_41f9c_row22_col3\" class=\"data row22 col3\" >0.670700</td>\n",
       "      <td id=\"T_41f9c_row22_col4\" class=\"data row22 col4\" >0.789300</td>\n",
       "      <td id=\"T_41f9c_row22_col5\" class=\"data row22 col5\" >0.584600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41f9c_level0_row23\" class=\"row_heading level0 row23\" >XGB_catboost</th>\n",
       "      <td id=\"T_41f9c_row23_col0\" class=\"data row23 col0\" >0.791500</td>\n",
       "      <td id=\"T_41f9c_row23_col1\" class=\"data row23 col1\" >0.864500</td>\n",
       "      <td id=\"T_41f9c_row23_col2\" class=\"data row23 col2\" >0.878100</td>\n",
       "      <td id=\"T_41f9c_row23_col3\" class=\"data row23 col3\" >0.663300</td>\n",
       "      <td id=\"T_41f9c_row23_col4\" class=\"data row23 col4\" >0.788700</td>\n",
       "      <td id=\"T_41f9c_row23_col5\" class=\"data row23 col5\" >0.556500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x12d560f6400>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(f\"../results/{dataset_name}/{experiment_name}/results_encodings.pickle\"):\n",
    "\n",
    "    results_encodings = {}\n",
    "    results_encodings_feature_importances = {}\n",
    "\n",
    "    for fold in range(folds):\n",
    "        results_encodings[fold] = {}\n",
    "        results_encodings_feature_importances[fold] = {}\n",
    "        # Create baseline\n",
    "        y_train = data_dict[f\"y_train_{fold}\"]\n",
    "        y_val = data_dict[f\"y_val_{fold}\"]\n",
    "        y_test = data_dict[f\"y_test_{fold}\"]\n",
    "        y_train_val = np.concatenate([y_train,y_val])\n",
    "\n",
    "        max_ = np.argmax(np.unique(y_train_val,return_counts=True)[1])\n",
    "        y_train_val_pred_base = np.ones(y_train_val.shape[0])*max_\n",
    "        y_test_pred_base = np.ones(y_test.shape[0])*max_\n",
    "\n",
    "        results_encodings[fold][\"Baseline\"] = {}\n",
    "        eval_res_train = get_metrics(y_train_val, y_train_val_pred_base, target=target)\n",
    "        for metric in eval_res_train.keys():\n",
    "            results_encodings[fold][\"Baseline\"][metric + \" Train\"] = eval_res_train[metric]\n",
    "        eval_res_test = get_metrics(y_test, y_test_pred_base, target=target)\n",
    "        for metric in eval_res_test.keys():\n",
    "            results_encodings[fold][\"Baseline\"][metric + \" Test\"] = eval_res_test[metric]\n",
    "\n",
    "        for condition in conditions:\n",
    "            print(f\"Preparing results for fold {fold}, condition={condition}\")\n",
    "            # Retrieve data\n",
    "            z_cols = data_dict[\"z_cols\"]\n",
    "\n",
    "            X_train = data_dict[f\"X_train_{fold}\"]\n",
    "            y_train = data_dict[f\"y_train_{fold}\"]\n",
    "\n",
    "            X_val = data_dict[f\"X_val_{fold}\"]\n",
    "            y_val = data_dict[f\"y_val_{fold}\"]\n",
    "\n",
    "            X_test = data_dict[f\"X_test_{fold}\"]\n",
    "            y_test = data_dict[f\"y_test_{fold}\"]\n",
    "\n",
    "    ## ALL BUT PERFORMANCE:\n",
    "            # Define data subset for evaluation\n",
    "    #         X_train = X_train[[i for i in X_train.columns if all([j not in i for j in perf_cols])]]\n",
    "    #         X_val = X_val[[i for i in X_val.columns if all([j not in i for j in perf_cols])]]\n",
    "    #         X_test = X_test[[i for i in X_test.columns if all([j not in i for j in perf_cols])]]\n",
    "\n",
    "            # Define condition data subset\n",
    "    #         if condition != \"ignore\":\n",
    "    #             z_encoded_train = data_dict[f\"z_{condition}_encoded_train_{fold}\"] \n",
    "    #             z_encoded_val = data_dict[f\"z_{condition}_encoded_val_{fold}\"] \n",
    "    #             z_encoded_test = data_dict[f\"z_{condition}_encoded_test_{fold}\"] \n",
    "\n",
    "    #             X_train = pd.concat([X_train,z_encoded_train],axis=1)\n",
    "    #             X_val = pd.concat([X_val,z_encoded_val],axis=1)\n",
    "    #             X_test = pd.concat([X_test,z_encoded_test],axis=1)\n",
    "\n",
    "    # ALL BUT PERFORMANCE & ACTIVITY:\n",
    "    #         Define data subset for evaluation\n",
    "            X_train = X_train[[i for i in X_train.columns if all([j not in i for j in perf_cols+activity_cols])]]\n",
    "            X_val = X_val[[i for i in X_val.columns if all([j not in i for j in perf_cols+activity_cols])]]\n",
    "            X_test = X_test[[i for i in X_test.columns if all([j not in i for j in perf_cols+activity_cols])]]\n",
    "\n",
    "    #         Define condition data subset\n",
    "            if condition != \"ignore\":\n",
    "                z_encoded_train = data_dict[f\"z_{condition}_encoded_train_{fold}\"] \n",
    "                z_encoded_val = data_dict[f\"z_{condition}_encoded_val_{fold}\"] \n",
    "                z_encoded_test = data_dict[f\"z_{condition}_encoded_test_{fold}\"] \n",
    "\n",
    "                X_train = pd.concat([X_train,z_encoded_train],axis=1)\n",
    "                X_val = pd.concat([X_val,z_encoded_val],axis=1)\n",
    "                X_test = pd.concat([X_test,z_encoded_test],axis=1)\n",
    "\n",
    "\n",
    "    ## ONLY CATEGORICAL: --> Produces trash as almost never better than baseline\n",
    "    #         if condition != \"ignore\":        \n",
    "    #             X_train = data_dict[f\"z_{condition}_encoded_train_{fold}\"] \n",
    "    #             X_val = data_dict[f\"z_{condition}_encoded_val_{fold}\"] \n",
    "    #             X_test = data_dict[f\"z_{condition}_encoded_test_{fold}\"] \n",
    "\n",
    "    #         else:\n",
    "    #             continue\n",
    "\n",
    "            X_train_val = pd.concat([X_train,X_val])\n",
    "            y_train_val = np.concatenate([y_train,y_val])\n",
    "\n",
    "            # Train base models\n",
    "            res, feats = evaluate_logreg(X_train_val, y_train_val, X_test, y_test, target=target,tune=False, seed=RS)\n",
    "            results_encodings[fold][\"LR_\"+condition] = res\n",
    "            results_encodings_feature_importances[fold][\"LR_\"+condition] = feats\n",
    "\n",
    "            res, feats = evaluate_xgb(X_train_val, y_train_val, X_test, y_test, target, tune=False, max_evals=max_evals, early_stopping_rounds=early_stopping_rounds, seed=RS)\n",
    "            results_encodings[fold][\"XGB_\"+condition] = res\n",
    "            results_encodings_feature_importances[fold][\"XGB_\"+condition] = feats\n",
    "\n",
    "            # Train tuned models\n",
    "            res, feats = evaluate_logreg(X_train_val, y_train_val, X_test, y_test, target=target, max_evals=max_evals, tune=True, seed=RS)\n",
    "            results_encodings[fold][\"LR_\"+condition+\"_tuned\"] = res\n",
    "            results_encodings_feature_importances[fold][\"LR_\"+condition+\"_tuned\"] = feats\n",
    "\n",
    "            res, feats = evaluate_xgb(X_train_val, y_train_val, X_test, y_test, target, tune=True, max_evals=max_evals, early_stopping_rounds=early_stopping_rounds, seed=RS)\n",
    "            results_encodings[fold][\"XGB_\"+condition+\"_tuned\"] = res\n",
    "            results_encodings_feature_importances[fold][\"XGB_\"+condition+\"_tuned\"] = feats\n",
    "    \n",
    "    if not os.path.exists(f\"../results/{dataset_name}/{experiment_name}\"):\n",
    "        os.makedirs(f\"../results/{dataset_name}/{experiment_name}\")\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_encodings.pickle\", 'wb') as handle:\n",
    "        pickle.dump(results_encodings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_encodings_feature_importances.pickle\", 'wb') as handle:\n",
    "        pickle.dump(results_encodings_feature_importances, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "else:\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_encodings.pickle\", 'rb') as handle:\n",
    "        results_encodings = pickle.load(handle)\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_encodings_feature_importances.pickle\", 'rb') as handle:\n",
    "        results_encodings_feature_importances = pickle.load(handle)\n",
    "        \n",
    "        \n",
    "results_encodings_df = pd.DataFrame(results_encodings[0]).transpose().sort_values(\"F1 Test\",ascending=False).round(4)\n",
    "results_encodings_df[[\"Accuracy Train\", \"F1 Train\", \"AUROC Train\", \"Accuracy Test\", \"F1 Test\", \"AUROC Test\"]].style.highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8cc7f1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_41693_row0_col4, #T_41693_row1_col4, #T_41693_row2_col4, #T_41693_row3_col4, #T_41693_row4_col4, #T_41693_row5_col5, #T_41693_row9_col3, #T_41693_row24_col0, #T_41693_row24_col1, #T_41693_row24_col2 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_41693\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_41693_level0_col0\" class=\"col_heading level0 col0\" >Accuracy Train</th>\n",
       "      <th id=\"T_41693_level0_col1\" class=\"col_heading level0 col1\" >F1 Train</th>\n",
       "      <th id=\"T_41693_level0_col2\" class=\"col_heading level0 col2\" >AUROC Train</th>\n",
       "      <th id=\"T_41693_level0_col3\" class=\"col_heading level0 col3\" >Accuracy Test</th>\n",
       "      <th id=\"T_41693_level0_col4\" class=\"col_heading level0 col4\" >F1 Test</th>\n",
       "      <th id=\"T_41693_level0_col5\" class=\"col_heading level0 col5\" >AUROC Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row0\" class=\"row_heading level0 row0\" >LR_ignore</th>\n",
       "      <td id=\"T_41693_row0_col0\" class=\"data row0 col0\" >0.687100</td>\n",
       "      <td id=\"T_41693_row0_col1\" class=\"data row0 col1\" >0.814500</td>\n",
       "      <td id=\"T_41693_row0_col2\" class=\"data row0 col2\" >0.521100</td>\n",
       "      <td id=\"T_41693_row0_col3\" class=\"data row0 col3\" >0.680300</td>\n",
       "      <td id=\"T_41693_row0_col4\" class=\"data row0 col4\" >0.809700</td>\n",
       "      <td id=\"T_41693_row0_col5\" class=\"data row0 col5\" >0.517400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row1\" class=\"row_heading level0 row1\" >XGB_ignore_tuned</th>\n",
       "      <td id=\"T_41693_row1_col0\" class=\"data row1 col0\" >0.687100</td>\n",
       "      <td id=\"T_41693_row1_col1\" class=\"data row1 col1\" >0.814500</td>\n",
       "      <td id=\"T_41693_row1_col2\" class=\"data row1 col2\" >0.521100</td>\n",
       "      <td id=\"T_41693_row1_col3\" class=\"data row1 col3\" >0.680300</td>\n",
       "      <td id=\"T_41693_row1_col4\" class=\"data row1 col4\" >0.809700</td>\n",
       "      <td id=\"T_41693_row1_col5\" class=\"data row1 col5\" >0.517400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row2\" class=\"row_heading level0 row2\" >XGB_ignore</th>\n",
       "      <td id=\"T_41693_row2_col0\" class=\"data row2 col0\" >0.687100</td>\n",
       "      <td id=\"T_41693_row2_col1\" class=\"data row2 col1\" >0.814500</td>\n",
       "      <td id=\"T_41693_row2_col2\" class=\"data row2 col2\" >0.521100</td>\n",
       "      <td id=\"T_41693_row2_col3\" class=\"data row2 col3\" >0.680300</td>\n",
       "      <td id=\"T_41693_row2_col4\" class=\"data row2 col4\" >0.809700</td>\n",
       "      <td id=\"T_41693_row2_col5\" class=\"data row2 col5\" >0.517400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row3\" class=\"row_heading level0 row3\" >Baseline</th>\n",
       "      <td id=\"T_41693_row3_col0\" class=\"data row3 col0\" >0.687100</td>\n",
       "      <td id=\"T_41693_row3_col1\" class=\"data row3 col1\" >0.814500</td>\n",
       "      <td id=\"T_41693_row3_col2\" class=\"data row3 col2\" >0.500000</td>\n",
       "      <td id=\"T_41693_row3_col3\" class=\"data row3 col3\" >0.680300</td>\n",
       "      <td id=\"T_41693_row3_col4\" class=\"data row3 col4\" >0.809700</td>\n",
       "      <td id=\"T_41693_row3_col5\" class=\"data row3 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row4\" class=\"row_heading level0 row4\" >LR_ignore_tuned</th>\n",
       "      <td id=\"T_41693_row4_col0\" class=\"data row4 col0\" >0.687100</td>\n",
       "      <td id=\"T_41693_row4_col1\" class=\"data row4 col1\" >0.814500</td>\n",
       "      <td id=\"T_41693_row4_col2\" class=\"data row4 col2\" >0.521100</td>\n",
       "      <td id=\"T_41693_row4_col3\" class=\"data row4 col3\" >0.680300</td>\n",
       "      <td id=\"T_41693_row4_col4\" class=\"data row4 col4\" >0.809700</td>\n",
       "      <td id=\"T_41693_row4_col5\" class=\"data row4 col5\" >0.517400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row5\" class=\"row_heading level0 row5\" >XGB_ordinal_tuned</th>\n",
       "      <td id=\"T_41693_row5_col0\" class=\"data row5 col0\" >0.692900</td>\n",
       "      <td id=\"T_41693_row5_col1\" class=\"data row5 col1\" >0.812400</td>\n",
       "      <td id=\"T_41693_row5_col2\" class=\"data row5 col2\" >0.646200</td>\n",
       "      <td id=\"T_41693_row5_col3\" class=\"data row5 col3\" >0.689200</td>\n",
       "      <td id=\"T_41693_row5_col4\" class=\"data row5 col4\" >0.809400</td>\n",
       "      <td id=\"T_41693_row5_col5\" class=\"data row5 col5\" >0.646100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row6\" class=\"row_heading level0 row6\" >LR_target_tuned</th>\n",
       "      <td id=\"T_41693_row6_col0\" class=\"data row6 col0\" >0.688800</td>\n",
       "      <td id=\"T_41693_row6_col1\" class=\"data row6 col1\" >0.810900</td>\n",
       "      <td id=\"T_41693_row6_col2\" class=\"data row6 col2\" >0.637100</td>\n",
       "      <td id=\"T_41693_row6_col3\" class=\"data row6 col3\" >0.687800</td>\n",
       "      <td id=\"T_41693_row6_col4\" class=\"data row6 col4\" >0.809000</td>\n",
       "      <td id=\"T_41693_row6_col5\" class=\"data row6 col5\" >0.639200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row7\" class=\"row_heading level0 row7\" >LR_catboost_tuned</th>\n",
       "      <td id=\"T_41693_row7_col0\" class=\"data row7 col0\" >0.686600</td>\n",
       "      <td id=\"T_41693_row7_col1\" class=\"data row7 col1\" >0.811500</td>\n",
       "      <td id=\"T_41693_row7_col2\" class=\"data row7 col2\" >0.627600</td>\n",
       "      <td id=\"T_41693_row7_col3\" class=\"data row7 col3\" >0.688900</td>\n",
       "      <td id=\"T_41693_row7_col4\" class=\"data row7 col4\" >0.809000</td>\n",
       "      <td id=\"T_41693_row7_col5\" class=\"data row7 col5\" >0.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row8\" class=\"row_heading level0 row8\" >LR_catboost</th>\n",
       "      <td id=\"T_41693_row8_col0\" class=\"data row8 col0\" >0.686900</td>\n",
       "      <td id=\"T_41693_row8_col1\" class=\"data row8 col1\" >0.811700</td>\n",
       "      <td id=\"T_41693_row8_col2\" class=\"data row8 col2\" >0.627600</td>\n",
       "      <td id=\"T_41693_row8_col3\" class=\"data row8 col3\" >0.688700</td>\n",
       "      <td id=\"T_41693_row8_col4\" class=\"data row8 col4\" >0.808800</td>\n",
       "      <td id=\"T_41693_row8_col5\" class=\"data row8 col5\" >0.623800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row9\" class=\"row_heading level0 row9\" >LR_ohe</th>\n",
       "      <td id=\"T_41693_row9_col0\" class=\"data row9 col0\" >0.691700</td>\n",
       "      <td id=\"T_41693_row9_col1\" class=\"data row9 col1\" >0.811000</td>\n",
       "      <td id=\"T_41693_row9_col2\" class=\"data row9 col2\" >0.639000</td>\n",
       "      <td id=\"T_41693_row9_col3\" class=\"data row9 col3\" >0.689600</td>\n",
       "      <td id=\"T_41693_row9_col4\" class=\"data row9 col4\" >0.808600</td>\n",
       "      <td id=\"T_41693_row9_col5\" class=\"data row9 col5\" >0.643500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row10\" class=\"row_heading level0 row10\" >LR_target</th>\n",
       "      <td id=\"T_41693_row10_col0\" class=\"data row10 col0\" >0.688700</td>\n",
       "      <td id=\"T_41693_row10_col1\" class=\"data row10 col1\" >0.810700</td>\n",
       "      <td id=\"T_41693_row10_col2\" class=\"data row10 col2\" >0.637200</td>\n",
       "      <td id=\"T_41693_row10_col3\" class=\"data row10 col3\" >0.687400</td>\n",
       "      <td id=\"T_41693_row10_col4\" class=\"data row10 col4\" >0.808500</td>\n",
       "      <td id=\"T_41693_row10_col5\" class=\"data row10 col5\" >0.639400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row11\" class=\"row_heading level0 row11\" >LR_ordinal_tuned</th>\n",
       "      <td id=\"T_41693_row11_col0\" class=\"data row11 col0\" >0.686600</td>\n",
       "      <td id=\"T_41693_row11_col1\" class=\"data row11 col1\" >0.813900</td>\n",
       "      <td id=\"T_41693_row11_col2\" class=\"data row11 col2\" >0.591200</td>\n",
       "      <td id=\"T_41693_row11_col3\" class=\"data row11 col3\" >0.678900</td>\n",
       "      <td id=\"T_41693_row11_col4\" class=\"data row11 col4\" >0.808500</td>\n",
       "      <td id=\"T_41693_row11_col5\" class=\"data row11 col5\" >0.596100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row12\" class=\"row_heading level0 row12\" >LR_ordinal</th>\n",
       "      <td id=\"T_41693_row12_col0\" class=\"data row12 col0\" >0.686600</td>\n",
       "      <td id=\"T_41693_row12_col1\" class=\"data row12 col1\" >0.813900</td>\n",
       "      <td id=\"T_41693_row12_col2\" class=\"data row12 col2\" >0.591200</td>\n",
       "      <td id=\"T_41693_row12_col3\" class=\"data row12 col3\" >0.678900</td>\n",
       "      <td id=\"T_41693_row12_col4\" class=\"data row12 col4\" >0.808500</td>\n",
       "      <td id=\"T_41693_row12_col5\" class=\"data row12 col5\" >0.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row13\" class=\"row_heading level0 row13\" >LR_ohe_tuned</th>\n",
       "      <td id=\"T_41693_row13_col0\" class=\"data row13 col0\" >0.691800</td>\n",
       "      <td id=\"T_41693_row13_col1\" class=\"data row13 col1\" >0.811100</td>\n",
       "      <td id=\"T_41693_row13_col2\" class=\"data row13 col2\" >0.639000</td>\n",
       "      <td id=\"T_41693_row13_col3\" class=\"data row13 col3\" >0.688900</td>\n",
       "      <td id=\"T_41693_row13_col4\" class=\"data row13 col4\" >0.808300</td>\n",
       "      <td id=\"T_41693_row13_col5\" class=\"data row13 col5\" >0.643400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row14\" class=\"row_heading level0 row14\" >LR_glmm</th>\n",
       "      <td id=\"T_41693_row14_col0\" class=\"data row14 col0\" >0.688000</td>\n",
       "      <td id=\"T_41693_row14_col1\" class=\"data row14 col1\" >0.810300</td>\n",
       "      <td id=\"T_41693_row14_col2\" class=\"data row14 col2\" >0.634200</td>\n",
       "      <td id=\"T_41693_row14_col3\" class=\"data row14 col3\" >0.686900</td>\n",
       "      <td id=\"T_41693_row14_col4\" class=\"data row14 col4\" >0.808100</td>\n",
       "      <td id=\"T_41693_row14_col5\" class=\"data row14 col5\" >0.639400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row15\" class=\"row_heading level0 row15\" >LR_glmm_tuned</th>\n",
       "      <td id=\"T_41693_row15_col0\" class=\"data row15 col0\" >0.688000</td>\n",
       "      <td id=\"T_41693_row15_col1\" class=\"data row15 col1\" >0.810300</td>\n",
       "      <td id=\"T_41693_row15_col2\" class=\"data row15 col2\" >0.634200</td>\n",
       "      <td id=\"T_41693_row15_col3\" class=\"data row15 col3\" >0.686900</td>\n",
       "      <td id=\"T_41693_row15_col4\" class=\"data row15 col4\" >0.808100</td>\n",
       "      <td id=\"T_41693_row15_col5\" class=\"data row15 col5\" >0.639400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row16\" class=\"row_heading level0 row16\" >XGB_ohe_tuned</th>\n",
       "      <td id=\"T_41693_row16_col0\" class=\"data row16 col0\" >0.692100</td>\n",
       "      <td id=\"T_41693_row16_col1\" class=\"data row16 col1\" >0.811800</td>\n",
       "      <td id=\"T_41693_row16_col2\" class=\"data row16 col2\" >0.643300</td>\n",
       "      <td id=\"T_41693_row16_col3\" class=\"data row16 col3\" >0.687200</td>\n",
       "      <td id=\"T_41693_row16_col4\" class=\"data row16 col4\" >0.807400</td>\n",
       "      <td id=\"T_41693_row16_col5\" class=\"data row16 col5\" >0.644300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row17\" class=\"row_heading level0 row17\" >XGB_catboost_tuned</th>\n",
       "      <td id=\"T_41693_row17_col0\" class=\"data row17 col0\" >0.694600</td>\n",
       "      <td id=\"T_41693_row17_col1\" class=\"data row17 col1\" >0.813000</td>\n",
       "      <td id=\"T_41693_row17_col2\" class=\"data row17 col2\" >0.653700</td>\n",
       "      <td id=\"T_41693_row17_col3\" class=\"data row17 col3\" >0.683400</td>\n",
       "      <td id=\"T_41693_row17_col4\" class=\"data row17 col4\" >0.807100</td>\n",
       "      <td id=\"T_41693_row17_col5\" class=\"data row17 col5\" >0.611500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row18\" class=\"row_heading level0 row18\" >XGB_target_tuned</th>\n",
       "      <td id=\"T_41693_row18_col0\" class=\"data row18 col0\" >0.694200</td>\n",
       "      <td id=\"T_41693_row18_col1\" class=\"data row18 col1\" >0.812000</td>\n",
       "      <td id=\"T_41693_row18_col2\" class=\"data row18 col2\" >0.649300</td>\n",
       "      <td id=\"T_41693_row18_col3\" class=\"data row18 col3\" >0.684900</td>\n",
       "      <td id=\"T_41693_row18_col4\" class=\"data row18 col4\" >0.806500</td>\n",
       "      <td id=\"T_41693_row18_col5\" class=\"data row18 col5\" >0.644100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row19\" class=\"row_heading level0 row19\" >XGB_glmm_tuned</th>\n",
       "      <td id=\"T_41693_row19_col0\" class=\"data row19 col0\" >0.696000</td>\n",
       "      <td id=\"T_41693_row19_col1\" class=\"data row19 col1\" >0.812200</td>\n",
       "      <td id=\"T_41693_row19_col2\" class=\"data row19 col2\" >0.653400</td>\n",
       "      <td id=\"T_41693_row19_col3\" class=\"data row19 col3\" >0.686500</td>\n",
       "      <td id=\"T_41693_row19_col4\" class=\"data row19 col4\" >0.804800</td>\n",
       "      <td id=\"T_41693_row19_col5\" class=\"data row19 col5\" >0.637700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row20\" class=\"row_heading level0 row20\" >XGB_ordinal</th>\n",
       "      <td id=\"T_41693_row20_col0\" class=\"data row20 col0\" >0.710500</td>\n",
       "      <td id=\"T_41693_row20_col1\" class=\"data row20 col1\" >0.818200</td>\n",
       "      <td id=\"T_41693_row20_col2\" class=\"data row20 col2\" >0.701500</td>\n",
       "      <td id=\"T_41693_row20_col3\" class=\"data row20 col3\" >0.673800</td>\n",
       "      <td id=\"T_41693_row20_col4\" class=\"data row20 col4\" >0.794300</td>\n",
       "      <td id=\"T_41693_row20_col5\" class=\"data row20 col5\" >0.615900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row21\" class=\"row_heading level0 row21\" >XGB_ohe</th>\n",
       "      <td id=\"T_41693_row21_col0\" class=\"data row21 col0\" >0.711900</td>\n",
       "      <td id=\"T_41693_row21_col1\" class=\"data row21 col1\" >0.818500</td>\n",
       "      <td id=\"T_41693_row21_col2\" class=\"data row21 col2\" >0.709100</td>\n",
       "      <td id=\"T_41693_row21_col3\" class=\"data row21 col3\" >0.674700</td>\n",
       "      <td id=\"T_41693_row21_col4\" class=\"data row21 col4\" >0.794100</td>\n",
       "      <td id=\"T_41693_row21_col5\" class=\"data row21 col5\" >0.615600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row22\" class=\"row_heading level0 row22\" >XGB_target</th>\n",
       "      <td id=\"T_41693_row22_col0\" class=\"data row22 col0\" >0.710100</td>\n",
       "      <td id=\"T_41693_row22_col1\" class=\"data row22 col1\" >0.817800</td>\n",
       "      <td id=\"T_41693_row22_col2\" class=\"data row22 col2\" >0.701400</td>\n",
       "      <td id=\"T_41693_row22_col3\" class=\"data row22 col3\" >0.672000</td>\n",
       "      <td id=\"T_41693_row22_col4\" class=\"data row22 col4\" >0.793100</td>\n",
       "      <td id=\"T_41693_row22_col5\" class=\"data row22 col5\" >0.618300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row23\" class=\"row_heading level0 row23\" >XGB_glmm</th>\n",
       "      <td id=\"T_41693_row23_col0\" class=\"data row23 col0\" >0.733100</td>\n",
       "      <td id=\"T_41693_row23_col1\" class=\"data row23 col1\" >0.830400</td>\n",
       "      <td id=\"T_41693_row23_col2\" class=\"data row23 col2\" >0.765200</td>\n",
       "      <td id=\"T_41693_row23_col3\" class=\"data row23 col3\" >0.669300</td>\n",
       "      <td id=\"T_41693_row23_col4\" class=\"data row23 col4\" >0.786800</td>\n",
       "      <td id=\"T_41693_row23_col5\" class=\"data row23 col5\" >0.610200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_41693_level0_row24\" class=\"row_heading level0 row24\" >XGB_catboost</th>\n",
       "      <td id=\"T_41693_row24_col0\" class=\"data row24 col0\" >0.788600</td>\n",
       "      <td id=\"T_41693_row24_col1\" class=\"data row24 col1\" >0.864200</td>\n",
       "      <td id=\"T_41693_row24_col2\" class=\"data row24 col2\" >0.870900</td>\n",
       "      <td id=\"T_41693_row24_col3\" class=\"data row24 col3\" >0.657500</td>\n",
       "      <td id=\"T_41693_row24_col4\" class=\"data row24 col4\" >0.779800</td>\n",
       "      <td id=\"T_41693_row24_col5\" class=\"data row24 col5\" >0.576300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x12d53d7a460>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_encodings_df = pd.DataFrame(results_encodings[1]).transpose().sort_values(\"F1 Test\",ascending=False).round(4)\n",
    "results_encodings_df[[\"Accuracy Train\", \"F1 Train\", \"AUROC Train\", \"Accuracy Test\", \"F1 Test\", \"AUROC Test\"]].style.highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be91e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "801297d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f1ebb_row0_col0, #T_f1ebb_row0_col1, #T_f1ebb_row1_col0, #T_f1ebb_row1_col1, #T_f1ebb_row2_col0, #T_f1ebb_row2_col1, #T_f1ebb_row3_col0, #T_f1ebb_row3_col1, #T_f1ebb_row4_col0, #T_f1ebb_row4_col1, #T_f1ebb_row5_col0, #T_f1ebb_row5_col1, #T_f1ebb_row6_col0, #T_f1ebb_row6_col1, #T_f1ebb_row7_col1, #T_f1ebb_row8_col1, #T_f1ebb_row9_col0, #T_f1ebb_row9_col1, #T_f1ebb_row10_col1, #T_f1ebb_row11_col1, #T_f1ebb_row12_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f1ebb\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f1ebb_level0_col0\" class=\"col_heading level0 col0\" >Untuned</th>\n",
       "      <th id=\"T_f1ebb_level0_col1\" class=\"col_heading level0 col1\" >Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f1ebb_level0_row0\" class=\"row_heading level0 row0\" >Baseline</th>\n",
       "      <td id=\"T_f1ebb_row0_col0\" class=\"data row0 col0\" >0.810000</td>\n",
       "      <td id=\"T_f1ebb_row0_col1\" class=\"data row0 col1\" >0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1ebb_level0_row1\" class=\"row_heading level0 row1\" >LR_catboost</th>\n",
       "      <td id=\"T_f1ebb_row1_col0\" class=\"data row1 col0\" >0.810000</td>\n",
       "      <td id=\"T_f1ebb_row1_col1\" class=\"data row1 col1\" >0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1ebb_level0_row2\" class=\"row_heading level0 row2\" >LR_glmm</th>\n",
       "      <td id=\"T_f1ebb_row2_col0\" class=\"data row2 col0\" >0.810000</td>\n",
       "      <td id=\"T_f1ebb_row2_col1\" class=\"data row2 col1\" >0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1ebb_level0_row3\" class=\"row_heading level0 row3\" >LR_ignore</th>\n",
       "      <td id=\"T_f1ebb_row3_col0\" class=\"data row3 col0\" >0.810000</td>\n",
       "      <td id=\"T_f1ebb_row3_col1\" class=\"data row3 col1\" >0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1ebb_level0_row4\" class=\"row_heading level0 row4\" >LR_ohe</th>\n",
       "      <td id=\"T_f1ebb_row4_col0\" class=\"data row4 col0\" >0.810000</td>\n",
       "      <td id=\"T_f1ebb_row4_col1\" class=\"data row4 col1\" >0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1ebb_level0_row5\" class=\"row_heading level0 row5\" >LR_ordinal</th>\n",
       "      <td id=\"T_f1ebb_row5_col0\" class=\"data row5 col0\" >0.810000</td>\n",
       "      <td id=\"T_f1ebb_row5_col1\" class=\"data row5 col1\" >0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1ebb_level0_row6\" class=\"row_heading level0 row6\" >LR_target</th>\n",
       "      <td id=\"T_f1ebb_row6_col0\" class=\"data row6 col0\" >0.810000</td>\n",
       "      <td id=\"T_f1ebb_row6_col1\" class=\"data row6 col1\" >0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1ebb_level0_row7\" class=\"row_heading level0 row7\" >XGB_catboost</th>\n",
       "      <td id=\"T_f1ebb_row7_col0\" class=\"data row7 col0\" >0.790000</td>\n",
       "      <td id=\"T_f1ebb_row7_col1\" class=\"data row7 col1\" >0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1ebb_level0_row8\" class=\"row_heading level0 row8\" >XGB_glmm</th>\n",
       "      <td id=\"T_f1ebb_row8_col0\" class=\"data row8 col0\" >0.780000</td>\n",
       "      <td id=\"T_f1ebb_row8_col1\" class=\"data row8 col1\" >0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1ebb_level0_row9\" class=\"row_heading level0 row9\" >XGB_ignore</th>\n",
       "      <td id=\"T_f1ebb_row9_col0\" class=\"data row9 col0\" >0.810000</td>\n",
       "      <td id=\"T_f1ebb_row9_col1\" class=\"data row9 col1\" >0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1ebb_level0_row10\" class=\"row_heading level0 row10\" >XGB_ohe</th>\n",
       "      <td id=\"T_f1ebb_row10_col0\" class=\"data row10 col0\" >0.800000</td>\n",
       "      <td id=\"T_f1ebb_row10_col1\" class=\"data row10 col1\" >0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1ebb_level0_row11\" class=\"row_heading level0 row11\" >XGB_ordinal</th>\n",
       "      <td id=\"T_f1ebb_row11_col0\" class=\"data row11 col0\" >0.800000</td>\n",
       "      <td id=\"T_f1ebb_row11_col1\" class=\"data row11 col1\" >0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1ebb_level0_row12\" class=\"row_heading level0 row12\" >XGB_target</th>\n",
       "      <td id=\"T_f1ebb_row12_col0\" class=\"data row12 col0\" >0.800000</td>\n",
       "      <td id=\"T_f1ebb_row12_col1\" class=\"data row12 col1\" >0.810000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x12d5a3c6be0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = results_encodings[0].keys()\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "encodings_folds_df = pd.DataFrame([pd.DataFrame(results_encodings[fold_num]).loc[metric,models] for fold_num in results_encodings.keys()],index=results_encodings.keys())\n",
    "encodings_mean_df = encodings_folds_df.mean(axis=0)\n",
    "encodings_std_df = encodings_folds_df.std(axis=0)\n",
    "\n",
    "methods = sorted(list(encodings_mean_df.index))[1:]\n",
    "not_tuned = [\"Baseline\"]+methods[::2]\n",
    "tuned = [\"Baseline\"]+methods[1::2]\n",
    "\n",
    "res_df_tune_comp_mean = pd.DataFrame([encodings_mean_df.loc[not_tuned].values,encodings_mean_df.loc[tuned]],index=[\"Untuned\",\"Tuned\"],columns=not_tuned).transpose()\n",
    "res_df_tune_comp_std = pd.DataFrame([encodings_std_df.loc[not_tuned].values,encodings_std_df.loc[tuned]],index=[\"Untuned\",\"Tuned\"],columns=not_tuned).transpose()\n",
    "\n",
    "res_df_tune_comp_mean.round(2).style.highlight_max(color = 'lightgreen', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4663d8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Untuned</th>\n",
       "      <th>Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.81 (0.005)</td>\n",
       "      <td>0.81 (0.005)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_catboost</th>\n",
       "      <td>0.81 (0.006)</td>\n",
       "      <td>0.81 (0.006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_glmm</th>\n",
       "      <td>0.81 (0.004)</td>\n",
       "      <td>0.81 (0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_ignore</th>\n",
       "      <td>0.81 (0.005)</td>\n",
       "      <td>0.81 (0.005)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_ohe</th>\n",
       "      <td>0.81 (0.004)</td>\n",
       "      <td>0.81 (0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_ordinal</th>\n",
       "      <td>0.81 (0.005)</td>\n",
       "      <td>0.81 (0.005)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_target</th>\n",
       "      <td>0.81 (0.004)</td>\n",
       "      <td>0.81 (0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_catboost</th>\n",
       "      <td>0.79 (0.005)</td>\n",
       "      <td>0.81 (0.006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_glmm</th>\n",
       "      <td>0.78 (0.01)</td>\n",
       "      <td>0.81 (0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_ignore</th>\n",
       "      <td>0.81 (0.005)</td>\n",
       "      <td>0.81 (0.005)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_ohe</th>\n",
       "      <td>0.8 (0.004)</td>\n",
       "      <td>0.81 (0.006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_ordinal</th>\n",
       "      <td>0.8 (0.003)</td>\n",
       "      <td>0.81 (0.005)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_target</th>\n",
       "      <td>0.8 (0.006)</td>\n",
       "      <td>0.81 (0.004)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Untuned         Tuned\n",
       "Baseline      0.81 (0.005)  0.81 (0.005)\n",
       "LR_catboost   0.81 (0.006)  0.81 (0.006)\n",
       "LR_glmm       0.81 (0.004)  0.81 (0.004)\n",
       "LR_ignore     0.81 (0.005)  0.81 (0.005)\n",
       "LR_ohe        0.81 (0.004)  0.81 (0.004)\n",
       "LR_ordinal    0.81 (0.005)  0.81 (0.005)\n",
       "LR_target     0.81 (0.004)  0.81 (0.004)\n",
       "XGB_catboost  0.79 (0.005)  0.81 (0.006)\n",
       "XGB_glmm       0.78 (0.01)  0.81 (0.004)\n",
       "XGB_ignore    0.81 (0.005)  0.81 (0.005)\n",
       "XGB_ohe        0.8 (0.004)  0.81 (0.006)\n",
       "XGB_ordinal    0.8 (0.003)  0.81 (0.005)\n",
       "XGB_target     0.8 (0.006)  0.81 (0.004)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_df = res_df_tune_comp_mean.round(2).astype(str) + \" (\" +  res_df_tune_comp_std.round(3).astype(str) + \")\"\n",
    "latex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d86c70de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lll}\n",
      "\\toprule\n",
      "{} &       Untuned &         Tuned \\\\\n",
      "\\midrule\n",
      "Baseline     &  0.81 (0.005) &  0.81 (0.005) \\\\\n",
      "LR\\_catboost  &  0.81 (0.006) &  0.81 (0.006) \\\\\n",
      "LR\\_glmm      &  0.81 (0.004) &  0.81 (0.004) \\\\\n",
      "LR\\_ignore    &  0.81 (0.005) &  0.81 (0.005) \\\\\n",
      "LR\\_ohe       &  0.81 (0.004) &  0.81 (0.004) \\\\\n",
      "LR\\_ordinal   &  0.81 (0.005) &  0.81 (0.005) \\\\\n",
      "LR\\_target    &  0.81 (0.004) &  0.81 (0.004) \\\\\n",
      "XGB\\_catboost &  0.79 (0.005) &  0.81 (0.006) \\\\\n",
      "XGB\\_glmm     &   0.78 (0.01) &  0.81 (0.004) \\\\\n",
      "XGB\\_ignore   &  0.81 (0.005) &  0.81 (0.005) \\\\\n",
      "XGB\\_ohe      &   0.8 (0.004) &  0.81 (0.006) \\\\\n",
      "XGB\\_ordinal  &   0.8 (0.003) &  0.81 (0.005) \\\\\n",
      "XGB\\_target   &   0.8 (0.006) &  0.81 (0.004) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_df.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9283b2bd",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84ecd634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2d3ba_row0_col0, #T_2d3ba_row0_col1, #T_2d3ba_row0_col4 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2d3ba\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_2d3ba_level0_col0\" class=\"col_heading level0 col0\" >Baseline</th>\n",
       "      <th id=\"T_2d3ba_level0_col1\" class=\"col_heading level0 col1\" >LR_ignore_tuned</th>\n",
       "      <th id=\"T_2d3ba_level0_col2\" class=\"col_heading level0 col2\" >LR_ohe_tuned</th>\n",
       "      <th id=\"T_2d3ba_level0_col3\" class=\"col_heading level0 col3\" >LR_target_tuned</th>\n",
       "      <th id=\"T_2d3ba_level0_col4\" class=\"col_heading level0 col4\" >LR_ordinal_tuned</th>\n",
       "      <th id=\"T_2d3ba_level0_col5\" class=\"col_heading level0 col5\" >LR_catboost_tuned</th>\n",
       "      <th id=\"T_2d3ba_level0_col6\" class=\"col_heading level0 col6\" >LR_glmm_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2d3ba_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_2d3ba_row0_col0\" class=\"data row0 col0\" >0.814 (0.005)</td>\n",
       "      <td id=\"T_2d3ba_row0_col1\" class=\"data row0 col1\" >0.814 (0.005)</td>\n",
       "      <td id=\"T_2d3ba_row0_col2\" class=\"data row0 col2\" >0.81 (0.004)</td>\n",
       "      <td id=\"T_2d3ba_row0_col3\" class=\"data row0 col3\" >0.81 (0.004)</td>\n",
       "      <td id=\"T_2d3ba_row0_col4\" class=\"data row0 col4\" >0.813 (0.005)</td>\n",
       "      <td id=\"T_2d3ba_row0_col5\" class=\"data row0 col5\" >0.81 (0.006)</td>\n",
       "      <td id=\"T_2d3ba_row0_col6\" class=\"data row0 col6\" >0.809 (0.004)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x12d5a442ac0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For LR\n",
    "models = [\"Baseline\"]+[i for i in results_encodings[0].keys() if (\"tuned\" in i and \"LR\" in i)]\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "use_df = pd.DataFrame([pd.DataFrame(results_encodings[fold_num]).loc[metric,models] for fold_num in results_encodings.keys()],index=results_encodings.keys())\n",
    "\n",
    "df_mean = pd.DataFrame((use_df).mean(axis=0).round(3).astype(str) + \" (\" + use_df.std(axis=0).round(3).astype(str) + \")\").transpose()\n",
    "model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "\n",
    "best_model = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "t_test_res = np.array([stats.ttest_rel(use_df[best_model].values, use_df[model].values)[1] for model in models]).round(3)\n",
    "t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    \n",
    "res_df_lr = pd.DataFrame([model_dict])\n",
    "\n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_res[i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "res_df_lr.style.apply(negative_bold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "151ac2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_53c74_row0_col0, #T_53c74_row0_col1 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_53c74\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_53c74_level0_col0\" class=\"col_heading level0 col0\" >Baseline</th>\n",
       "      <th id=\"T_53c74_level0_col1\" class=\"col_heading level0 col1\" >XGB_ignore_tuned</th>\n",
       "      <th id=\"T_53c74_level0_col2\" class=\"col_heading level0 col2\" >XGB_ohe_tuned</th>\n",
       "      <th id=\"T_53c74_level0_col3\" class=\"col_heading level0 col3\" >XGB_target_tuned</th>\n",
       "      <th id=\"T_53c74_level0_col4\" class=\"col_heading level0 col4\" >XGB_ordinal_tuned</th>\n",
       "      <th id=\"T_53c74_level0_col5\" class=\"col_heading level0 col5\" >XGB_catboost_tuned</th>\n",
       "      <th id=\"T_53c74_level0_col6\" class=\"col_heading level0 col6\" >XGB_glmm_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_53c74_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_53c74_row0_col0\" class=\"data row0 col0\" >0.814 (0.005)</td>\n",
       "      <td id=\"T_53c74_row0_col1\" class=\"data row0 col1\" >0.814 (0.005)</td>\n",
       "      <td id=\"T_53c74_row0_col2\" class=\"data row0 col2\" >0.808 (0.006)</td>\n",
       "      <td id=\"T_53c74_row0_col3\" class=\"data row0 col3\" >0.808 (0.004)</td>\n",
       "      <td id=\"T_53c74_row0_col4\" class=\"data row0 col4\" >0.809 (0.005)</td>\n",
       "      <td id=\"T_53c74_row0_col5\" class=\"data row0 col5\" >0.808 (0.006)</td>\n",
       "      <td id=\"T_53c74_row0_col6\" class=\"data row0 col6\" >0.807 (0.004)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x12d5a442b20>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For LR\n",
    "models = [\"Baseline\"]+[i for i in results_encodings[0].keys() if (\"tuned\" in i and \"XGB\" in i)]\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "use_df = pd.DataFrame([pd.DataFrame(results_encodings[fold_num]).loc[metric,models] for fold_num in results_encodings.keys()],index=results_encodings.keys())\n",
    "\n",
    "df_mean = pd.DataFrame((use_df).mean(axis=0).round(3).astype(str) + \" (\" + use_df.std(axis=0).round(3).astype(str) + \")\").transpose()\n",
    "model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "\n",
    "best_model = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "t_test_res = np.array([stats.ttest_rel(use_df[best_model].values, use_df[model].values)[1] for model in models]).round(3)\n",
    "t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    \n",
    "res_df_xgb = pd.DataFrame([model_dict])\n",
    "\n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_res[i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "res_df_xgb.style.apply(negative_bold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8c4ad95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline</th>\n",
       "      <th>ignore</th>\n",
       "      <th>ohe</th>\n",
       "      <th>target</th>\n",
       "      <th>ordinal</th>\n",
       "      <th>catboost</th>\n",
       "      <th>glmm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.814 (0.005)</td>\n",
       "      <td>0.814 (0.005)</td>\n",
       "      <td>0.81 (0.004)</td>\n",
       "      <td>0.81 (0.004)</td>\n",
       "      <td>0.813 (0.005)</td>\n",
       "      <td>0.81 (0.006)</td>\n",
       "      <td>0.809 (0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>0.814 (0.005)</td>\n",
       "      <td>0.814 (0.005)</td>\n",
       "      <td>0.808 (0.006)</td>\n",
       "      <td>0.808 (0.004)</td>\n",
       "      <td>0.809 (0.005)</td>\n",
       "      <td>0.808 (0.006)</td>\n",
       "      <td>0.807 (0.004)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Baseline         ignore            ohe         target  \\\n",
       "LR   0.814 (0.005)  0.814 (0.005)   0.81 (0.004)   0.81 (0.004)   \n",
       "XGB  0.814 (0.005)  0.814 (0.005)  0.808 (0.006)  0.808 (0.004)   \n",
       "\n",
       "           ordinal       catboost           glmm  \n",
       "LR   0.813 (0.005)   0.81 (0.006)  0.809 (0.004)  \n",
       "XGB  0.809 (0.005)  0.808 (0.006)  0.807 (0.004)  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df_lr.columns = [i.split(\"_\")[1] if i != \"Baseline\" else \"Baseline\" for i in res_df_lr.columns]    \n",
    "res_df_xgb.columns = [i.split(\"_\")[1] if i != \"Baseline\" else \"Baseline\" for i in res_df_xgb.columns]    \n",
    "\n",
    "latex_df_encodings = pd.concat([res_df_lr,res_df_xgb],axis=0)\n",
    "latex_df_encodings.index = [\"LR\", \"XGB\"]\n",
    "latex_df_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c25d79f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "{} &       Baseline &         ignore &            ohe &         target &        ordinal &       catboost &           glmm \\\\\n",
      "\\midrule\n",
      "LR  &  0.814 (0.005) &  0.814 (0.005) &   0.81 (0.004) &   0.81 (0.004) &  0.813 (0.005) &   0.81 (0.006) &  0.809 (0.004) \\\\\n",
      "XGB &  0.814 (0.005) &  0.814 (0.005) &  0.808 (0.006) &  0.808 (0.004) &  0.809 (0.005) &  0.808 (0.006) &  0.807 (0.004) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_df_encodings.round(2).to_latex())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b8faf",
   "metadata": {},
   "source": [
    "## Data Subset Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "347f47a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = {\"demo_only\": demographic_cols,\n",
    "           \"performance_only\": perf_cols,\n",
    "           \"activity_only\": activity_cols,\n",
    "           \"activity_and_demo\": activity_cols+demographic_cols,\n",
    "           \"performance_and_demo\": perf_cols+demographic_cols,\n",
    "           \"all\": list(df.columns)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f27a0a08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing results for fold 0, subset=demo_only\n",
      "SCORE: 0.5997530764282681                                                                                              \n",
      "SCORE: 0.599757334248441                                                                                               \n",
      "SCORE: 0.5997585094362267                                                                                              \n",
      "SCORE: 0.5997575681963878                                                                                              \n",
      "SCORE: 0.5998072402564697                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00,  9.80trial/s, best loss: 0.5997530764282681]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.8723696155139968}\n",
      "Default performance on Test: 0.6131218793938061\n",
      "SCORE: 0.6040828138937523                                                                                              \n",
      "SCORE: 0.6042171226536865                                                                                              \n",
      "SCORE: 0.6034828437412691                                                                                              \n",
      "SCORE: 0.6042271132507147                                                                                              \n",
      "SCORE: 0.6039693690547934                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.42trial/s, best loss: 0.6034828437412691]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.27886085965617197, 'n_estimators': 306.0}\n",
      "Test Performance after first tuning round: 0.6282293857617798\n",
      "SCORE: 0.6040234948183087                                                                                              \n",
      "SCORE: 0.6016321154987075                                                                                              \n",
      "SCORE: 0.6035118995016614                                                                                              \n",
      "SCORE: 0.6040234948183087                                                                                              \n",
      "SCORE: 0.6106261901767979                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:04<00:00,  1.04trial/s, best loss: 0.6016321154987075]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.27886085965617197, 'n_estimators': 306.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 0.5911644277124208\n",
      "SCORE: 0.6063891614181914                                                                                              \n",
      "SCORE: 0.6076438873781713                                                                                              \n",
      "SCORE: 0.6054312343224926                                                                                              \n",
      "SCORE: 0.6017889276164372                                                                                              \n",
      "SCORE: 0.6054068097290725                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:07<00:00,  1.42s/trial, best loss: 0.6017889276164372]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.27886085965617197, 'n_estimators': 306.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.5070283052262237, 'subsample': 0.8129506138454023}\n",
      "Test Performance after third tuning round: 0.5912886873304082\n",
      "SCORE: 0.6036528957515671                                                                                              \n",
      "SCORE: 0.6032441746735012                                                                                              \n",
      "SCORE: 0.6021223356988541                                                                                              \n",
      "SCORE: 0.6039366557141788                                                                                              \n",
      "SCORE: 0.6019003515146177                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:08<00:00,  1.78s/trial, best loss: 0.6019003515146177]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.27886085965617197, 'n_estimators': 306.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.5070283052262237, 'subsample': 0.8129506138454023, 'gamma': 0.9989622735740649, 'reg_alpha': 2.0, 'reg_lambda': 1.6619215976957595}\n",
      "Test Performance after last tuning round: 0.5914485371498123\n",
      "Preparing results for fold 0, subset=performance_only\n",
      "SCORE: 0.5399150666743747                                                                                              \n",
      "SCORE: 0.5399152478226243                                                                                              \n",
      "SCORE: 0.5399152462916589                                                                                              \n",
      "SCORE: 0.5399150111203272                                                                                              \n",
      "SCORE: 0.5399152142007196                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:00<00:00, 16.77trial/s, best loss: 0.5399150111203272]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.19924356089906473}\n",
      "Default performance on Test: 0.41408507786092663\n",
      "SCORE: 0.4325793640830259                                                                                              \n",
      "SCORE: 0.4330795906932405                                                                                              \n",
      "SCORE: 0.43525847428492226                                                                                             \n",
      "SCORE: 0.4335118877069795                                                                                              \n",
      "SCORE: 0.43568672405909703                                                                                             \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:14<00:00,  2.92s/trial, best loss: 0.4325793640830259]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.07962946763929467, 'n_estimators': 290.0}\n",
      "Test Performance after first tuning round: 0.4138958981979942\n",
      "SCORE: 0.44032204894243987                                                                                             \n",
      "SCORE: 0.4522503961969616                                                                                              \n",
      "SCORE: 0.4348594471880542                                                                                              \n",
      "SCORE: 0.4613074266193298                                                                                              \n",
      "SCORE: 0.4350681827054778                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:21<00:00,  4.38s/trial, best loss: 0.4348594471880542]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.07962946763929467, 'n_estimators': 290.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 0.0}\n",
      "Test Performance after second tuning round: 0.41475998234360373\n",
      "SCORE: 0.4290982484292307                                                                                              \n",
      "SCORE: 0.6026088082237098                                                                                              \n",
      "SCORE: 0.43430355072745697                                                                                             \n",
      "SCORE: 0.42897914146194155                                                                                             \n",
      "SCORE: 0.42942615849694465                                                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 5/5 [00:24<00:00,  4.81s/trial, best loss: 0.42897914146194155]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.07962946763929467, 'n_estimators': 290.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 0.0, 'colsample_bytree': 0.8871226731983411, 'subsample': 0.881874405691565}\n",
      "Test Performance after third tuning round: 0.41191625253215597\n",
      "SCORE: 0.46393554032968537                                                                                             \n",
      "SCORE: 0.456348230475644                                                                                               \n",
      "SCORE: 0.4524504744119887                                                                                              \n",
      "SCORE: 0.4654499841343897                                                                                              \n",
      "SCORE: 0.4659927818832427                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:17<00:00,  3.57s/trial, best loss: 0.4524504744119887]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.07962946763929467, 'n_estimators': 290.0, 'seed': 0, 'max_depth': 6.0, 'min_child_weight': 0.0, 'colsample_bytree': 0.8871226731983411, 'subsample': 0.881874405691565, 'gamma': 1.4831712703783266, 'reg_alpha': 3.0, 'reg_lambda': 1.7288841163121211}\n",
      "Test Performance after last tuning round: 0.4139999481271415\n",
      "Preparing results for fold 0, subset=activity_only\n",
      "SCORE: 0.49633079703367866                                                                                             \n",
      "SCORE: 0.49632732559073134                                                                                             \n",
      "SCORE: 0.496330686306164                                                                                               \n",
      "SCORE: 0.49633003129500475                                                                                             \n",
      "SCORE: 0.4963392431928672                                                                                              \n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:00<00:00,  6.77trial/s, best loss: 0.49632732559073134]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.27241786831048764}\n",
      "Default performance on Test: 0.36707322700996037\n",
      "SCORE: 0.37183429340054086                                                                                             \n",
      "SCORE: 0.3755436440794985                                                                                              \n",
      "SCORE: 0.3710580615351019                                                                                              \n",
      "SCORE: 0.37398410782794933                                                                                             \n",
      "SCORE: 0.37090275176733273                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.19s/trial, best loss: 0.37090275176733273]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.147543254477517, 'n_estimators': 367.0}\n",
      "Test Performance after first tuning round: 0.37287387820404283\n",
      "SCORE: 0.37048178448988883                                                                                             \n",
      "SCORE: 0.38648794991776503                                                                                             \n",
      "SCORE: 0.37161218851608685                                                                                             \n",
      "SCORE: 0.3752796173366041                                                                                              \n",
      "SCORE: 0.37384654403895096                                                                                             \n",
      "100%|█████████████████████████████████████████████████| 5/5 [00:20<00:00,  4.16s/trial, best loss: 0.37048178448988883]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.147543254477517, 'n_estimators': 367.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.39827563470285215\n",
      "SCORE: 0.3721651326461076                                                                                              \n",
      "SCORE: 0.36981964372829623                                                                                             \n",
      "SCORE: 0.3691643399242548                                                                                              \n",
      "SCORE: 0.37275083546301163                                                                                             \n",
      "SCORE: 0.3713486490616656                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:18<00:00,  3.66s/trial, best loss: 0.3691643399242548]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.147543254477517, 'n_estimators': 367.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.8745635887700051, 'subsample': 0.961664099400371}\n",
      "Test Performance after third tuning round: 0.40626083276906505\n",
      "SCORE: 0.3679509256580383                                                                                              \n",
      "SCORE: 0.3660328802521042                                                                                              \n",
      "SCORE: 0.3674824624692481                                                                                              \n",
      "SCORE: 0.36920320204700224                                                                                             \n",
      "SCORE: 0.36673097029161317                                                                                             \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:30<00:00,  6.16s/trial, best loss: 0.3660328802521042]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.147543254477517, 'n_estimators': 367.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.8745635887700051, 'subsample': 0.961664099400371, 'gamma': 3.1530691279984664, 'reg_alpha': 6.0, 'reg_lambda': 1.0378787816838417}\n",
      "Test Performance after last tuning round: 0.35458448837382534\n",
      "Preparing results for fold 0, subset=activity_and_demo\n",
      "SCORE: 0.4791986282692828                                                                                              \n",
      "SCORE: 0.4791939442068224                                                                                              \n",
      "SCORE: 0.4791972047937202                                                                                              \n",
      "SCORE: 0.47920702090556055                                                                                             \n",
      "SCORE: 0.47921298724376526                                                                                             \n",
      "100%|██████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.95trial/s, best loss: 0.4791939442068224]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.34866700326596883}\n",
      "Default performance on Test: 0.3626235710452811\n",
      "SCORE: 0.3650776832418455                                                                                              \n",
      "SCORE: 0.5667370121284042                                                                                              \n",
      "SCORE: 0.36545780167111325                                                                                             \n",
      "SCORE: 0.3744426116128963                                                                                              \n",
      " 80%|████████████████████████████████████████          | 4/5 [00:21<00:05,  5.35s/trial, best loss: 0.3650776832418455]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     81\u001b[0m results_subsets[fold][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLR_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39msubset_key\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_tuned\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m     82\u001b[0m results_subsets_feature_importances[fold][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLR_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39msubset_key\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_tuned\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m feats\n\u001b[1;32m---> 84\u001b[0m res, feats \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_xgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_val_xgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_xgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m results_subsets[fold][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGB_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39msubset_key\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_tuned\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m     86\u001b[0m results_subsets_feature_importances[fold][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGB_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39msubset_key\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_tuned\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m feats\n",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36mevaluate_xgb\u001b[1;34m(X_train, y_train, X_test, y_test, target, tune, max_evals, early_stopping_rounds, seed)\u001b[0m\n\u001b[0;32m    543\u001b[0m     xgb_objective \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti:softproba\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tune:\n\u001b[1;32m--> 547\u001b[0m     final_hyperparameters \u001b[38;5;241m=\u001b[39m \u001b[43mtune_xgboost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m     xgb_model \u001b[38;5;241m=\u001b[39m xgb_model_type(\n\u001b[0;32m    549\u001b[0m         objective \u001b[38;5;241m=\u001b[39m xgb_objective,\n\u001b[0;32m    550\u001b[0m         \u001b[38;5;66;03m# eval_metric=xgb_metric,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    560\u001b[0m         reg_lambda\u001b[38;5;241m=\u001b[39mfinal_hyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_lambda\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    561\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36mtune_xgboost\u001b[1;34m(X, y, X_test, y_test, target, max_evals, early_stopping_rounds, seed)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: score, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: STATUS_OK}\n\u001b[0;32m    269\u001b[0m trials \u001b[38;5;241m=\u001b[39m Trials()\n\u001b[1;32m--> 271\u001b[0m best_hyperparams \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m                        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m final_hyperparameters \u001b[38;5;241m=\u001b[39m best_hyperparams\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe best hyperparameters after step 1  are : \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    537\u001b[0m     fn \u001b[38;5;241m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[1;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_trials_fmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -- prevent recursion\u001b[39;49;00m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[0;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[0;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[1;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[0;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36mtune_xgboost.<locals>.objective\u001b[1;34m(space)\u001b[0m\n\u001b[0;32m    245\u001b[0m model \u001b[38;5;241m=\u001b[39m xgb_model(\n\u001b[0;32m    246\u001b[0m     objective\u001b[38;5;241m=\u001b[39mxgb_objective,\n\u001b[0;32m    247\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39mxgb_metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    251\u001b[0m     seed \u001b[38;5;241m=\u001b[39m space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    252\u001b[0m )\n\u001b[0;32m    254\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m [(X_train, y_train), (X_val, y_val)]\n\u001b[1;32m--> 256\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    261\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:575\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    574\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1400\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1379\u001b[0m model, metric, params, early_stopping_rounds, callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(\n\u001b[0;32m   1380\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1381\u001b[0m )\n\u001b[0;32m   1382\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1383\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1384\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1397\u001b[0m     enable_categorical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_categorical,\n\u001b[0;32m   1398\u001b[0m )\n\u001b[1;32m-> 1400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1412\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:575\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    574\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py:1778\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_features(dtrain)\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1778\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1779\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.exists(f\"../results/{dataset_name}/{experiment_name}/results_subsets.pickle\"):\n",
    "\n",
    "    results_subsets = {}\n",
    "    results_subsets_feature_importances = {}\n",
    "\n",
    "    for fold in range(folds):\n",
    "        results_subsets[fold] = {}\n",
    "        results_subsets_feature_importances[fold] = {}\n",
    "        # Create baseline\n",
    "        y_train = data_dict[f\"y_train_{fold}\"]\n",
    "        y_val = data_dict[f\"y_val_{fold}\"]\n",
    "        y_test = data_dict[f\"y_test_{fold}\"]\n",
    "        y_train_val = np.concatenate([y_train,y_val])\n",
    "\n",
    "        max_ = np.argmax(np.unique(y_train_val,return_counts=True)[1])\n",
    "        y_train_val_pred_base = np.ones(y_train_val.shape[0])*max_\n",
    "        y_test_pred_base = np.ones(y_test.shape[0])*max_\n",
    "\n",
    "        results_encodings[fold][\"Baseline\"] = {}\n",
    "        eval_res_train = get_metrics(y_train_val, y_train_val_pred_base, target=target)\n",
    "        for metric in eval_res_train.keys():\n",
    "            results_encodings[fold][\"Baseline\"][metric + \" Train\"] = eval_res_train[metric]\n",
    "        eval_res_test = get_metrics(y_test, y_test_pred_base, target=target)\n",
    "        for metric in eval_res_test.keys():\n",
    "            results_encodings[fold][\"Baseline\"][metric + \" Test\"] = eval_res_test[metric]\n",
    "\n",
    "\n",
    "        for subset_key in subsets:\n",
    "            print(f\"Preparing results for fold {fold}, subset={subset_key}\")\n",
    "            # Retrieve data\n",
    "            z_cols = data_dict[\"z_cols\"]\n",
    "\n",
    "            X_train = data_dict[f\"X_train_{fold}\"]\n",
    "            y_train = data_dict[f\"y_train_{fold}\"]\n",
    "\n",
    "            X_val = data_dict[f\"X_val_{fold}\"]\n",
    "            y_val = data_dict[f\"y_val_{fold}\"]\n",
    "\n",
    "            X_test = data_dict[f\"X_test_{fold}\"]\n",
    "            y_test = data_dict[f\"y_test_{fold}\"]\n",
    "        \n",
    "            y_train_val = np.concatenate([y_train,y_val])\n",
    "\n",
    "            # Define data subset for LR\n",
    "            z_glmm_encoded_train = data_dict[f\"z_glmm_encoded_train_{fold}\"] \n",
    "            z_glmm_encoded_val = data_dict[f\"z_glmm_encoded_val_{fold}\"] \n",
    "            z_glmm_encoded_test = data_dict[f\"z_glmm_encoded_test_{fold}\"] \n",
    "            X_train_lr = pd.concat([X_train,z_glmm_encoded_train],axis=1)\n",
    "            X_val_lr = pd.concat([X_val,z_glmm_encoded_val],axis=1)\n",
    "            X_test_lr = pd.concat([X_test,z_glmm_encoded_test],axis=1)      \n",
    "            X_train_val_lr = pd.concat([X_train_lr,X_val_lr])\n",
    "\n",
    "            # Define data subset for XGB\n",
    "            z_ordinal_encoded_train = data_dict[f\"z_ordinal_encoded_train_{fold}\"] \n",
    "            z_ordinal_encoded_val = data_dict[f\"z_ordinal_encoded_val_{fold}\"] \n",
    "            z_ordinal_encoded_test = data_dict[f\"z_ordinal_encoded_test_{fold}\"] \n",
    "            X_train_xgb = pd.concat([X_train,z_ordinal_encoded_train],axis=1)\n",
    "            X_val_xgb = pd.concat([X_val,z_ordinal_encoded_val],axis=1)\n",
    "            X_test_xgb = pd.concat([X_test,z_ordinal_encoded_test],axis=1)\n",
    "            X_train_val_xgb = pd.concat([X_train_xgb,X_val_xgb])\n",
    "\n",
    "\n",
    "            # Define data subset for evaluation\n",
    "            X_train_val_lr = X_train_val_lr[[i for i in X_train_val_lr.columns if any([j in i for j in subsets[subset_key]])]]\n",
    "            X_test_lr = X_test_lr[[i for i in X_test_lr.columns if any([j in i for j in subsets[subset_key]])]]\n",
    "            X_train_val_xgb = X_train_val_xgb[[i for i in X_train_val_xgb.columns if any([j in i for j in subsets[subset_key]])]]\n",
    "            X_test_xgb = X_test_xgb[[i for i in X_test_xgb.columns if any([j in i for j in subsets[subset_key]])]]\n",
    "\n",
    "\n",
    "            # Train base models\n",
    "            res, feats = evaluate_logreg(X_train_val_lr, y_train_val, X_test_lr, y_test, target=target,tune=False, seed=RS)\n",
    "            results_subsets[fold][\"LR_\"+subset_key] = res\n",
    "            results_subsets_feature_importances[fold][\"LR_\"+subset_key] = feats\n",
    "\n",
    "            res, feats = evaluate_xgb(X_train_val_xgb, y_train_val, X_test_xgb, y_test, target, tune=False, max_evals=max_evals, early_stopping_rounds=early_stopping_rounds, seed=RS)\n",
    "            results_subsets[fold][\"XGB_\"+subset_key] = res\n",
    "            results_subsets_feature_importances[fold][\"XGB_\"+subset_key] = feats\n",
    "\n",
    "            # Train tuned models\n",
    "            res, feats = evaluate_logreg(X_train_val_lr, y_train_val, X_test_lr, y_test, target=target, max_evals=max_evals, tune=True, seed=RS)\n",
    "            results_subsets[fold][\"LR_\"+subset_key+\"_tuned\"] = res\n",
    "            results_subsets_feature_importances[fold][\"LR_\"+subset_key+\"_tuned\"] = feats\n",
    "\n",
    "            res, feats = evaluate_xgb(X_train_val_xgb, y_train_val, X_test_xgb, y_test, target, tune=True, max_evals=max_evals, early_stopping_rounds=early_stopping_rounds, seed=RS)\n",
    "            results_subsets[fold][\"XGB_\"+subset_key+\"_tuned\"] = res\n",
    "            results_subsets_feature_importances[fold][\"XGB_\"+subset_key+\"_tuned\"] = feats\n",
    "    \n",
    "    if not os.path.exists(f\"../results/{dataset_name}/{experiment_name}\"):\n",
    "        os.makedirs(f\"../results/{dataset_name}/{experiment_name}\")\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_subsets.pickle\", 'wb') as handle:\n",
    "        pickle.dump(results_subsets, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_subsets_feature_importances.pickle\", 'wb') as handle:\n",
    "        pickle.dump(results_subsets_feature_importances, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "else:\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_subsets.pickle\", 'rb') as handle:\n",
    "        results_subsets = pickle.load(handle)\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_subsets_feature_importances.pickle\", 'rb') as handle:\n",
    "        results_subsets_feature_importances = pickle.load(handle)\n",
    "        \n",
    "        \n",
    "results_subsets_df = pd.DataFrame(results_subsets[0]).transpose().sort_values(\"F1 Test\",ascending=False).round(4)\n",
    "results_subsets_df[[\"Accuracy Train\", \"F1 Train\", \"AUROC Train\", \"Accuracy Test\", \"F1 Test\", \"AUROC Test\"]].style.highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_subsets_df = pd.DataFrame(results_subsets[1]).transpose().sort_values(\"F1 Test\",ascending=False).round(4)\n",
    "results_subsets_df[[\"Accuracy Train\", \"F1 Train\", \"AUROC Train\", \"Accuracy Test\", \"F1 Test\", \"AUROC Test\"]].style.highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2048f8",
   "metadata": {},
   "source": [
    "### Effectiveness of Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a5ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = results_subsets[0].keys()\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "subsets_folds_df = pd.DataFrame([pd.DataFrame(results_subsets[fold_num]).loc[metric,models] for fold_num in results_subsets.keys()],index=results_subsets.keys())\n",
    "subsets_mean_df = subsets_folds_df.mean(axis=0)\n",
    "subsets_std_df = subsets_folds_df.std(axis=0)\n",
    "\n",
    "methods = sorted(list(subsets_mean_df.index))[1:]\n",
    "not_tuned = [\"Baseline\"]+methods[::2]\n",
    "tuned = [\"Baseline\"]+methods[1::2]\n",
    "\n",
    "res_df_tune_comp_mean = pd.DataFrame([subsets_mean_df.loc[not_tuned].values,subsets_mean_df.loc[tuned]],index=[\"Untuned\",\"Tuned\"],columns=not_tuned).transpose()\n",
    "res_df_tune_comp_std = pd.DataFrame([subsets_std_df.loc[not_tuned].values,subsets_std_df.loc[tuned]],index=[\"Untuned\",\"Tuned\"],columns=not_tuned).transpose()\n",
    "\n",
    "res_df_tune_comp_mean.round(2).style.highlight_max(color = 'lightgreen', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd97240",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_df = res_df_tune_comp_mean.round(2).astype(str) + \" (\" +  res_df_tune_comp_std.round(3).astype(str) + \")\"\n",
    "latex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a184e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df_tune_comp_diff = res_df_tune_comp_mean[[\"Tuned\"]]-res_df_tune_comp_mean[[\"Untuned\"]].values\n",
    "res_df_tune_comp_diff.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56800a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df_tune_comp_diff_lr = res_df_tune_comp_diff.loc[[i for i in res_df_tune_comp_diff.index if (i == \"Baseline\" or \"LR\" in i)]]\n",
    "res_df_tune_comp_diff_xgb = res_df_tune_comp_diff.loc[[i for i in res_df_tune_comp_diff.index if (i == \"Baseline\" or \"XGB\" in i)]]\n",
    "\n",
    "res_df_tune_comp_diff_lr.index = [i[3:] if i!=\"Baseline\" else \"Baseline\" for i in res_df_tune_comp_diff_lr.index]\n",
    "res_df_tune_comp_diff_xgb.index = [i[4:] if i!=\"Baseline\" else \"Baseline\" for i in res_df_tune_comp_diff_xgb.index]\n",
    "res_df_tune_comp_diff_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616822a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_df_diff = pd.concat([res_df_tune_comp_diff_lr,res_df_tune_comp_diff_xgb],axis=1)\n",
    "latex_df_diff.columns = [\"LR\", \"XGB\"]\n",
    "latex_df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latex_df_diff.round(2).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31e1eee",
   "metadata": {},
   "source": [
    "### Performance Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e710e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For LR\n",
    "models = [\"Baseline\"]+[i for i in results_subsets[0].keys() if (\"tuned\" in i and \"LR\" in i)]\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "use_df = pd.DataFrame([pd.DataFrame(results_subsets[fold_num]).loc[metric,models] for fold_num in results_subsets.keys()],index=results_subsets.keys())\n",
    "\n",
    "df_mean = pd.DataFrame((use_df).mean(axis=0).round(3).astype(str) + \" (\" + use_df.std(axis=0).round(3).astype(str) + \")\").transpose()\n",
    "model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "\n",
    "best_model = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "t_test_res = np.array([stats.ttest_rel(use_df[best_model].values, use_df[model].values)[1] for model in models]).round(3)\n",
    "t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    \n",
    "res_df_lr = pd.DataFrame([model_dict])\n",
    "\n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_res[i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "res_df_lr.style.apply(negative_bold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6528561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For XGB\n",
    "models = [\"Baseline\"]+[i for i in results_subsets[0].keys() if (\"tuned\" in i and \"XGB\" in i)]\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "use_df = pd.DataFrame([pd.DataFrame(results_subsets[fold_num]).loc[metric,models] for fold_num in results_subsets.keys()],index=results_subsets.keys())\n",
    "\n",
    "df_mean = pd.DataFrame((use_df).mean(axis=0).round(3).astype(str) + \" (\" + use_df.std(axis=0).round(3).astype(str) + \")\").transpose()\n",
    "model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "\n",
    "best_model = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "t_test_res = np.array([stats.ttest_rel(use_df[best_model].values, use_df[model].values)[1] for model in models]).round(3)\n",
    "t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    \n",
    "res_df_xgb = pd.DataFrame([model_dict])\n",
    "\n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_res[i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "res_df_xgb.style.apply(negative_bold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a23bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df_lr.columns = [i[3:-6] if i != \"Baseline\" else \"Baseline\" for i in res_df_lr.columns]    \n",
    "res_df_xgb.columns = [i[4:-6] if i != \"Baseline\" else \"Baseline\" for i in res_df_xgb.columns]    \n",
    "\n",
    "latex_df_subsets = pd.concat([res_df_lr,res_df_xgb],axis=0)\n",
    "latex_df_subsets.index = [\"LR\", \"XGB\"]\n",
    "latex_df_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e2ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latex_df_subsets.round(2).transpose().to_latex())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a622d201",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7014a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_10_importances = {}\n",
    "\n",
    "# for model in list(results_subsets_feature_importances[fold].keys()):\n",
    "#     imp_df = pd.concat([results_subsets_feature_importances[fold][model] for fold in range(folds)],axis=1)\n",
    "\n",
    "#     if \"LR\" in model:\n",
    "#         direction = imp_df.apply(lambda x: np.sign(x))\n",
    "#         imp_df = imp_df.abs()\n",
    "\n",
    "#     imp_df = imp_df/imp_df.sum(axis=0)\n",
    "\n",
    "#     mean_imp_df = imp_df.mean(axis=1)\n",
    "#     std_imp_df = imp_df.std(axis=1)\n",
    "\n",
    "#     mean_imp_df = mean_imp_df.sort_values(ascending=False)\n",
    "#     std_imp_df = std_imp_df.loc[mean_imp_df.index]\n",
    "#     final_imps = mean_imp_df[:10]\n",
    "#     final_imps[\"Rest\"] = sum(mean_imp_df[10:])\n",
    "#     top_5_importances[model] = np.array([final_imps.index.values, final_imps.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_importances = {}\n",
    "demo_importances_stds = {}\n",
    "\n",
    "for model in list(results_subsets_feature_importances[fold].keys()):\n",
    "    if \"demo\" in model or \"all\" in model:\n",
    "        imp_df_all = pd.concat([results_subsets_feature_importances[fold][model] for fold in range(folds)],axis=1)\n",
    "        \n",
    "        if \"LR\" in model:\n",
    "            direction = imp_df_all.apply(lambda x: np.sign(x))\n",
    "            imp_df_all = imp_df_all.abs()\n",
    "        if imp_df_all.sum().sum()!=0:\n",
    "            imp_df = imp_df_all/imp_df_all.sum(axis=0)\n",
    "        imp_df = imp_df.fillna(1/imp_df.shape[0])\n",
    "#         imp_df = imp_df.loc[demographic_cols]\n",
    "\n",
    "#         mean_imp_df = imp_df.mean(axis=1)\n",
    "#         std_imp_df = imp_df.std(axis=1)\n",
    "\n",
    "#         mean_imp_df = mean_imp_df.sort_values(ascending=False)\n",
    "#         std_imp_df = std_imp_df.loc[mean_imp_df.index]\n",
    "#         final_imps = mean_imp_df#[:10]\n",
    "#         final_imps[\"Rest\"] = sum(mean_imp_df[10:])\n",
    "#         final_imps[\"Total\"] = sum(mean_imp_df)\n",
    "        demo_importances[model] = np.round(np.mean(imp_df.loc[[i for i in imp_df.index if any([j in i for j in demographic_cols])]].sum(axis=0)),2)#final_imps.values\n",
    "        demo_importances_stds[model] = np.round(np.std(imp_df.loc[[i for i in imp_df.index if any([j in i for j in demographic_cols])]].sum(axis=0)),2)#final_imps.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b06d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_demo_imp = pd.Series({i: demo_importances[i] for i in demo_importances if \"LR\" in i and \"tuned\" in i})\n",
    "xgb_demo_imp = pd.Series({i: demo_importances[i] for i in demo_importances if \"XGB\" in i and \"tuned\" in i})\n",
    "lr_demo_imp.index = [i[3:-6] for i in lr_demo_imp.index]    \n",
    "xgb_demo_imp.index = [i[4:-6] for i in xgb_demo_imp.index]    \n",
    "\n",
    "lr_demo_imp_stds = pd.Series({i: demo_importances_stds[i] for i in demo_importances_stds if \"LR\" in i and \"tuned\" in i})\n",
    "xgb_demo_imp_stds = pd.Series({i: demo_importances_stds[i] for i in demo_importances_stds if \"XGB\" in i and \"tuned\" in i})\n",
    "lr_demo_imp_stds.index = [i[3:-6] for i in lr_demo_imp_stds.index]    \n",
    "xgb_demo_imp_stds.index = [i[4:-6] for i in xgb_demo_imp_stds.index]    \n",
    "\n",
    "\n",
    "latex_df_imp = pd.DataFrame([lr_demo_imp.astype(str) + \" (\" + lr_demo_imp_stds.astype(str) + \")\",\n",
    "                             xgb_demo_imp.astype(str) + \" (\" + xgb_demo_imp_stds.astype(str) + \")\"])\n",
    "latex_df_imp.index = [\"LR\", \"XGB\"]\n",
    "latex_df_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218900e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latex_df_subsets.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ce6f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d777e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713dc552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc07112e7ae1e8e28a0232207620ff002934c05692de8df42430404c766a0a8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
