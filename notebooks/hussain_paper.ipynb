{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955f54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils.evaluation import *\n",
    "from utils.utils import *\n",
    "\n",
    "from data import dataset_preprocessing\n",
    "\n",
    "from utils.evaluation import get_metrics\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6dd2461",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"hussain\"\n",
    "mode=\"cv\"\n",
    "RS=1\n",
    "hct=10\n",
    "test_ratio=0.2\n",
    "val_ratio=0.1\n",
    "folds=5\n",
    "target = \"categorical\"\n",
    "experiment_name = \"5CV_paper_final\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae51df",
   "metadata": {},
   "source": [
    "### Describe raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21567c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arff # make sure to pip install liac-arff\n",
    "\n",
    "\n",
    "\n",
    "dataset = arff.load(open(f\"../data/raw/{dataset_name}/Sapfile1.arff\", 'rt'))\n",
    "df = pd.DataFrame(dataset['data'], columns=[i[0] for i in dataset[\"attributes\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baeaba73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_col = \"esp\"\n",
    "demographic_cols = [\"as\", \"cst\", \"fmi\", \"fo\", 'fq', 'fs', 'ge', 'ls', 'me', 'mo', 'mq', 'ms', \"ss\", \"tt\"]\n",
    "perf_cols = [\"tnp\", \"twp\", \"iap\"]\n",
    "activity_cols = [\"arr\", \"sh\", \"atd\"]\n",
    "other_cols = ['nf'] # no. of friends\n",
    "set(df.columns)-set([y_col]+demographic_cols+perf_cols+activity_cols+other_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "529261c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No. of samples</th>\n",
       "      <th>No. of features</th>\n",
       "      <th>Performance features</th>\n",
       "      <th>Demographic features</th>\n",
       "      <th>Activity features</th>\n",
       "      <th>Other features</th>\n",
       "      <th>Categorical features</th>\n",
       "      <th>Total cardinality</th>\n",
       "      <th>% NA</th>\n",
       "      <th>Target $\\textbf{y} \\in$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cortez</th>\n",
       "      <td>131</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1..4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        No. of samples  No. of features  Performance features  \\\n",
       "cortez             131               22                     3   \n",
       "\n",
       "        Demographic features  Activity features  Other features  \\\n",
       "cortez                    14                  3               1   \n",
       "\n",
       "        Categorical features  Total cardinality  % NA Target $\\textbf{y} \\in$  \n",
       "cortez                    16                 67   0.0                  [1..4]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_df_dict = {\"No. of samples\": df.shape[0],\n",
    "           \"No. of features\": df.shape[1],\n",
    "           \"Performance features\": len(perf_cols),\n",
    "           \"Demographic features\": len(demographic_cols),\n",
    "           \"Activity features\": len(activity_cols),\n",
    "           \"Other features\": len(other_cols),\n",
    "           \"Categorical features\": len(df.columns[list(np.logical_and(df.nunique() > 2, df.dtypes == \"object\"))]),     \n",
    "           \"Total cardinality\": df[df.columns[list(np.logical_and(df.nunique() > 2, df.dtypes == \"object\"))]].nunique().sum(),     \n",
    "           \"% NA\": df.isna().sum().sum()/sum(df.shape),\n",
    "           \"Target $\\textbf{y} \\in$\": f\"[1..{df[y_col].nunique()}]\",\n",
    "#            \"High cardinality levels\":  list(df.loc[:,list(df.columns[list(np.logical_and(df.nunique() >= 10, df.dtypes == \"object\"))])].nunique().sort_values().values),\n",
    "          \n",
    "}\n",
    "desc_df = pd.DataFrame([desc_df_dict],index=[\"cortez\"])\n",
    "desc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f8598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0e1463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{ll}\n",
      "\\toprule\n",
      "{} &  cortez \\\\\n",
      "\\midrule\n",
      "No. of samples          &     131 \\\\\n",
      "No. of features         &      22 \\\\\n",
      "Performance features    &       3 \\\\\n",
      "Demographic features    &      14 \\\\\n",
      "Activity features       &       3 \\\\\n",
      "Other features          &       1 \\\\\n",
      "Categorical features    &      16 \\\\\n",
      "Total cardinality       &      67 \\\\\n",
      "\\% NA                    &     0.0 \\\\\n",
      "Target \\$\\textbackslash textbf\\{y\\} \\textbackslash in\\$ &  [1..4] \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(desc_df.transpose().to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6390a689",
   "metadata": {},
   "source": [
    "### Preprocessing and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5cc6a91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = f\"{mode}_RS{RS}_hct{hct}\"\n",
    "if mode == \"cv\":\n",
    "    data_path += f\"_{folds}folds\"\n",
    "elif mode == \"train_test\":\n",
    "    data_path += f\"_split{1-test_ratio*100}-{test_ratio*100}\"\n",
    "elif mode == \"train_val_test\":\n",
    "    data_path += f\"_split{round(100-(test_ratio+val_ratio)*100)}-{round(test_ratio*100)}-{round(val_ratio*100)}\"\n",
    "\n",
    "\n",
    "# If no data_dict for the configuration exists, run preprocessing, else load data_dict\n",
    "if not os.path.exists(f\"../data/prepared/{dataset_name}/\"+data_path+\"/data_dict.pickle\"):\n",
    "    dataset_preprocessing.process_dataset(dataset_name, target, mode, RS, hct, test_ratio, val_ratio, folds)\n",
    "with open(f\"../data/prepared/{dataset_name}/{data_path}/data_dict.pickle\", 'rb') as handle:\n",
    "        data_dict = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4a0d47",
   "metadata": {},
   "source": [
    "## Evaluation of categorical data treatment methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db4f3c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\"ignore\", \"ohe\", \"target\", \"ordinal\", \"catboost\", \"glmm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b7cda26",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_rounds = 10\n",
    "max_evals = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a9a3929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing results for fold 0, condition=ignore\n",
      "SCORE: 1.1886530664996733                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 35.71trial/s, best loss: 1.1886530664996733]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.055522167024990354}\n",
      "Default performance on Test: 1.3305249133874055\n",
      "SCORE: 1.241719925056295                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.79trial/s, best loss: 1.241719925056295]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4426839592573333, 'n_estimators': 305.0}\n",
      "Test Performance after first tuning round: 1.4913895531475576\n",
      "SCORE: 1.2140397339616196                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.62trial/s, best loss: 1.2140397339616196]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4426839592573333, 'n_estimators': 305.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 9.0}\n",
      "Test Performance after second tuning round: 1.1920911198695563\n",
      "SCORE: 1.2665325319047327                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.38trial/s, best loss: 1.2665325319047327]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4426839592573333, 'n_estimators': 305.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.9683885280480586, 'subsample': 0.5545316601687593}\n",
      "Test Performance after third tuning round: 1.147259619318665\n",
      "SCORE: 1.3862943611198906                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.93trial/s, best loss: 1.3862943611198906]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4426839592573333, 'n_estimators': 305.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.9683885280480586, 'subsample': 0.5545316601687593, 'gamma': 0.03487651513612272, 'reg_alpha': 10.0, 'reg_lambda': 2.4301889328869897}\n",
      "Test Performance after last tuning round: 1.3505709449344563\n",
      "Preparing results for fold 0, condition=ohe\n",
      "SCORE: 1.0640754381575506                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.99trial/s, best loss: 1.0640754381575506]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.022351374250553845}\n",
      "Default performance on Test: 1.1519449958763266\n",
      "SCORE: 1.2679715178019104                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.00trial/s, best loss: 1.2679715178019104]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.055302944256484604, 'n_estimators': 360.0}\n",
      "Test Performance after first tuning round: 1.1197141885310107\n",
      "SCORE: 1.2766644455397635                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.17trial/s, best loss: 1.2766644455397635]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.055302944256484604, 'n_estimators': 360.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.9234571616835064\n",
      "SCORE: 1.2885398094002318                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.48trial/s, best loss: 1.2885398094002318]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.055302944256484604, 'n_estimators': 360.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.5900986460810831, 'subsample': 0.9612134067320115}\n",
      "Test Performance after third tuning round: 0.8606698589699427\n",
      "SCORE: 1.3741040089006211                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.35trial/s, best loss: 1.3741040089006211]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.055302944256484604, 'n_estimators': 360.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.5900986460810831, 'subsample': 0.9612134067320115, 'gamma': 3.924173926274639, 'reg_alpha': 5.0, 'reg_lambda': 3.784632164724912}\n",
      "Test Performance after last tuning round: 1.2553935916696917\n",
      "Preparing results for fold 0, condition=target\n",
      "SCORE: 0.8428319871432641                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.77trial/s, best loss: 0.8428319871432641]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.3133678329453152}\n",
      "Default performance on Test: 1.0744926434005515\n",
      "SCORE: 0.9880169839470723                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.11trial/s, best loss: 0.9880169839470723]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3781140051478571, 'n_estimators': 106.0}\n",
      "Test Performance after first tuning round: 1.1165109147012229\n",
      "SCORE: 0.952447807637537                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.44trial/s, best loss: 0.952447807637537]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3781140051478571, 'n_estimators': 106.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 9.0}\n",
      "Test Performance after second tuning round: 1.2005461105355877\n",
      "SCORE: 1.0364956838254449                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.02trial/s, best loss: 1.0364956838254449]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3781140051478571, 'n_estimators': 106.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.9193329954009855, 'subsample': 0.6742874155366566}\n",
      "Test Performance after third tuning round: 1.0426837442843202\n",
      "SCORE: 1.1408038867575176                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.17trial/s, best loss: 1.1408038867575176]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3781140051478571, 'n_estimators': 106.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.9193329954009855, 'subsample': 0.6742874155366566, 'gamma': 3.5067234833594343, 'reg_alpha': 0.0, 'reg_lambda': 3.037587052206961}\n",
      "Test Performance after last tuning round: 1.063167970710101\n",
      "Preparing results for fold 0, condition=ordinal\n",
      "SCORE: 1.0969992124343606                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.52trial/s, best loss: 1.0969992124343606]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.39968173799129075}\n",
      "Default performance on Test: 1.018951833051307\n",
      "SCORE: 1.159712345183762                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.25trial/s, best loss: 1.159712345183762]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.18202223522796737, 'n_estimators': 215.0}\n",
      "Test Performance after first tuning round: 1.0274772577077835\n",
      "SCORE: 1.0725422225737646                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.61trial/s, best loss: 1.0725422225737646]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.18202223522796737, 'n_estimators': 215.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 0.8983303665905547\n",
      "SCORE: 1.0326391664919343                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.93trial/s, best loss: 1.0326391664919343]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.18202223522796737, 'n_estimators': 215.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.6422528723260953, 'subsample': 0.6313668683972973}\n",
      "Test Performance after third tuning round: 0.9647630779605811\n",
      "SCORE: 1.2499736218924864                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.72trial/s, best loss: 1.2499736218924864]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.18202223522796737, 'n_estimators': 215.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.6422528723260953, 'subsample': 0.6313668683972973, 'gamma': 2.325141018923045, 'reg_alpha': 1.0, 'reg_lambda': 1.9978758979427}\n",
      "Test Performance after last tuning round: 1.0711379668188932\n",
      "Preparing results for fold 0, condition=catboost\n",
      "SCORE: 0.9930391706531584                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.37trial/s, best loss: 0.9930391706531584]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.9803306004751714}\n",
      "Default performance on Test: 1.4616571648183099\n",
      "SCORE: 1.2256959178931361                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.38trial/s, best loss: 1.2256959178931361]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.17589451837777645, 'n_estimators': 331.0}\n",
      "Test Performance after first tuning round: 1.4326094300275973\n",
      "SCORE: 1.1554990618941132                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.18trial/s, best loss: 1.1554990618941132]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.17589451837777645, 'n_estimators': 331.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 1.4131399444822617\n",
      "SCORE: 1.0861272015742565                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.23trial/s, best loss: 1.0861272015742565]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.17589451837777645, 'n_estimators': 331.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.546186334691118, 'subsample': 0.6513045076089409}\n",
      "Test Performance after third tuning round: 1.3586183404197039\n",
      "SCORE: 1.2717860320234564                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.43trial/s, best loss: 1.2717860320234564]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.17589451837777645, 'n_estimators': 331.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.546186334691118, 'subsample': 0.6513045076089409, 'gamma': 6.760408471319129, 'reg_alpha': 0.0, 'reg_lambda': 3.7860132652078065}\n",
      "Test Performance after last tuning round: 1.2392854012989483\n",
      "Preparing results for fold 0, condition=glmm\n",
      "SCORE: 0.9098619396935362                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 23.84trial/s, best loss: 0.9098619396935362]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.08521803207973946}\n",
      "Default performance on Test: 1.2898851684025845\n",
      "SCORE: 0.9139848011368954                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.44trial/s, best loss: 0.9139848011368954]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.2847975940128856, 'n_estimators': 123.0}\n",
      "Test Performance after first tuning round: 1.2856616090412312\n",
      "SCORE: 1.0730529560435438                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.78trial/s, best loss: 1.0730529560435438]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.2847975940128856, 'n_estimators': 123.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 1.1491390386551632\n",
      "SCORE: 1.0392245556506605                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.20trial/s, best loss: 1.0392245556506605]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.2847975940128856, 'n_estimators': 123.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.6031064048532631, 'subsample': 0.8347143137931372}\n",
      "Test Performance after third tuning round: 1.254624456584055\n",
      "SCORE: 1.2746213070588042                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.92trial/s, best loss: 1.2746213070588042]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.2847975940128856, 'n_estimators': 123.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.6031064048532631, 'subsample': 0.8347143137931372, 'gamma': 7.6425663119494684, 'reg_alpha': 3.0, 'reg_lambda': 1.4456328072661286}\n",
      "Test Performance after last tuning round: 1.1879792139865828\n",
      "Preparing results for fold 1, condition=ignore\n",
      "SCORE: 1.0839436258010209                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.97trial/s, best loss: 1.0839436258010209]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.6109189433574256}\n",
      "Default performance on Test: 2.3308125312240984\n",
      "SCORE: 1.3219945179743424                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.67trial/s, best loss: 1.3219945179743424]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.0227307642018375, 'n_estimators': 419.0}\n",
      "Test Performance after first tuning round: 2.064821379914642\n",
      "SCORE: 1.3509422031060605                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.41trial/s, best loss: 1.3509422031060605]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.0227307642018375, 'n_estimators': 419.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 4.0}\n",
      "Test Performance after second tuning round: 1.8027406518746396\n",
      "SCORE: 1.3227788677682994                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.10trial/s, best loss: 1.3227788677682994]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.0227307642018375, 'n_estimators': 419.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.8957691336420902, 'subsample': 0.9623777384321047}\n",
      "Test Performance after third tuning round: 1.7457152931705382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 1.3845897076865545                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.12trial/s, best loss: 1.3845897076865545]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.0227307642018375, 'n_estimators': 419.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.8957691336420902, 'subsample': 0.9623777384321047, 'gamma': 3.393384843217633, 'reg_alpha': 8.0, 'reg_lambda': 3.779063190549759}\n",
      "Test Performance after last tuning round: 1.3210486163206236\n",
      "Preparing results for fold 1, condition=ohe\n",
      "SCORE: 0.8691431798733763                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 16.98trial/s, best loss: 0.8691431798733763]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.2730547940486917}\n",
      "Default performance on Test: 1.5188810029269446\n",
      "SCORE: 0.9600877401785011                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.92trial/s, best loss: 0.9600877401785011]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4601254329913081, 'n_estimators': 296.0}\n",
      "Test Performance after first tuning round: 1.720620522931481\n",
      "SCORE: 0.963408637989415                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.65trial/s, best loss: 0.963408637989415]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4601254329913081, 'n_estimators': 296.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 1.4757817941750906\n",
      "SCORE: 0.9814556443178134                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.77trial/s, best loss: 0.9814556443178134]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4601254329913081, 'n_estimators': 296.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.693573347335269, 'subsample': 0.7709932978352478}\n",
      "Test Performance after third tuning round: 1.4263501586797764\n",
      "SCORE: 1.3249980873181135                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.52trial/s, best loss: 1.3249980873181135]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4601254329913081, 'n_estimators': 296.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.693573347335269, 'subsample': 0.7709932978352478, 'gamma': 2.8902787641115455, 'reg_alpha': 6.0, 'reg_lambda': 1.3387888084334065}\n",
      "Test Performance after last tuning round: 1.3228792355698535\n",
      "Preparing results for fold 1, condition=target\n",
      "SCORE: 0.8279524645129651                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.30trial/s, best loss: 0.8279524645129651]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.8628023333153872}\n",
      "Default performance on Test: 1.4663574186781079\n",
      "SCORE: 0.9992378953900957                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.40trial/s, best loss: 0.9992378953900957]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3142404549695648, 'n_estimators': 410.0}\n",
      "Test Performance after first tuning round: 1.7955036944499616\n",
      "SCORE: 0.9749161337505502                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.89trial/s, best loss: 0.9749161337505502]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3142404549695648, 'n_estimators': 410.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 2.166359560642589\n",
      "SCORE: 0.9248249437049288                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.30trial/s, best loss: 0.9248249437049288]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3142404549695648, 'n_estimators': 410.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.5463907092968734, 'subsample': 0.7989780254263892}\n",
      "Test Performance after third tuning round: 2.317217431027056\n",
      "SCORE: 1.2427448475062404                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.40trial/s, best loss: 1.2427448475062404]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3142404549695648, 'n_estimators': 410.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.5463907092968734, 'subsample': 0.7989780254263892, 'gamma': 8.735407583265184, 'reg_alpha': 1.0, 'reg_lambda': 1.9867134821589136}\n",
      "Test Performance after last tuning round: 1.3304451284307708\n",
      "Preparing results for fold 1, condition=ordinal\n",
      "SCORE: 1.163644986762419                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.95trial/s, best loss: 1.163644986762419]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.560475839971763}\n",
      "Default performance on Test: 1.1836923411197924\n",
      "SCORE: 1.0731405220209138                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.11trial/s, best loss: 1.0731405220209138]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.09890964037060514, 'n_estimators': 61.0}\n",
      "Test Performance after first tuning round: 0.839800674635018\n",
      "SCORE: 1.0986312700598428                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.27trial/s, best loss: 1.0986312700598428]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.09890964037060514, 'n_estimators': 61.0, 'seed': 0, 'max_depth': 12.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 1.1722638017222553\n",
      "SCORE: 1.1955108526632383                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.51trial/s, best loss: 1.1955108526632383]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.09890964037060514, 'n_estimators': 61.0, 'seed': 0, 'max_depth': 12.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.6112202640571172, 'subsample': 0.8188076175782868}\n",
      "Test Performance after third tuning round: 1.10721630658472\n",
      "SCORE: 1.3685431173615086                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.19trial/s, best loss: 1.3685431173615086]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.09890964037060514, 'n_estimators': 61.0, 'seed': 0, 'max_depth': 12.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.6112202640571172, 'subsample': 0.8188076175782868, 'gamma': 5.4985553698061835, 'reg_alpha': 1.0, 'reg_lambda': 3.7919564702059088}\n",
      "Test Performance after last tuning round: 1.3575231884231629\n",
      "Preparing results for fold 1, condition=catboost\n",
      "SCORE: 0.9473142375572501                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.55trial/s, best loss: 0.9473142375572501]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.4958135281949422}\n",
      "Default performance on Test: 1.667103247859645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 1.192321348152122                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.83trial/s, best loss: 1.192321348152122]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.27188907939550405, 'n_estimators': 149.0}\n",
      "Test Performance after first tuning round: 1.8192365153066818\n",
      "SCORE: 1.168705431194378                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.15trial/s, best loss: 1.168705431194378]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.27188907939550405, 'n_estimators': 149.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 1.6651379000338575\n",
      "SCORE: 1.1503592368871889                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.90trial/s, best loss: 1.1503592368871889]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.27188907939550405, 'n_estimators': 149.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.92653129590064, 'subsample': 0.5244520145345271}\n",
      "Test Performance after third tuning round: 1.601649503202386\n",
      "SCORE: 1.3763431083350317                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.63trial/s, best loss: 1.3763431083350317]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.27188907939550405, 'n_estimators': 149.0, 'seed': 0, 'max_depth': 7.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.92653129590064, 'subsample': 0.5244520145345271, 'gamma': 4.861141388853464, 'reg_alpha': 7.0, 'reg_lambda': 3.17598041971942}\n",
      "Test Performance after last tuning round: 1.3361416601696194\n",
      "Preparing results for fold 1, condition=glmm\n",
      "SCORE: 0.8761392209080434                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.16trial/s, best loss: 0.8761392209080434]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.7388234059941943}\n",
      "Default performance on Test: 1.0709252946101477\n",
      "SCORE: 1.038003741703181                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.04trial/s, best loss: 1.038003741703181]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.1584388463714125, 'n_estimators': 325.0}\n",
      "Test Performance after first tuning round: 1.314713365120544\n",
      "SCORE: 1.0312937048140842                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.67trial/s, best loss: 1.0312937048140842]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.1584388463714125, 'n_estimators': 325.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 1.406514573139436\n",
      "SCORE: 1.0716503391844199                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.39trial/s, best loss: 1.0716503391844199]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.1584388463714125, 'n_estimators': 325.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.7183035426841013, 'subsample': 0.6212479277249219}\n",
      "Test Performance after third tuning round: 1.296758117525312\n",
      "SCORE: 1.3837845557802413                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.40trial/s, best loss: 1.3837845557802413]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.1584388463714125, 'n_estimators': 325.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.7183035426841013, 'subsample': 0.6212479277249219, 'gamma': 3.892946133032369, 'reg_alpha': 10.0, 'reg_lambda': 1.4071710664184027}\n",
      "Test Performance after last tuning round: 1.3324564373494838\n",
      "Preparing results for fold 2, condition=ignore\n",
      "SCORE: 1.2091949579012236                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 32.25trial/s, best loss: 1.2091949579012236]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.5728101865995395}\n",
      "Default performance on Test: 0.9140206043733328\n",
      "SCORE: 1.3001278213373362                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.27trial/s, best loss: 1.3001278213373362]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.2540950716911179, 'n_estimators': 178.0}\n",
      "Test Performance after first tuning round: 0.9115293697597342\n",
      "SCORE: 1.265286966929359                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.99trial/s, best loss: 1.265286966929359]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.2540950716911179, 'n_estimators': 178.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 0.9422685179540592\n",
      "SCORE: 1.2202051606177764                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.73trial/s, best loss: 1.2202051606177764]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.2540950716911179, 'n_estimators': 178.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.97655589921364, 'subsample': 0.6862361587215207}\n",
      "Test Performance after third tuning round: 0.9786433681427269\n",
      "SCORE: 1.3833183572269518                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.17trial/s, best loss: 1.3833183572269518]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.2540950716911179, 'n_estimators': 178.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.97655589921364, 'subsample': 0.6862361587215207, 'gamma': 7.834196269849576, 'reg_alpha': 9.0, 'reg_lambda': 3.980833795327884}\n",
      "Test Performance after last tuning round: 1.2617865495770024\n",
      "Preparing results for fold 2, condition=ohe\n",
      "SCORE: 0.8929818072210736                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 20.83trial/s, best loss: 0.8929818072210736]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.1480222121029931}\n",
      "Default performance on Test: 1.1436047016272266\n",
      "SCORE: 1.2908266737148537                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.63trial/s, best loss: 1.2908266737148537]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.031025240948914685, 'n_estimators': 88.0}\n",
      "Test Performance after first tuning round: 0.9845456118638238\n",
      "SCORE: 1.2060427983825541                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.81trial/s, best loss: 1.2060427983825541]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.031025240948914685, 'n_estimators': 88.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 2.0}\n",
      "Test Performance after second tuning round: 0.9490510106652982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 1.3219954580091116                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.54trial/s, best loss: 1.3219954580091116]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.031025240948914685, 'n_estimators': 88.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.7038561526341564, 'subsample': 0.5966973006706313}\n",
      "Test Performance after third tuning round: 0.8994728453127354\n",
      "SCORE: 1.3748060828905073                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.65trial/s, best loss: 1.3748060828905073]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.031025240948914685, 'n_estimators': 88.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.7038561526341564, 'subsample': 0.5966973006706313, 'gamma': 0.7948217546385531, 'reg_alpha': 6.0, 'reg_lambda': 1.8184368580797368}\n",
      "Test Performance after last tuning round: 1.2231449563640084\n",
      "Preparing results for fold 2, condition=target\n",
      "SCORE: 0.7593685224141378                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.04trial/s, best loss: 0.7593685224141378]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.8157576884685557}\n",
      "Default performance on Test: 1.167819227116627\n",
      "SCORE: 1.063538387456612                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.26trial/s, best loss: 1.063538387456612]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.44630448229465125, 'n_estimators': 221.0}\n",
      "Test Performance after first tuning round: 1.226572727878608\n",
      "SCORE: 0.997119084212029                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.25trial/s, best loss: 0.997119084212029]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.44630448229465125, 'n_estimators': 221.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 1.19193900516827\n",
      "SCORE: 1.056705267678709                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.59trial/s, best loss: 1.056705267678709]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.44630448229465125, 'n_estimators': 221.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.5355582174749755, 'subsample': 0.8383492197405829}\n",
      "Test Performance after third tuning round: 1.2208058305557095\n",
      "SCORE: 1.3608983444179847                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.30trial/s, best loss: 1.3608983444179847]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.44630448229465125, 'n_estimators': 221.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.5355582174749755, 'subsample': 0.8383492197405829, 'gamma': 8.031933974450562, 'reg_alpha': 5.0, 'reg_lambda': 3.0710283042532454}\n",
      "Test Performance after last tuning round: 1.1674538971727426\n",
      "Preparing results for fold 2, condition=ordinal\n",
      "SCORE: 0.9743441056316818                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.33trial/s, best loss: 0.9743441056316818]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.3315564488941897}\n",
      "Default performance on Test: 1.2426837661090624\n",
      "SCORE: 1.0609725445496114                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.54trial/s, best loss: 1.0609725445496114]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.49531060572318886, 'n_estimators': 223.0}\n",
      "Test Performance after first tuning round: 1.4997239300910967\n",
      "SCORE: 0.8273307334463855                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.13trial/s, best loss: 0.8273307334463855]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.49531060572318886, 'n_estimators': 223.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 1.4439233713708879\n",
      "SCORE: 0.923201879037963                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.96trial/s, best loss: 0.923201879037963]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.49531060572318886, 'n_estimators': 223.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.6381224037902855, 'subsample': 0.5376617308360028}\n",
      "Test Performance after third tuning round: 1.5040243982223211\n",
      "SCORE: 1.3298737871667963                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.38trial/s, best loss: 1.3298737871667963]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.49531060572318886, 'n_estimators': 223.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.6381224037902855, 'subsample': 0.5376617308360028, 'gamma': 6.603497383094435, 'reg_alpha': 1.0, 'reg_lambda': 2.9595055914793438}\n",
      "Test Performance after last tuning round: 1.047468371778432\n",
      "Preparing results for fold 2, condition=catboost\n",
      "SCORE: 0.9571828242441647                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.85trial/s, best loss: 0.9571828242441647]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.7517573271544806}\n",
      "Default performance on Test: 2.1618883318054802\n",
      "SCORE: 1.180435293982354                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.91trial/s, best loss: 1.180435293982354]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.2676504808094834, 'n_estimators': 249.0}\n",
      "Test Performance after first tuning round: 2.260219103002913\n",
      "SCORE: 1.12170985905543                                                                                                \n",
      "100%|████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.81trial/s, best loss: 1.12170985905543]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.2676504808094834, 'n_estimators': 249.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 4.0}\n",
      "Test Performance after second tuning round: 2.007190360287791\n",
      "SCORE: 1.1056211291912779                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.02trial/s, best loss: 1.1056211291912779]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.2676504808094834, 'n_estimators': 249.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.6160335376264301, 'subsample': 0.6495834820204212}\n",
      "Test Performance after third tuning round: 1.5186387521023659\n",
      "SCORE: 1.3764610874762266                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.46trial/s, best loss: 1.3764610874762266]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.2676504808094834, 'n_estimators': 249.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.6160335376264301, 'subsample': 0.6495834820204212, 'gamma': 4.162292853400006, 'reg_alpha': 6.0, 'reg_lambda': 3.717681345777147}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance after last tuning round: 1.1750486921420964\n",
      "Preparing results for fold 2, condition=glmm\n",
      "SCORE: 0.8350927580725195                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 20.40trial/s, best loss: 0.8350927580725195]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.33365650399746993}\n",
      "Default performance on Test: 1.1812612223438603\n",
      "SCORE: 1.1811235584298394                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.86trial/s, best loss: 1.1811235584298394]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.2514609293272148, 'n_estimators': 389.0}\n",
      "Test Performance after first tuning round: 1.2277540735586772\n",
      "SCORE: 1.1202814750873187                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.58trial/s, best loss: 1.1202814750873187]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.2514609293272148, 'n_estimators': 389.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 1.2000213225918523\n",
      "SCORE: 1.0171743173515524                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.02trial/s, best loss: 1.0171743173515524]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.2514609293272148, 'n_estimators': 389.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.7804016274725403, 'subsample': 0.6798089614648128}\n",
      "Test Performance after third tuning round: 1.3375813186005212\n",
      "SCORE: 1.1958384889618907                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.24trial/s, best loss: 1.1958384889618907]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.2514609293272148, 'n_estimators': 389.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.7804016274725403, 'subsample': 0.6798089614648128, 'gamma': 4.885915215555521, 'reg_alpha': 0.0, 'reg_lambda': 3.8073441172733666}\n",
      "Test Performance after last tuning round: 0.915263123888573\n",
      "Preparing results for fold 3, condition=ignore\n",
      "SCORE: 1.195543891332834                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00, 29.41trial/s, best loss: 1.195543891332834]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.5530701056472554}\n",
      "Default performance on Test: 1.4280780799491015\n",
      "SCORE: 1.2595593042844242                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.54trial/s, best loss: 1.2595593042844242]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3183481672626029, 'n_estimators': 377.0}\n",
      "Test Performance after first tuning round: 1.7947027238352986\n",
      "SCORE: 1.2595593042844242                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.71trial/s, best loss: 1.2595593042844242]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3183481672626029, 'n_estimators': 377.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 1.7947027238352986\n",
      "SCORE: 1.2081589415250185                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.92trial/s, best loss: 1.2081589415250185]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3183481672626029, 'n_estimators': 377.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.7932957175163708, 'subsample': 0.9617977015618823}\n",
      "Test Performance after third tuning round: 1.582369972975093\n",
      "SCORE: 1.3745071355236687                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.77trial/s, best loss: 1.3745071355236687]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3183481672626029, 'n_estimators': 377.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.7932957175163708, 'subsample': 0.9617977015618823, 'gamma': 5.7268262049775736, 'reg_alpha': 10.0, 'reg_lambda': 3.2754586748673797}\n",
      "Test Performance after last tuning round: 1.2821439641525894\n",
      "Preparing results for fold 3, condition=ohe\n",
      "SCORE: 0.9489393493973793                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.89trial/s, best loss: 0.9489393493973793]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.6958316615462429}\n",
      "Default performance on Test: 1.7601546411417306\n",
      "SCORE: 0.9303331299671933                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.00trial/s, best loss: 0.9303331299671933]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4420498904321236, 'n_estimators': 447.0}\n",
      "Test Performance after first tuning round: 1.7660137692759383\n",
      "SCORE: 1.0276566487377778                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.66trial/s, best loss: 1.0276566487377778]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4420498904321236, 'n_estimators': 447.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 1.3348710025684174\n",
      "SCORE: 0.9421342403351491                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.10trial/s, best loss: 0.9421342403351491]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4420498904321236, 'n_estimators': 447.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.8238241785539336, 'subsample': 0.526186691343287}\n",
      "Test Performance after third tuning round: 1.1728221953131492\n",
      "SCORE: 1.3842546168776568                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.49trial/s, best loss: 1.3842546168776568]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4420498904321236, 'n_estimators': 447.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.8238241785539336, 'subsample': 0.526186691343287, 'gamma': 0.8848897174205138, 'reg_alpha': 9.0, 'reg_lambda': 3.409845071739279}\n",
      "Test Performance after last tuning round: 1.2601476112808878\n",
      "Preparing results for fold 3, condition=target\n",
      "SCORE: 0.9037351991608361                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.01trial/s, best loss: 0.9037351991608361]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.21295535058378154}\n",
      "Default performance on Test: 1.4885487444950445\n",
      "SCORE: 1.193385942163522                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.65trial/s, best loss: 1.193385942163522]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.07025861066831315, 'n_estimators': 153.0}\n",
      "Test Performance after first tuning round: 1.2983063685376155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 1.2343060362194085                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.78trial/s, best loss: 1.2343060362194085]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.07025861066831315, 'n_estimators': 153.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 1.0377734366091027\n",
      "SCORE: 1.1416583011649917                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.63trial/s, best loss: 1.1416583011649917]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.07025861066831315, 'n_estimators': 153.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.9500002375809609, 'subsample': 0.8448622810427866}\n",
      "Test Performance after third tuning round: 1.0754781727906202\n",
      "SCORE: 1.320209717302945                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.45trial/s, best loss: 1.320209717302945]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.07025861066831315, 'n_estimators': 153.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.9500002375809609, 'subsample': 0.8448622810427866, 'gamma': 3.3130476284611956, 'reg_alpha': 2.0, 'reg_lambda': 1.850214984801005}\n",
      "Test Performance after last tuning round: 1.1392327254359127\n",
      "Preparing results for fold 3, condition=ordinal\n",
      "SCORE: 1.027630689061994                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.32trial/s, best loss: 1.027630689061994]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.2784838980747456}\n",
      "Default performance on Test: 1.5825841906068066\n",
      "SCORE: 0.9637041687440938                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.72trial/s, best loss: 0.9637041687440938]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.34139381022568455, 'n_estimators': 98.0}\n",
      "Test Performance after first tuning round: 1.607135898760009\n",
      "SCORE: 1.0125977450336217                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.29trial/s, best loss: 1.0125977450336217]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.34139381022568455, 'n_estimators': 98.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 1.1970880638673194\n",
      "SCORE: 1.0161949098230747                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.21trial/s, best loss: 1.0161949098230747]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.34139381022568455, 'n_estimators': 98.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.9642577963442325, 'subsample': 0.5690088671414997}\n",
      "Test Performance after third tuning round: 0.9854895640811789\n",
      "SCORE: 1.3775497308603062                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.24trial/s, best loss: 1.3775497308603062]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.34139381022568455, 'n_estimators': 98.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.9642577963442325, 'subsample': 0.5690088671414997, 'gamma': 4.829408937936041, 'reg_alpha': 7.0, 'reg_lambda': 3.7449378114709386}\n",
      "Test Performance after last tuning round: 1.2345942977736741\n",
      "Preparing results for fold 3, condition=catboost\n",
      "SCORE: 1.03526349281019                                                                                                \n",
      "100%|████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.54trial/s, best loss: 1.03526349281019]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.7948372108438148}\n",
      "Default performance on Test: 2.1406366365152865\n",
      "SCORE: 1.2819708487236467                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.78trial/s, best loss: 1.2819708487236467]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.10769851284914837, 'n_estimators': 321.0}\n",
      "Test Performance after first tuning round: 2.1150742183767397\n",
      "SCORE: 1.1813463885431994                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.99trial/s, best loss: 1.1813463885431994]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.10769851284914837, 'n_estimators': 321.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 9.0}\n",
      "Test Performance after second tuning round: 1.5813542912955885\n",
      "SCORE: 1.1859900762891917                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.19trial/s, best loss: 1.1859900762891917]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.10769851284914837, 'n_estimators': 321.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.8937441622908543, 'subsample': 0.5885542950955498}\n",
      "Test Performance after third tuning round: 1.324968326712394\n",
      "SCORE: 1.3850308055212326                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.41trial/s, best loss: 1.3850308055212326]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.10769851284914837, 'n_estimators': 321.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.8937441622908543, 'subsample': 0.5885542950955498, 'gamma': 4.345399574440016, 'reg_alpha': 9.0, 'reg_lambda': 1.6744898035297169}\n",
      "Test Performance after last tuning round: 1.291695728471923\n",
      "Preparing results for fold 3, condition=glmm\n",
      "SCORE: 0.8845956683760262                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.51trial/s, best loss: 0.8845956683760262]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.43887782683791454}\n",
      "Default performance on Test: 1.3657183099515835\n",
      "SCORE: 1.114328592389813                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.81trial/s, best loss: 1.114328592389813]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.1750417468390788, 'n_estimators': 272.0}\n",
      "Test Performance after first tuning round: 1.4431468386926574\n",
      "SCORE: 1.1015480266568947                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.03trial/s, best loss: 1.1015480266568947]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.1750417468390788, 'n_estimators': 272.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 4.0}\n",
      "Test Performance after second tuning round: 1.2755333222516758\n",
      "SCORE: 1.0786493078780037                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.24trial/s, best loss: 1.0786493078780037]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.1750417468390788, 'n_estimators': 272.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.9325921342424375, 'subsample': 0.7001895279679604}\n",
      "Test Performance after third tuning round: 1.274147693544952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 1.372047151238121                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.38trial/s, best loss: 1.372047151238121]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.1750417468390788, 'n_estimators': 272.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.9325921342424375, 'subsample': 0.7001895279679604, 'gamma': 6.910156556412882, 'reg_alpha': 4.0, 'reg_lambda': 2.5515511719729638}\n",
      "Test Performance after last tuning round: 1.1825487046158554\n",
      "Preparing results for fold 4, condition=ignore\n",
      "SCORE: 1.1986935566883763                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 30.30trial/s, best loss: 1.1986935566883763]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.6625397169186523}\n",
      "Default performance on Test: 1.488985227233453\n",
      "SCORE: 1.259686446782701                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.02trial/s, best loss: 1.259686446782701]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4981893512264845, 'n_estimators': 224.0}\n",
      "Test Performance after first tuning round: 1.6901176153473498\n",
      "SCORE: 1.3018628841989919                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.21trial/s, best loss: 1.3018628841989919]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4981893512264845, 'n_estimators': 224.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 0.0}\n",
      "Test Performance after second tuning round: 1.9481070528336029\n",
      "SCORE: 1.3099445783901462                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.59trial/s, best loss: 1.3099445783901462]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4981893512264845, 'n_estimators': 224.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 0.0, 'colsample_bytree': 0.84861581499424, 'subsample': 0.8702263462579787}\n",
      "Test Performance after third tuning round: 1.9592792528480023\n",
      "SCORE: 1.3625678745441057                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.69trial/s, best loss: 1.3625678745441057]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4981893512264845, 'n_estimators': 224.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 0.0, 'colsample_bytree': 0.84861581499424, 'subsample': 0.8702263462579787, 'gamma': 8.672975841155472, 'reg_alpha': 9.0, 'reg_lambda': 1.5823701271846007}\n",
      "Test Performance after last tuning round: 1.285740822650527\n",
      "Preparing results for fold 4, condition=ohe\n",
      "SCORE: 0.8375578911288525                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 12.81trial/s, best loss: 0.8375578911288525]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.6822674311043812}\n",
      "Default performance on Test: 1.1153270982630514\n",
      "SCORE: 1.3163042110987884                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.90trial/s, best loss: 1.3163042110987884]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.02523752266515643, 'n_estimators': 489.0}\n",
      "Test Performance after first tuning round: 0.9318525979831175\n",
      "SCORE: 1.264678744458111                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.64trial/s, best loss: 1.264678744458111]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.02523752266515643, 'n_estimators': 489.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 4.0}\n",
      "Test Performance after second tuning round: 0.9133745679824666\n",
      "SCORE: 1.3214664475904558                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.35trial/s, best loss: 1.3214664475904558]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.02523752266515643, 'n_estimators': 489.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.8523917250858846, 'subsample': 0.8115697566538174}\n",
      "Test Performance after third tuning round: 0.9508035131925063\n",
      "SCORE: 1.377860420023826                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.14trial/s, best loss: 1.377860420023826]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.02523752266515643, 'n_estimators': 489.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.8523917250858846, 'subsample': 0.8115697566538174, 'gamma': 6.97477704665683, 'reg_alpha': 1.0, 'reg_lambda': 3.423055873658225}\n",
      "Test Performance after last tuning round: 1.1817055390055837\n",
      "Preparing results for fold 4, condition=target\n",
      "SCORE: 0.924599922553916                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.49trial/s, best loss: 0.924599922553916]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.1540634050579025}\n",
      "Default performance on Test: 1.2447352829474867\n",
      "SCORE: 1.0968844732933367                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.33trial/s, best loss: 1.0968844732933367]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.46046550833458694, 'n_estimators': 298.0}\n",
      "Test Performance after first tuning round: 1.3523220801113167\n",
      "SCORE: 1.0073040142134935                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.15trial/s, best loss: 1.0073040142134935]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.46046550833458694, 'n_estimators': 298.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 1.3717450405848561\n",
      "SCORE: 0.9915811615979525                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.52trial/s, best loss: 0.9915811615979525]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.46046550833458694, 'n_estimators': 298.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.7833056892140549, 'subsample': 0.9219861047530326}\n",
      "Test Performance after third tuning round: 1.441814145071938\n",
      "SCORE: 1.3533595463785366                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.46trial/s, best loss: 1.3533595463785366]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.46046550833458694, 'n_estimators': 298.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.7833056892140549, 'subsample': 0.9219861047530326, 'gamma': 8.706133016410552, 'reg_alpha': 7.0, 'reg_lambda': 2.2444264898867554}\n",
      "Test Performance after last tuning round: 1.265101431486087\n",
      "Preparing results for fold 4, condition=ordinal\n",
      "SCORE: 0.854362703236422                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.13trial/s, best loss: 0.854362703236422]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.2769177220658577}\n",
      "Default performance on Test: 1.203740258779224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 1.0080844653353245                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.55trial/s, best loss: 1.0080844653353245]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3583877038506449, 'n_estimators': 478.0}\n",
      "Test Performance after first tuning round: 1.3976144241391741\n",
      "SCORE: 1.1083569022478104                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.55trial/s, best loss: 1.1083569022478104]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3583877038506449, 'n_estimators': 478.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 1.5032019229637388\n",
      "SCORE: 1.077919318562937                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.10trial/s, best loss: 1.077919318562937]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3583877038506449, 'n_estimators': 478.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.7983492864826249, 'subsample': 0.9966468291498921}\n",
      "Test Performance after third tuning round: 1.5086174909899603\n",
      "SCORE: 1.344314555853477                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.46trial/s, best loss: 1.344314555853477]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3583877038506449, 'n_estimators': 478.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.7983492864826249, 'subsample': 0.9966468291498921, 'gamma': 2.1113405855544634, 'reg_alpha': 9.0, 'reg_lambda': 3.6518473778574596}\n",
      "Test Performance after last tuning round: 1.2534312945080048\n",
      "Preparing results for fold 4, condition=catboost\n",
      "SCORE: 1.0205608796850796                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.77trial/s, best loss: 1.0205608796850796]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.8710277696030938}\n",
      "Default performance on Test: 1.9001028571288687\n",
      "SCORE: 1.1994627002164353                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.07trial/s, best loss: 1.1994627002164353]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.1984948257653628, 'n_estimators': 177.0}\n",
      "Test Performance after first tuning round: 1.8841259142112081\n",
      "SCORE: 1.169409931590469                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.83trial/s, best loss: 1.169409931590469]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.1984948257653628, 'n_estimators': 177.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 1.955024074593628\n",
      "SCORE: 1.1542419304906706                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.10trial/s, best loss: 1.1542419304906706]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.1984948257653628, 'n_estimators': 177.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.6610182647450704, 'subsample': 0.9100337858124993}\n",
      "Test Performance after third tuning round: 1.699829366487084\n",
      "SCORE: 1.365260000582835                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.49trial/s, best loss: 1.365260000582835]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.1984948257653628, 'n_estimators': 177.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.6610182647450704, 'subsample': 0.9100337858124993, 'gamma': 3.0493605315850583, 'reg_alpha': 6.0, 'reg_lambda': 2.8768039049467578}\n",
      "Test Performance after last tuning round: 1.2094486556459212\n",
      "Preparing results for fold 4, condition=glmm\n",
      "SCORE: 0.9156903655365982                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.64trial/s, best loss: 0.9156903655365982]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.15203721913409185}\n",
      "Default performance on Test: 1.2322366722519975\n",
      "SCORE: 1.2310462736109042                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.42trial/s, best loss: 1.2310462736109042]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.13703365086855018, 'n_estimators': 255.0}\n",
      "Test Performance after first tuning round: 1.2277819507795078\n",
      "SCORE: 1.2043567390525993                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.88trial/s, best loss: 1.2043567390525993]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.13703365086855018, 'n_estimators': 255.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 4.0}\n",
      "Test Performance after second tuning round: 1.213688338131891\n",
      "SCORE: 1.125795650317809                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.68trial/s, best loss: 1.125795650317809]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.13703365086855018, 'n_estimators': 255.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.587311308115853, 'subsample': 0.5854582012546841}\n",
      "Test Performance after third tuning round: 1.0823181700949258\n",
      "SCORE: 1.3784292348700495                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.55trial/s, best loss: 1.3784292348700495]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.13703365086855018, 'n_estimators': 255.0, 'seed': 0, 'max_depth': 2.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.587311308115853, 'subsample': 0.5854582012546841, 'gamma': 6.662506715607223, 'reg_alpha': 6.0, 'reg_lambda': 3.1929855024496847}\n",
      "Test Performance after last tuning round: 1.2742081404994796\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_39ae3_row0_col0, #T_39ae3_row0_col1, #T_39ae3_row0_col2, #T_39ae3_row0_col4, #T_39ae3_row4_col3, #T_39ae3_row4_col5, #T_39ae3_row5_col0, #T_39ae3_row5_col1, #T_39ae3_row5_col2, #T_39ae3_row6_col0, #T_39ae3_row6_col1, #T_39ae3_row6_col2, #T_39ae3_row7_col0, #T_39ae3_row7_col1, #T_39ae3_row7_col2, #T_39ae3_row12_col0, #T_39ae3_row12_col1, #T_39ae3_row12_col2 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_39ae3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_39ae3_level0_col0\" class=\"col_heading level0 col0\" >Accuracy Train</th>\n",
       "      <th id=\"T_39ae3_level0_col1\" class=\"col_heading level0 col1\" >F1 Train</th>\n",
       "      <th id=\"T_39ae3_level0_col2\" class=\"col_heading level0 col2\" >AUROC Train</th>\n",
       "      <th id=\"T_39ae3_level0_col3\" class=\"col_heading level0 col3\" >Accuracy Test</th>\n",
       "      <th id=\"T_39ae3_level0_col4\" class=\"col_heading level0 col4\" >F1 Test</th>\n",
       "      <th id=\"T_39ae3_level0_col5\" class=\"col_heading level0 col5\" >AUROC Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row0\" class=\"row_heading level0 row0\" >XGB_target</th>\n",
       "      <td id=\"T_39ae3_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_39ae3_row0_col1\" class=\"data row0 col1\" >1.000000</td>\n",
       "      <td id=\"T_39ae3_row0_col2\" class=\"data row0 col2\" >1.000000</td>\n",
       "      <td id=\"T_39ae3_row0_col3\" class=\"data row0 col3\" >0.703700</td>\n",
       "      <td id=\"T_39ae3_row0_col4\" class=\"data row0 col4\" >0.767500</td>\n",
       "      <td id=\"T_39ae3_row0_col5\" class=\"data row0 col5\" >0.870600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row1\" class=\"row_heading level0 row1\" >LR_glmm</th>\n",
       "      <td id=\"T_39ae3_row1_col0\" class=\"data row1 col0\" >0.846200</td>\n",
       "      <td id=\"T_39ae3_row1_col1\" class=\"data row1 col1\" >0.838500</td>\n",
       "      <td id=\"T_39ae3_row1_col2\" class=\"data row1 col2\" >0.968700</td>\n",
       "      <td id=\"T_39ae3_row1_col3\" class=\"data row1 col3\" >0.592600</td>\n",
       "      <td id=\"T_39ae3_row1_col4\" class=\"data row1 col4\" >0.680600</td>\n",
       "      <td id=\"T_39ae3_row1_col5\" class=\"data row1 col5\" >0.866100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row2\" class=\"row_heading level0 row2\" >LR_ordinal_tuned</th>\n",
       "      <td id=\"T_39ae3_row2_col0\" class=\"data row2 col0\" >0.769200</td>\n",
       "      <td id=\"T_39ae3_row2_col1\" class=\"data row2 col1\" >0.798300</td>\n",
       "      <td id=\"T_39ae3_row2_col2\" class=\"data row2 col2\" >0.927300</td>\n",
       "      <td id=\"T_39ae3_row2_col3\" class=\"data row2 col3\" >0.518500</td>\n",
       "      <td id=\"T_39ae3_row2_col4\" class=\"data row2 col4\" >0.630300</td>\n",
       "      <td id=\"T_39ae3_row2_col5\" class=\"data row2 col5\" >0.842700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row3\" class=\"row_heading level0 row3\" >LR_ordinal</th>\n",
       "      <td id=\"T_39ae3_row3_col0\" class=\"data row3 col0\" >0.807700</td>\n",
       "      <td id=\"T_39ae3_row3_col1\" class=\"data row3 col1\" >0.827200</td>\n",
       "      <td id=\"T_39ae3_row3_col2\" class=\"data row3 col2\" >0.934700</td>\n",
       "      <td id=\"T_39ae3_row3_col3\" class=\"data row3 col3\" >0.481500</td>\n",
       "      <td id=\"T_39ae3_row3_col4\" class=\"data row3 col4\" >0.603600</td>\n",
       "      <td id=\"T_39ae3_row3_col5\" class=\"data row3 col5\" >0.834800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row4\" class=\"row_heading level0 row4\" >LR_ohe</th>\n",
       "      <td id=\"T_39ae3_row4_col0\" class=\"data row4 col0\" >0.923100</td>\n",
       "      <td id=\"T_39ae3_row4_col1\" class=\"data row4 col1\" >0.920800</td>\n",
       "      <td id=\"T_39ae3_row4_col2\" class=\"data row4 col2\" >0.992100</td>\n",
       "      <td id=\"T_39ae3_row4_col3\" class=\"data row4 col3\" >0.777800</td>\n",
       "      <td id=\"T_39ae3_row4_col4\" class=\"data row4 col4\" >0.598500</td>\n",
       "      <td id=\"T_39ae3_row4_col5\" class=\"data row4 col5\" >0.918300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row5\" class=\"row_heading level0 row5\" >XGB_ohe</th>\n",
       "      <td id=\"T_39ae3_row5_col0\" class=\"data row5 col0\" >1.000000</td>\n",
       "      <td id=\"T_39ae3_row5_col1\" class=\"data row5 col1\" >1.000000</td>\n",
       "      <td id=\"T_39ae3_row5_col2\" class=\"data row5 col2\" >1.000000</td>\n",
       "      <td id=\"T_39ae3_row5_col3\" class=\"data row5 col3\" >0.666700</td>\n",
       "      <td id=\"T_39ae3_row5_col4\" class=\"data row5 col4\" >0.508300</td>\n",
       "      <td id=\"T_39ae3_row5_col5\" class=\"data row5 col5\" >0.856400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row6\" class=\"row_heading level0 row6\" >XGB_glmm</th>\n",
       "      <td id=\"T_39ae3_row6_col0\" class=\"data row6 col0\" >1.000000</td>\n",
       "      <td id=\"T_39ae3_row6_col1\" class=\"data row6 col1\" >1.000000</td>\n",
       "      <td id=\"T_39ae3_row6_col2\" class=\"data row6 col2\" >1.000000</td>\n",
       "      <td id=\"T_39ae3_row6_col3\" class=\"data row6 col3\" >0.629600</td>\n",
       "      <td id=\"T_39ae3_row6_col4\" class=\"data row6 col4\" >0.486500</td>\n",
       "      <td id=\"T_39ae3_row6_col5\" class=\"data row6 col5\" >0.839400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row7\" class=\"row_heading level0 row7\" >XGB_ordinal</th>\n",
       "      <td id=\"T_39ae3_row7_col0\" class=\"data row7 col0\" >1.000000</td>\n",
       "      <td id=\"T_39ae3_row7_col1\" class=\"data row7 col1\" >1.000000</td>\n",
       "      <td id=\"T_39ae3_row7_col2\" class=\"data row7 col2\" >1.000000</td>\n",
       "      <td id=\"T_39ae3_row7_col3\" class=\"data row7 col3\" >0.592600</td>\n",
       "      <td id=\"T_39ae3_row7_col4\" class=\"data row7 col4\" >0.454600</td>\n",
       "      <td id=\"T_39ae3_row7_col5\" class=\"data row7 col5\" >0.879100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row8\" class=\"row_heading level0 row8\" >LR_target</th>\n",
       "      <td id=\"T_39ae3_row8_col0\" class=\"data row8 col0\" >0.798100</td>\n",
       "      <td id=\"T_39ae3_row8_col1\" class=\"data row8 col1\" >0.803400</td>\n",
       "      <td id=\"T_39ae3_row8_col2\" class=\"data row8 col2\" >0.931700</td>\n",
       "      <td id=\"T_39ae3_row8_col3\" class=\"data row8 col3\" >0.555600</td>\n",
       "      <td id=\"T_39ae3_row8_col4\" class=\"data row8 col4\" >0.406700</td>\n",
       "      <td id=\"T_39ae3_row8_col5\" class=\"data row8 col5\" >0.872100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row9\" class=\"row_heading level0 row9\" >XGB_ordinal_tuned</th>\n",
       "      <td id=\"T_39ae3_row9_col0\" class=\"data row9 col0\" >0.625000</td>\n",
       "      <td id=\"T_39ae3_row9_col1\" class=\"data row9 col1\" >0.415100</td>\n",
       "      <td id=\"T_39ae3_row9_col2\" class=\"data row9 col2\" >0.844500</td>\n",
       "      <td id=\"T_39ae3_row9_col3\" class=\"data row9 col3\" >0.555600</td>\n",
       "      <td id=\"T_39ae3_row9_col4\" class=\"data row9 col4\" >0.403800</td>\n",
       "      <td id=\"T_39ae3_row9_col5\" class=\"data row9 col5\" >0.700600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row10\" class=\"row_heading level0 row10\" >LR_glmm_tuned</th>\n",
       "      <td id=\"T_39ae3_row10_col0\" class=\"data row10 col0\" >0.682700</td>\n",
       "      <td id=\"T_39ae3_row10_col1\" class=\"data row10 col1\" >0.509500</td>\n",
       "      <td id=\"T_39ae3_row10_col2\" class=\"data row10 col2\" >0.904800</td>\n",
       "      <td id=\"T_39ae3_row10_col3\" class=\"data row10 col3\" >0.555600</td>\n",
       "      <td id=\"T_39ae3_row10_col4\" class=\"data row10 col4\" >0.374200</td>\n",
       "      <td id=\"T_39ae3_row10_col5\" class=\"data row10 col5\" >0.845800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row11\" class=\"row_heading level0 row11\" >LR_target_tuned</th>\n",
       "      <td id=\"T_39ae3_row11_col0\" class=\"data row11 col0\" >0.730800</td>\n",
       "      <td id=\"T_39ae3_row11_col1\" class=\"data row11 col1\" >0.650800</td>\n",
       "      <td id=\"T_39ae3_row11_col2\" class=\"data row11 col2\" >0.913800</td>\n",
       "      <td id=\"T_39ae3_row11_col3\" class=\"data row11 col3\" >0.518500</td>\n",
       "      <td id=\"T_39ae3_row11_col4\" class=\"data row11 col4\" >0.356800</td>\n",
       "      <td id=\"T_39ae3_row11_col5\" class=\"data row11 col5\" >0.867600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row12\" class=\"row_heading level0 row12\" >XGB_catboost</th>\n",
       "      <td id=\"T_39ae3_row12_col0\" class=\"data row12 col0\" >1.000000</td>\n",
       "      <td id=\"T_39ae3_row12_col1\" class=\"data row12 col1\" >1.000000</td>\n",
       "      <td id=\"T_39ae3_row12_col2\" class=\"data row12 col2\" >1.000000</td>\n",
       "      <td id=\"T_39ae3_row12_col3\" class=\"data row12 col3\" >0.407400</td>\n",
       "      <td id=\"T_39ae3_row12_col4\" class=\"data row12 col4\" >0.308300</td>\n",
       "      <td id=\"T_39ae3_row12_col5\" class=\"data row12 col5\" >0.562700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row13\" class=\"row_heading level0 row13\" >XGB_target_tuned</th>\n",
       "      <td id=\"T_39ae3_row13_col0\" class=\"data row13 col0\" >0.625000</td>\n",
       "      <td id=\"T_39ae3_row13_col1\" class=\"data row13 col1\" >0.361400</td>\n",
       "      <td id=\"T_39ae3_row13_col2\" class=\"data row13 col2\" >0.799100</td>\n",
       "      <td id=\"T_39ae3_row13_col3\" class=\"data row13 col3\" >0.518500</td>\n",
       "      <td id=\"T_39ae3_row13_col4\" class=\"data row13 col4\" >0.306000</td>\n",
       "      <td id=\"T_39ae3_row13_col5\" class=\"data row13 col5\" >0.706400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row14\" class=\"row_heading level0 row14\" >XGB_glmm_tuned</th>\n",
       "      <td id=\"T_39ae3_row14_col0\" class=\"data row14 col0\" >0.625000</td>\n",
       "      <td id=\"T_39ae3_row14_col1\" class=\"data row14 col1\" >0.362500</td>\n",
       "      <td id=\"T_39ae3_row14_col2\" class=\"data row14 col2\" >0.633100</td>\n",
       "      <td id=\"T_39ae3_row14_col3\" class=\"data row14 col3\" >0.518500</td>\n",
       "      <td id=\"T_39ae3_row14_col4\" class=\"data row14 col4\" >0.306000</td>\n",
       "      <td id=\"T_39ae3_row14_col5\" class=\"data row14 col5\" >0.539700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row15\" class=\"row_heading level0 row15\" >LR_ignore</th>\n",
       "      <td id=\"T_39ae3_row15_col0\" class=\"data row15 col0\" >0.490400</td>\n",
       "      <td id=\"T_39ae3_row15_col1\" class=\"data row15 col1\" >0.279900</td>\n",
       "      <td id=\"T_39ae3_row15_col2\" class=\"data row15 col2\" >0.720600</td>\n",
       "      <td id=\"T_39ae3_row15_col3\" class=\"data row15 col3\" >0.481500</td>\n",
       "      <td id=\"T_39ae3_row15_col4\" class=\"data row15 col4\" >0.282600</td>\n",
       "      <td id=\"T_39ae3_row15_col5\" class=\"data row15 col5\" >0.727400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row16\" class=\"row_heading level0 row16\" >LR_catboost</th>\n",
       "      <td id=\"T_39ae3_row16_col0\" class=\"data row16 col0\" >0.711500</td>\n",
       "      <td id=\"T_39ae3_row16_col1\" class=\"data row16 col1\" >0.599600</td>\n",
       "      <td id=\"T_39ae3_row16_col2\" class=\"data row16 col2\" >0.921600</td>\n",
       "      <td id=\"T_39ae3_row16_col3\" class=\"data row16 col3\" >0.481500</td>\n",
       "      <td id=\"T_39ae3_row16_col4\" class=\"data row16 col4\" >0.271900</td>\n",
       "      <td id=\"T_39ae3_row16_col5\" class=\"data row16 col5\" >0.694100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row17\" class=\"row_heading level0 row17\" >LR_catboost_tuned</th>\n",
       "      <td id=\"T_39ae3_row17_col0\" class=\"data row17 col0\" >0.711500</td>\n",
       "      <td id=\"T_39ae3_row17_col1\" class=\"data row17 col1\" >0.599600</td>\n",
       "      <td id=\"T_39ae3_row17_col2\" class=\"data row17 col2\" >0.921300</td>\n",
       "      <td id=\"T_39ae3_row17_col3\" class=\"data row17 col3\" >0.481500</td>\n",
       "      <td id=\"T_39ae3_row17_col4\" class=\"data row17 col4\" >0.271900</td>\n",
       "      <td id=\"T_39ae3_row17_col5\" class=\"data row17 col5\" >0.694100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row18\" class=\"row_heading level0 row18\" >XGB_ignore</th>\n",
       "      <td id=\"T_39ae3_row18_col0\" class=\"data row18 col0\" >0.519200</td>\n",
       "      <td id=\"T_39ae3_row18_col1\" class=\"data row18 col1\" >0.382700</td>\n",
       "      <td id=\"T_39ae3_row18_col2\" class=\"data row18 col2\" >0.794300</td>\n",
       "      <td id=\"T_39ae3_row18_col3\" class=\"data row18 col3\" >0.333300</td>\n",
       "      <td id=\"T_39ae3_row18_col4\" class=\"data row18 col4\" >0.267500</td>\n",
       "      <td id=\"T_39ae3_row18_col5\" class=\"data row18 col5\" >0.533400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row19\" class=\"row_heading level0 row19\" >LR_ohe_tuned</th>\n",
       "      <td id=\"T_39ae3_row19_col0\" class=\"data row19 col0\" >0.653800</td>\n",
       "      <td id=\"T_39ae3_row19_col1\" class=\"data row19 col1\" >0.413700</td>\n",
       "      <td id=\"T_39ae3_row19_col2\" class=\"data row19 col2\" >0.934000</td>\n",
       "      <td id=\"T_39ae3_row19_col3\" class=\"data row19 col3\" >0.407400</td>\n",
       "      <td id=\"T_39ae3_row19_col4\" class=\"data row19 col4\" >0.229000</td>\n",
       "      <td id=\"T_39ae3_row19_col5\" class=\"data row19 col5\" >0.913600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row20\" class=\"row_heading level0 row20\" >XGB_catboost_tuned</th>\n",
       "      <td id=\"T_39ae3_row20_col0\" class=\"data row20 col0\" >0.596200</td>\n",
       "      <td id=\"T_39ae3_row20_col1\" class=\"data row20 col1\" >0.347800</td>\n",
       "      <td id=\"T_39ae3_row20_col2\" class=\"data row20 col2\" >0.817600</td>\n",
       "      <td id=\"T_39ae3_row20_col3\" class=\"data row20 col3\" >0.370400</td>\n",
       "      <td id=\"T_39ae3_row20_col4\" class=\"data row20 col4\" >0.196400</td>\n",
       "      <td id=\"T_39ae3_row20_col5\" class=\"data row20 col5\" >0.523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row21\" class=\"row_heading level0 row21\" >XGB_ohe_tuned</th>\n",
       "      <td id=\"T_39ae3_row21_col0\" class=\"data row21 col0\" >0.442300</td>\n",
       "      <td id=\"T_39ae3_row21_col1\" class=\"data row21 col1\" >0.153300</td>\n",
       "      <td id=\"T_39ae3_row21_col2\" class=\"data row21 col2\" >0.639500</td>\n",
       "      <td id=\"T_39ae3_row21_col3\" class=\"data row21 col3\" >0.296300</td>\n",
       "      <td id=\"T_39ae3_row21_col4\" class=\"data row21 col4\" >0.114300</td>\n",
       "      <td id=\"T_39ae3_row21_col5\" class=\"data row21 col5\" >0.510700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row22\" class=\"row_heading level0 row22\" >XGB_ignore_tuned</th>\n",
       "      <td id=\"T_39ae3_row22_col0\" class=\"data row22 col0\" >0.442300</td>\n",
       "      <td id=\"T_39ae3_row22_col1\" class=\"data row22 col1\" >0.153300</td>\n",
       "      <td id=\"T_39ae3_row22_col2\" class=\"data row22 col2\" >0.572400</td>\n",
       "      <td id=\"T_39ae3_row22_col3\" class=\"data row22 col3\" >0.296300</td>\n",
       "      <td id=\"T_39ae3_row22_col4\" class=\"data row22 col4\" >0.114300</td>\n",
       "      <td id=\"T_39ae3_row22_col5\" class=\"data row22 col5\" >0.533900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row23\" class=\"row_heading level0 row23\" >LR_ignore_tuned</th>\n",
       "      <td id=\"T_39ae3_row23_col0\" class=\"data row23 col0\" >0.442300</td>\n",
       "      <td id=\"T_39ae3_row23_col1\" class=\"data row23 col1\" >0.153300</td>\n",
       "      <td id=\"T_39ae3_row23_col2\" class=\"data row23 col2\" >0.719800</td>\n",
       "      <td id=\"T_39ae3_row23_col3\" class=\"data row23 col3\" >0.296300</td>\n",
       "      <td id=\"T_39ae3_row23_col4\" class=\"data row23 col4\" >0.114300</td>\n",
       "      <td id=\"T_39ae3_row23_col5\" class=\"data row23 col5\" >0.716600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_39ae3_level0_row24\" class=\"row_heading level0 row24\" >Baseline</th>\n",
       "      <td id=\"T_39ae3_row24_col0\" class=\"data row24 col0\" >0.442300</td>\n",
       "      <td id=\"T_39ae3_row24_col1\" class=\"data row24 col1\" >0.153300</td>\n",
       "      <td id=\"T_39ae3_row24_col2\" class=\"data row24 col2\" >0.500000</td>\n",
       "      <td id=\"T_39ae3_row24_col3\" class=\"data row24 col3\" >0.296300</td>\n",
       "      <td id=\"T_39ae3_row24_col4\" class=\"data row24 col4\" >0.114300</td>\n",
       "      <td id=\"T_39ae3_row24_col5\" class=\"data row24 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x298882431f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(f\"../results/{dataset_name}/{experiment_name}/results_encodings.pickle\"):\n",
    "\n",
    "    results_encodings = {}\n",
    "    results_encodings_feature_importances = {}\n",
    "\n",
    "    for fold in range(folds):\n",
    "        results_encodings[fold] = {}\n",
    "        results_encodings_feature_importances[fold] = {}\n",
    "        # Create baseline\n",
    "        y_train = data_dict[f\"y_train_{fold}\"]\n",
    "        y_val = data_dict[f\"y_val_{fold}\"]\n",
    "        y_test = data_dict[f\"y_test_{fold}\"]\n",
    "        y_train_val = np.concatenate([y_train,y_val])\n",
    "\n",
    "        u,c = np.unique(y_train_val,return_counts=True)\n",
    "        nb_classes = len(u)\n",
    "        baseline = np.argmax(c)\n",
    "\n",
    "        y_train_val_pred_base = np.ones(y_train_val.shape[0])*baseline\n",
    "        y_test_pred_base = np.ones(y_test.shape[0])*baseline\n",
    "\n",
    "        results_encodings[fold][\"Baseline\"] = {}\n",
    "        eval_res_train = get_metrics(get_one_hot(y_train_val, nb_classes), get_one_hot(y_train_val_pred_base.astype(int), nb_classes), target=target)\n",
    "        for metric in eval_res_train.keys():\n",
    "            results_encodings[fold][\"Baseline\"][metric + \" Train\"] = eval_res_train[metric]\n",
    "        eval_res_test = get_metrics(get_one_hot(y_test, nb_classes), get_one_hot(y_test_pred_base.astype(int), nb_classes), target=target)\n",
    "        for metric in eval_res_test.keys():\n",
    "            results_encodings[fold][\"Baseline\"][metric + \" Test\"] = eval_res_test[metric]\n",
    "\n",
    "\n",
    "        for condition in conditions:\n",
    "            print(f\"Preparing results for fold {fold}, condition={condition}\")\n",
    "            # Retrieve data\n",
    "            z_cols = data_dict[\"z_cols\"]\n",
    "\n",
    "            X_train = data_dict[f\"X_train_{fold}\"]\n",
    "            y_train = data_dict[f\"y_train_{fold}\"]\n",
    "\n",
    "            X_val = data_dict[f\"X_val_{fold}\"]\n",
    "            y_val = data_dict[f\"y_val_{fold}\"]\n",
    "\n",
    "            X_test = data_dict[f\"X_test_{fold}\"]\n",
    "            y_test = data_dict[f\"y_test_{fold}\"]\n",
    "\n",
    "    ## ALL BUT PERFORMANCE:\n",
    "            # Define data subset for evaluation\n",
    "    #         X_train = X_train[[i for i in X_train.columns if all([j not in i for j in perf_cols])]]\n",
    "    #         X_val = X_val[[i for i in X_val.columns if all([j not in i for j in perf_cols])]]\n",
    "    #         X_test = X_test[[i for i in X_test.columns if all([j not in i for j in perf_cols])]]\n",
    "\n",
    "            # Define condition data subset\n",
    "    #         if condition != \"ignore\":\n",
    "    #             z_encoded_train = data_dict[f\"z_{condition}_encoded_train_{fold}\"] \n",
    "    #             z_encoded_val = data_dict[f\"z_{condition}_encoded_val_{fold}\"] \n",
    "    #             z_encoded_test = data_dict[f\"z_{condition}_encoded_test_{fold}\"] \n",
    "\n",
    "    #             X_train = pd.concat([X_train,z_encoded_train],axis=1)\n",
    "    #             X_val = pd.concat([X_val,z_encoded_val],axis=1)\n",
    "    #             X_test = pd.concat([X_test,z_encoded_test],axis=1)\n",
    "\n",
    "    # ALL BUT PERFORMANCE & ACTIVITY:\n",
    "    #         Define data subset for evaluation\n",
    "            X_train = X_train[[i for i in X_train.columns if all([j not in i for j in perf_cols+activity_cols])]]\n",
    "            X_val = X_val[[i for i in X_val.columns if all([j not in i for j in perf_cols+activity_cols])]]\n",
    "            X_test = X_test[[i for i in X_test.columns if all([j not in i for j in perf_cols+activity_cols])]]\n",
    "\n",
    "    #         Define condition data subset\n",
    "            if condition != \"ignore\":\n",
    "                z_encoded_train = data_dict[f\"z_{condition}_encoded_train_{fold}\"] \n",
    "                z_encoded_val = data_dict[f\"z_{condition}_encoded_val_{fold}\"] \n",
    "                z_encoded_test = data_dict[f\"z_{condition}_encoded_test_{fold}\"] \n",
    "\n",
    "                X_train = pd.concat([X_train,z_encoded_train],axis=1)\n",
    "                X_val = pd.concat([X_val,z_encoded_val],axis=1)\n",
    "                X_test = pd.concat([X_test,z_encoded_test],axis=1)\n",
    "\n",
    "\n",
    "    ## ONLY CATEGORICAL: --> Produces trash as almost never better than baseline\n",
    "    #         if condition != \"ignore\":        \n",
    "    #             X_train = data_dict[f\"z_{condition}_encoded_train_{fold}\"] \n",
    "    #             X_val = data_dict[f\"z_{condition}_encoded_val_{fold}\"] \n",
    "    #             X_test = data_dict[f\"z_{condition}_encoded_test_{fold}\"] \n",
    "\n",
    "    #         else:\n",
    "    #             continue\n",
    "\n",
    "            X_train_val = pd.concat([X_train,X_val])\n",
    "            y_train_val = np.concatenate([y_train,y_val])\n",
    "\n",
    "            # Train base models\n",
    "            res, feats = evaluate_logreg(X_train_val, y_train_val, X_test, y_test, target=target,tune=False, seed=RS)\n",
    "            results_encodings[fold][\"LR_\"+condition] = res\n",
    "            results_encodings_feature_importances[fold][\"LR_\"+condition] = feats\n",
    "\n",
    "            res, feats = evaluate_xgb(X_train_val, y_train_val, X_test, y_test, target, tune=False, max_evals=max_evals, early_stopping_rounds=early_stopping_rounds, seed=RS)\n",
    "            results_encodings[fold][\"XGB_\"+condition] = res\n",
    "            results_encodings_feature_importances[fold][\"XGB_\"+condition] = feats\n",
    "\n",
    "            # Train tuned models\n",
    "            res, feats = evaluate_logreg(X_train_val, y_train_val, X_test, y_test, target=target, max_evals=max_evals, tune=True, seed=RS)\n",
    "            results_encodings[fold][\"LR_\"+condition+\"_tuned\"] = res\n",
    "            results_encodings_feature_importances[fold][\"LR_\"+condition+\"_tuned\"] = feats\n",
    "\n",
    "            res, feats = evaluate_xgb(X_train_val, y_train_val, X_test, y_test, target, tune=True, max_evals=max_evals, early_stopping_rounds=early_stopping_rounds, seed=RS)\n",
    "            results_encodings[fold][\"XGB_\"+condition+\"_tuned\"] = res\n",
    "            results_encodings_feature_importances[fold][\"XGB_\"+condition+\"_tuned\"] = feats\n",
    "    \n",
    "    if not os.path.exists(f\"../results/{dataset_name}/{experiment_name}\"):\n",
    "        os.makedirs(f\"../results/{dataset_name}/{experiment_name}\")\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_encodings.pickle\", 'wb') as handle:\n",
    "        pickle.dump(results_encodings, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_encodings_feature_importances.pickle\", 'wb') as handle:\n",
    "        pickle.dump(results_encodings_feature_importances, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "else:\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_encodings.pickle\", 'rb') as handle:\n",
    "        results_encodings = pickle.load(handle)\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_encodings_feature_importances.pickle\", 'rb') as handle:\n",
    "        results_encodings_feature_importances = pickle.load(handle)\n",
    "        \n",
    "        \n",
    "results_encodings_df = pd.DataFrame(results_encodings[0]).transpose().sort_values(\"F1 Test\",ascending=False).round(4)\n",
    "results_encodings_df[[\"Accuracy Train\", \"F1 Train\", \"AUROC Train\", \"Accuracy Test\", \"F1 Test\", \"AUROC Test\"]].style.highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07ab0956",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_02b56_row0_col0, #T_02b56_row0_col1, #T_02b56_row0_col2, #T_02b56_row0_col4, #T_02b56_row4_col3, #T_02b56_row4_col5, #T_02b56_row5_col0, #T_02b56_row5_col1, #T_02b56_row5_col2, #T_02b56_row6_col0, #T_02b56_row6_col1, #T_02b56_row6_col2, #T_02b56_row7_col0, #T_02b56_row7_col1, #T_02b56_row7_col2, #T_02b56_row12_col0, #T_02b56_row12_col1, #T_02b56_row12_col2 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_02b56\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_02b56_level0_col0\" class=\"col_heading level0 col0\" >Accuracy Train</th>\n",
       "      <th id=\"T_02b56_level0_col1\" class=\"col_heading level0 col1\" >F1 Train</th>\n",
       "      <th id=\"T_02b56_level0_col2\" class=\"col_heading level0 col2\" >AUROC Train</th>\n",
       "      <th id=\"T_02b56_level0_col3\" class=\"col_heading level0 col3\" >Accuracy Test</th>\n",
       "      <th id=\"T_02b56_level0_col4\" class=\"col_heading level0 col4\" >F1 Test</th>\n",
       "      <th id=\"T_02b56_level0_col5\" class=\"col_heading level0 col5\" >AUROC Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row0\" class=\"row_heading level0 row0\" >XGB_target</th>\n",
       "      <td id=\"T_02b56_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_02b56_row0_col1\" class=\"data row0 col1\" >1.000000</td>\n",
       "      <td id=\"T_02b56_row0_col2\" class=\"data row0 col2\" >1.000000</td>\n",
       "      <td id=\"T_02b56_row0_col3\" class=\"data row0 col3\" >0.703700</td>\n",
       "      <td id=\"T_02b56_row0_col4\" class=\"data row0 col4\" >0.767500</td>\n",
       "      <td id=\"T_02b56_row0_col5\" class=\"data row0 col5\" >0.870600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row1\" class=\"row_heading level0 row1\" >LR_glmm</th>\n",
       "      <td id=\"T_02b56_row1_col0\" class=\"data row1 col0\" >0.846200</td>\n",
       "      <td id=\"T_02b56_row1_col1\" class=\"data row1 col1\" >0.838500</td>\n",
       "      <td id=\"T_02b56_row1_col2\" class=\"data row1 col2\" >0.968700</td>\n",
       "      <td id=\"T_02b56_row1_col3\" class=\"data row1 col3\" >0.592600</td>\n",
       "      <td id=\"T_02b56_row1_col4\" class=\"data row1 col4\" >0.680600</td>\n",
       "      <td id=\"T_02b56_row1_col5\" class=\"data row1 col5\" >0.866100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row2\" class=\"row_heading level0 row2\" >LR_ordinal_tuned</th>\n",
       "      <td id=\"T_02b56_row2_col0\" class=\"data row2 col0\" >0.769200</td>\n",
       "      <td id=\"T_02b56_row2_col1\" class=\"data row2 col1\" >0.798300</td>\n",
       "      <td id=\"T_02b56_row2_col2\" class=\"data row2 col2\" >0.927300</td>\n",
       "      <td id=\"T_02b56_row2_col3\" class=\"data row2 col3\" >0.518500</td>\n",
       "      <td id=\"T_02b56_row2_col4\" class=\"data row2 col4\" >0.630300</td>\n",
       "      <td id=\"T_02b56_row2_col5\" class=\"data row2 col5\" >0.842700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row3\" class=\"row_heading level0 row3\" >LR_ordinal</th>\n",
       "      <td id=\"T_02b56_row3_col0\" class=\"data row3 col0\" >0.807700</td>\n",
       "      <td id=\"T_02b56_row3_col1\" class=\"data row3 col1\" >0.827200</td>\n",
       "      <td id=\"T_02b56_row3_col2\" class=\"data row3 col2\" >0.934700</td>\n",
       "      <td id=\"T_02b56_row3_col3\" class=\"data row3 col3\" >0.481500</td>\n",
       "      <td id=\"T_02b56_row3_col4\" class=\"data row3 col4\" >0.603600</td>\n",
       "      <td id=\"T_02b56_row3_col5\" class=\"data row3 col5\" >0.834800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row4\" class=\"row_heading level0 row4\" >LR_ohe</th>\n",
       "      <td id=\"T_02b56_row4_col0\" class=\"data row4 col0\" >0.923100</td>\n",
       "      <td id=\"T_02b56_row4_col1\" class=\"data row4 col1\" >0.920800</td>\n",
       "      <td id=\"T_02b56_row4_col2\" class=\"data row4 col2\" >0.992100</td>\n",
       "      <td id=\"T_02b56_row4_col3\" class=\"data row4 col3\" >0.777800</td>\n",
       "      <td id=\"T_02b56_row4_col4\" class=\"data row4 col4\" >0.598500</td>\n",
       "      <td id=\"T_02b56_row4_col5\" class=\"data row4 col5\" >0.918300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row5\" class=\"row_heading level0 row5\" >XGB_ohe</th>\n",
       "      <td id=\"T_02b56_row5_col0\" class=\"data row5 col0\" >1.000000</td>\n",
       "      <td id=\"T_02b56_row5_col1\" class=\"data row5 col1\" >1.000000</td>\n",
       "      <td id=\"T_02b56_row5_col2\" class=\"data row5 col2\" >1.000000</td>\n",
       "      <td id=\"T_02b56_row5_col3\" class=\"data row5 col3\" >0.666700</td>\n",
       "      <td id=\"T_02b56_row5_col4\" class=\"data row5 col4\" >0.508300</td>\n",
       "      <td id=\"T_02b56_row5_col5\" class=\"data row5 col5\" >0.856400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row6\" class=\"row_heading level0 row6\" >XGB_glmm</th>\n",
       "      <td id=\"T_02b56_row6_col0\" class=\"data row6 col0\" >1.000000</td>\n",
       "      <td id=\"T_02b56_row6_col1\" class=\"data row6 col1\" >1.000000</td>\n",
       "      <td id=\"T_02b56_row6_col2\" class=\"data row6 col2\" >1.000000</td>\n",
       "      <td id=\"T_02b56_row6_col3\" class=\"data row6 col3\" >0.629600</td>\n",
       "      <td id=\"T_02b56_row6_col4\" class=\"data row6 col4\" >0.486500</td>\n",
       "      <td id=\"T_02b56_row6_col5\" class=\"data row6 col5\" >0.839400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row7\" class=\"row_heading level0 row7\" >XGB_ordinal</th>\n",
       "      <td id=\"T_02b56_row7_col0\" class=\"data row7 col0\" >1.000000</td>\n",
       "      <td id=\"T_02b56_row7_col1\" class=\"data row7 col1\" >1.000000</td>\n",
       "      <td id=\"T_02b56_row7_col2\" class=\"data row7 col2\" >1.000000</td>\n",
       "      <td id=\"T_02b56_row7_col3\" class=\"data row7 col3\" >0.592600</td>\n",
       "      <td id=\"T_02b56_row7_col4\" class=\"data row7 col4\" >0.454600</td>\n",
       "      <td id=\"T_02b56_row7_col5\" class=\"data row7 col5\" >0.879100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row8\" class=\"row_heading level0 row8\" >LR_target</th>\n",
       "      <td id=\"T_02b56_row8_col0\" class=\"data row8 col0\" >0.798100</td>\n",
       "      <td id=\"T_02b56_row8_col1\" class=\"data row8 col1\" >0.803400</td>\n",
       "      <td id=\"T_02b56_row8_col2\" class=\"data row8 col2\" >0.931700</td>\n",
       "      <td id=\"T_02b56_row8_col3\" class=\"data row8 col3\" >0.555600</td>\n",
       "      <td id=\"T_02b56_row8_col4\" class=\"data row8 col4\" >0.406700</td>\n",
       "      <td id=\"T_02b56_row8_col5\" class=\"data row8 col5\" >0.872100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row9\" class=\"row_heading level0 row9\" >XGB_ordinal_tuned</th>\n",
       "      <td id=\"T_02b56_row9_col0\" class=\"data row9 col0\" >0.625000</td>\n",
       "      <td id=\"T_02b56_row9_col1\" class=\"data row9 col1\" >0.415100</td>\n",
       "      <td id=\"T_02b56_row9_col2\" class=\"data row9 col2\" >0.844500</td>\n",
       "      <td id=\"T_02b56_row9_col3\" class=\"data row9 col3\" >0.555600</td>\n",
       "      <td id=\"T_02b56_row9_col4\" class=\"data row9 col4\" >0.403800</td>\n",
       "      <td id=\"T_02b56_row9_col5\" class=\"data row9 col5\" >0.700600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row10\" class=\"row_heading level0 row10\" >LR_glmm_tuned</th>\n",
       "      <td id=\"T_02b56_row10_col0\" class=\"data row10 col0\" >0.682700</td>\n",
       "      <td id=\"T_02b56_row10_col1\" class=\"data row10 col1\" >0.509500</td>\n",
       "      <td id=\"T_02b56_row10_col2\" class=\"data row10 col2\" >0.904800</td>\n",
       "      <td id=\"T_02b56_row10_col3\" class=\"data row10 col3\" >0.555600</td>\n",
       "      <td id=\"T_02b56_row10_col4\" class=\"data row10 col4\" >0.374200</td>\n",
       "      <td id=\"T_02b56_row10_col5\" class=\"data row10 col5\" >0.845800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row11\" class=\"row_heading level0 row11\" >LR_target_tuned</th>\n",
       "      <td id=\"T_02b56_row11_col0\" class=\"data row11 col0\" >0.730800</td>\n",
       "      <td id=\"T_02b56_row11_col1\" class=\"data row11 col1\" >0.650800</td>\n",
       "      <td id=\"T_02b56_row11_col2\" class=\"data row11 col2\" >0.913800</td>\n",
       "      <td id=\"T_02b56_row11_col3\" class=\"data row11 col3\" >0.518500</td>\n",
       "      <td id=\"T_02b56_row11_col4\" class=\"data row11 col4\" >0.356800</td>\n",
       "      <td id=\"T_02b56_row11_col5\" class=\"data row11 col5\" >0.867600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row12\" class=\"row_heading level0 row12\" >XGB_catboost</th>\n",
       "      <td id=\"T_02b56_row12_col0\" class=\"data row12 col0\" >1.000000</td>\n",
       "      <td id=\"T_02b56_row12_col1\" class=\"data row12 col1\" >1.000000</td>\n",
       "      <td id=\"T_02b56_row12_col2\" class=\"data row12 col2\" >1.000000</td>\n",
       "      <td id=\"T_02b56_row12_col3\" class=\"data row12 col3\" >0.407400</td>\n",
       "      <td id=\"T_02b56_row12_col4\" class=\"data row12 col4\" >0.308300</td>\n",
       "      <td id=\"T_02b56_row12_col5\" class=\"data row12 col5\" >0.562700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row13\" class=\"row_heading level0 row13\" >XGB_target_tuned</th>\n",
       "      <td id=\"T_02b56_row13_col0\" class=\"data row13 col0\" >0.625000</td>\n",
       "      <td id=\"T_02b56_row13_col1\" class=\"data row13 col1\" >0.361400</td>\n",
       "      <td id=\"T_02b56_row13_col2\" class=\"data row13 col2\" >0.799100</td>\n",
       "      <td id=\"T_02b56_row13_col3\" class=\"data row13 col3\" >0.518500</td>\n",
       "      <td id=\"T_02b56_row13_col4\" class=\"data row13 col4\" >0.306000</td>\n",
       "      <td id=\"T_02b56_row13_col5\" class=\"data row13 col5\" >0.706400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row14\" class=\"row_heading level0 row14\" >XGB_glmm_tuned</th>\n",
       "      <td id=\"T_02b56_row14_col0\" class=\"data row14 col0\" >0.625000</td>\n",
       "      <td id=\"T_02b56_row14_col1\" class=\"data row14 col1\" >0.362500</td>\n",
       "      <td id=\"T_02b56_row14_col2\" class=\"data row14 col2\" >0.633100</td>\n",
       "      <td id=\"T_02b56_row14_col3\" class=\"data row14 col3\" >0.518500</td>\n",
       "      <td id=\"T_02b56_row14_col4\" class=\"data row14 col4\" >0.306000</td>\n",
       "      <td id=\"T_02b56_row14_col5\" class=\"data row14 col5\" >0.539700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row15\" class=\"row_heading level0 row15\" >LR_ignore</th>\n",
       "      <td id=\"T_02b56_row15_col0\" class=\"data row15 col0\" >0.490400</td>\n",
       "      <td id=\"T_02b56_row15_col1\" class=\"data row15 col1\" >0.279900</td>\n",
       "      <td id=\"T_02b56_row15_col2\" class=\"data row15 col2\" >0.720600</td>\n",
       "      <td id=\"T_02b56_row15_col3\" class=\"data row15 col3\" >0.481500</td>\n",
       "      <td id=\"T_02b56_row15_col4\" class=\"data row15 col4\" >0.282600</td>\n",
       "      <td id=\"T_02b56_row15_col5\" class=\"data row15 col5\" >0.727400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row16\" class=\"row_heading level0 row16\" >LR_catboost</th>\n",
       "      <td id=\"T_02b56_row16_col0\" class=\"data row16 col0\" >0.711500</td>\n",
       "      <td id=\"T_02b56_row16_col1\" class=\"data row16 col1\" >0.599600</td>\n",
       "      <td id=\"T_02b56_row16_col2\" class=\"data row16 col2\" >0.921600</td>\n",
       "      <td id=\"T_02b56_row16_col3\" class=\"data row16 col3\" >0.481500</td>\n",
       "      <td id=\"T_02b56_row16_col4\" class=\"data row16 col4\" >0.271900</td>\n",
       "      <td id=\"T_02b56_row16_col5\" class=\"data row16 col5\" >0.694100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row17\" class=\"row_heading level0 row17\" >LR_catboost_tuned</th>\n",
       "      <td id=\"T_02b56_row17_col0\" class=\"data row17 col0\" >0.711500</td>\n",
       "      <td id=\"T_02b56_row17_col1\" class=\"data row17 col1\" >0.599600</td>\n",
       "      <td id=\"T_02b56_row17_col2\" class=\"data row17 col2\" >0.921300</td>\n",
       "      <td id=\"T_02b56_row17_col3\" class=\"data row17 col3\" >0.481500</td>\n",
       "      <td id=\"T_02b56_row17_col4\" class=\"data row17 col4\" >0.271900</td>\n",
       "      <td id=\"T_02b56_row17_col5\" class=\"data row17 col5\" >0.694100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row18\" class=\"row_heading level0 row18\" >XGB_ignore</th>\n",
       "      <td id=\"T_02b56_row18_col0\" class=\"data row18 col0\" >0.519200</td>\n",
       "      <td id=\"T_02b56_row18_col1\" class=\"data row18 col1\" >0.382700</td>\n",
       "      <td id=\"T_02b56_row18_col2\" class=\"data row18 col2\" >0.794300</td>\n",
       "      <td id=\"T_02b56_row18_col3\" class=\"data row18 col3\" >0.333300</td>\n",
       "      <td id=\"T_02b56_row18_col4\" class=\"data row18 col4\" >0.267500</td>\n",
       "      <td id=\"T_02b56_row18_col5\" class=\"data row18 col5\" >0.533400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row19\" class=\"row_heading level0 row19\" >LR_ohe_tuned</th>\n",
       "      <td id=\"T_02b56_row19_col0\" class=\"data row19 col0\" >0.653800</td>\n",
       "      <td id=\"T_02b56_row19_col1\" class=\"data row19 col1\" >0.413700</td>\n",
       "      <td id=\"T_02b56_row19_col2\" class=\"data row19 col2\" >0.934000</td>\n",
       "      <td id=\"T_02b56_row19_col3\" class=\"data row19 col3\" >0.407400</td>\n",
       "      <td id=\"T_02b56_row19_col4\" class=\"data row19 col4\" >0.229000</td>\n",
       "      <td id=\"T_02b56_row19_col5\" class=\"data row19 col5\" >0.913600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row20\" class=\"row_heading level0 row20\" >XGB_catboost_tuned</th>\n",
       "      <td id=\"T_02b56_row20_col0\" class=\"data row20 col0\" >0.596200</td>\n",
       "      <td id=\"T_02b56_row20_col1\" class=\"data row20 col1\" >0.347800</td>\n",
       "      <td id=\"T_02b56_row20_col2\" class=\"data row20 col2\" >0.817600</td>\n",
       "      <td id=\"T_02b56_row20_col3\" class=\"data row20 col3\" >0.370400</td>\n",
       "      <td id=\"T_02b56_row20_col4\" class=\"data row20 col4\" >0.196400</td>\n",
       "      <td id=\"T_02b56_row20_col5\" class=\"data row20 col5\" >0.523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row21\" class=\"row_heading level0 row21\" >XGB_ohe_tuned</th>\n",
       "      <td id=\"T_02b56_row21_col0\" class=\"data row21 col0\" >0.442300</td>\n",
       "      <td id=\"T_02b56_row21_col1\" class=\"data row21 col1\" >0.153300</td>\n",
       "      <td id=\"T_02b56_row21_col2\" class=\"data row21 col2\" >0.639500</td>\n",
       "      <td id=\"T_02b56_row21_col3\" class=\"data row21 col3\" >0.296300</td>\n",
       "      <td id=\"T_02b56_row21_col4\" class=\"data row21 col4\" >0.114300</td>\n",
       "      <td id=\"T_02b56_row21_col5\" class=\"data row21 col5\" >0.510700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row22\" class=\"row_heading level0 row22\" >XGB_ignore_tuned</th>\n",
       "      <td id=\"T_02b56_row22_col0\" class=\"data row22 col0\" >0.442300</td>\n",
       "      <td id=\"T_02b56_row22_col1\" class=\"data row22 col1\" >0.153300</td>\n",
       "      <td id=\"T_02b56_row22_col2\" class=\"data row22 col2\" >0.572400</td>\n",
       "      <td id=\"T_02b56_row22_col3\" class=\"data row22 col3\" >0.296300</td>\n",
       "      <td id=\"T_02b56_row22_col4\" class=\"data row22 col4\" >0.114300</td>\n",
       "      <td id=\"T_02b56_row22_col5\" class=\"data row22 col5\" >0.533900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row23\" class=\"row_heading level0 row23\" >LR_ignore_tuned</th>\n",
       "      <td id=\"T_02b56_row23_col0\" class=\"data row23 col0\" >0.442300</td>\n",
       "      <td id=\"T_02b56_row23_col1\" class=\"data row23 col1\" >0.153300</td>\n",
       "      <td id=\"T_02b56_row23_col2\" class=\"data row23 col2\" >0.719800</td>\n",
       "      <td id=\"T_02b56_row23_col3\" class=\"data row23 col3\" >0.296300</td>\n",
       "      <td id=\"T_02b56_row23_col4\" class=\"data row23 col4\" >0.114300</td>\n",
       "      <td id=\"T_02b56_row23_col5\" class=\"data row23 col5\" >0.716600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_02b56_level0_row24\" class=\"row_heading level0 row24\" >Baseline</th>\n",
       "      <td id=\"T_02b56_row24_col0\" class=\"data row24 col0\" >0.442300</td>\n",
       "      <td id=\"T_02b56_row24_col1\" class=\"data row24 col1\" >0.153300</td>\n",
       "      <td id=\"T_02b56_row24_col2\" class=\"data row24 col2\" >0.500000</td>\n",
       "      <td id=\"T_02b56_row24_col3\" class=\"data row24 col3\" >0.296300</td>\n",
       "      <td id=\"T_02b56_row24_col4\" class=\"data row24 col4\" >0.114300</td>\n",
       "      <td id=\"T_02b56_row24_col5\" class=\"data row24 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x29888243b80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_encodings_df = pd.DataFrame(results_encodings[0]).transpose().sort_values(\"F1 Test\",ascending=False).round(4)\n",
    "results_encodings_df[[\"Accuracy Train\", \"F1 Train\", \"AUROC Train\", \"Accuracy Test\", \"F1 Test\", \"AUROC Test\"]].style.highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c0bf64",
   "metadata": {},
   "source": [
    "### Effectiveness of Parameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "344961c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_15a56_row0_col0, #T_15a56_row0_col1, #T_15a56_row1_col0, #T_15a56_row1_col1, #T_15a56_row2_col0, #T_15a56_row3_col0, #T_15a56_row4_col0, #T_15a56_row5_col1, #T_15a56_row6_col0, #T_15a56_row7_col0, #T_15a56_row8_col0, #T_15a56_row9_col0, #T_15a56_row10_col0, #T_15a56_row11_col0, #T_15a56_row12_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_15a56\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_15a56_level0_col0\" class=\"col_heading level0 col0\" >Untuned</th>\n",
       "      <th id=\"T_15a56_level0_col1\" class=\"col_heading level0 col1\" >Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_15a56_level0_row0\" class=\"row_heading level0 row0\" >Baseline</th>\n",
       "      <td id=\"T_15a56_row0_col0\" class=\"data row0 col0\" >0.140000</td>\n",
       "      <td id=\"T_15a56_row0_col1\" class=\"data row0 col1\" >0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15a56_level0_row1\" class=\"row_heading level0 row1\" >LR_catboost</th>\n",
       "      <td id=\"T_15a56_row1_col0\" class=\"data row1 col0\" >0.280000</td>\n",
       "      <td id=\"T_15a56_row1_col1\" class=\"data row1 col1\" >0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15a56_level0_row2\" class=\"row_heading level0 row2\" >LR_glmm</th>\n",
       "      <td id=\"T_15a56_row2_col0\" class=\"data row2 col0\" >0.520000</td>\n",
       "      <td id=\"T_15a56_row2_col1\" class=\"data row2 col1\" >0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15a56_level0_row3\" class=\"row_heading level0 row3\" >LR_ignore</th>\n",
       "      <td id=\"T_15a56_row3_col0\" class=\"data row3 col0\" >0.290000</td>\n",
       "      <td id=\"T_15a56_row3_col1\" class=\"data row3 col1\" >0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15a56_level0_row4\" class=\"row_heading level0 row4\" >LR_ohe</th>\n",
       "      <td id=\"T_15a56_row4_col0\" class=\"data row4 col0\" >0.480000</td>\n",
       "      <td id=\"T_15a56_row4_col1\" class=\"data row4 col1\" >0.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15a56_level0_row5\" class=\"row_heading level0 row5\" >LR_ordinal</th>\n",
       "      <td id=\"T_15a56_row5_col0\" class=\"data row5 col0\" >0.500000</td>\n",
       "      <td id=\"T_15a56_row5_col1\" class=\"data row5 col1\" >0.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15a56_level0_row6\" class=\"row_heading level0 row6\" >LR_target</th>\n",
       "      <td id=\"T_15a56_row6_col0\" class=\"data row6 col0\" >0.530000</td>\n",
       "      <td id=\"T_15a56_row6_col1\" class=\"data row6 col1\" >0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15a56_level0_row7\" class=\"row_heading level0 row7\" >XGB_catboost</th>\n",
       "      <td id=\"T_15a56_row7_col0\" class=\"data row7 col0\" >0.240000</td>\n",
       "      <td id=\"T_15a56_row7_col1\" class=\"data row7 col1\" >0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15a56_level0_row8\" class=\"row_heading level0 row8\" >XGB_glmm</th>\n",
       "      <td id=\"T_15a56_row8_col0\" class=\"data row8 col0\" >0.520000</td>\n",
       "      <td id=\"T_15a56_row8_col1\" class=\"data row8 col1\" >0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15a56_level0_row9\" class=\"row_heading level0 row9\" >XGB_ignore</th>\n",
       "      <td id=\"T_15a56_row9_col0\" class=\"data row9 col0\" >0.250000</td>\n",
       "      <td id=\"T_15a56_row9_col1\" class=\"data row9 col1\" >0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15a56_level0_row10\" class=\"row_heading level0 row10\" >XGB_ohe</th>\n",
       "      <td id=\"T_15a56_row10_col0\" class=\"data row10 col0\" >0.510000</td>\n",
       "      <td id=\"T_15a56_row10_col1\" class=\"data row10 col1\" >0.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15a56_level0_row11\" class=\"row_heading level0 row11\" >XGB_ordinal</th>\n",
       "      <td id=\"T_15a56_row11_col0\" class=\"data row11 col0\" >0.510000</td>\n",
       "      <td id=\"T_15a56_row11_col1\" class=\"data row11 col1\" >0.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15a56_level0_row12\" class=\"row_heading level0 row12\" >XGB_target</th>\n",
       "      <td id=\"T_15a56_row12_col0\" class=\"data row12 col0\" >0.580000</td>\n",
       "      <td id=\"T_15a56_row12_col1\" class=\"data row12 col1\" >0.260000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x298e75d9af0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = results_encodings[0].keys()\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "encodings_folds_df = pd.DataFrame([pd.DataFrame(results_encodings[fold_num]).loc[metric,models] for fold_num in results_encodings.keys()],index=results_encodings.keys())\n",
    "encodings_mean_df = encodings_folds_df.mean(axis=0)\n",
    "encodings_std_df = encodings_folds_df.std(axis=0)\n",
    "\n",
    "methods = sorted(list(encodings_mean_df.index))[1:]\n",
    "not_tuned = [\"Baseline\"]+methods[::2]\n",
    "tuned = [\"Baseline\"]+methods[1::2]\n",
    "\n",
    "res_df_tune_comp_mean = pd.DataFrame([encodings_mean_df.loc[not_tuned].values,encodings_mean_df.loc[tuned]],index=[\"Untuned\",\"Tuned\"],columns=not_tuned).transpose()\n",
    "res_df_tune_comp_std = pd.DataFrame([encodings_std_df.loc[not_tuned].values,encodings_std_df.loc[tuned]],index=[\"Untuned\",\"Tuned\"],columns=not_tuned).transpose()\n",
    "\n",
    "res_df_tune_comp_mean.round(2).style.highlight_max(color = 'lightgreen', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21c8bf56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Untuned</th>\n",
       "      <th>Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.14 (0.026)</td>\n",
       "      <td>0.14 (0.026)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_catboost</th>\n",
       "      <td>0.28 (0.032)</td>\n",
       "      <td>0.28 (0.032)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_glmm</th>\n",
       "      <td>0.52 (0.147)</td>\n",
       "      <td>0.44 (0.113)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_ignore</th>\n",
       "      <td>0.29 (0.111)</td>\n",
       "      <td>0.25 (0.136)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_ohe</th>\n",
       "      <td>0.48 (0.151)</td>\n",
       "      <td>0.41 (0.14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_ordinal</th>\n",
       "      <td>0.5 (0.077)</td>\n",
       "      <td>0.51 (0.091)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_target</th>\n",
       "      <td>0.53 (0.15)</td>\n",
       "      <td>0.4 (0.027)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_catboost</th>\n",
       "      <td>0.24 (0.053)</td>\n",
       "      <td>0.16 (0.027)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_glmm</th>\n",
       "      <td>0.52 (0.088)</td>\n",
       "      <td>0.25 (0.112)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_ignore</th>\n",
       "      <td>0.25 (0.094)</td>\n",
       "      <td>0.14 (0.026)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_ohe</th>\n",
       "      <td>0.51 (0.082)</td>\n",
       "      <td>0.21 (0.116)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_ordinal</th>\n",
       "      <td>0.51 (0.081)</td>\n",
       "      <td>0.31 (0.102)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_target</th>\n",
       "      <td>0.58 (0.128)</td>\n",
       "      <td>0.26 (0.105)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Untuned         Tuned\n",
       "Baseline      0.14 (0.026)  0.14 (0.026)\n",
       "LR_catboost   0.28 (0.032)  0.28 (0.032)\n",
       "LR_glmm       0.52 (0.147)  0.44 (0.113)\n",
       "LR_ignore     0.29 (0.111)  0.25 (0.136)\n",
       "LR_ohe        0.48 (0.151)   0.41 (0.14)\n",
       "LR_ordinal     0.5 (0.077)  0.51 (0.091)\n",
       "LR_target      0.53 (0.15)   0.4 (0.027)\n",
       "XGB_catboost  0.24 (0.053)  0.16 (0.027)\n",
       "XGB_glmm      0.52 (0.088)  0.25 (0.112)\n",
       "XGB_ignore    0.25 (0.094)  0.14 (0.026)\n",
       "XGB_ohe       0.51 (0.082)  0.21 (0.116)\n",
       "XGB_ordinal   0.51 (0.081)  0.31 (0.102)\n",
       "XGB_target    0.58 (0.128)  0.26 (0.105)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_df = res_df_tune_comp_mean.round(2).astype(str) + \" (\" +  res_df_tune_comp_std.round(3).astype(str) + \")\"\n",
    "latex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1888cc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lll}\n",
      "\\toprule\n",
      "{} &       Untuned &         Tuned \\\\\n",
      "\\midrule\n",
      "Baseline     &  0.14 (0.026) &  0.14 (0.026) \\\\\n",
      "LR\\_catboost  &  0.28 (0.032) &  0.28 (0.032) \\\\\n",
      "LR\\_glmm      &  0.52 (0.147) &  0.44 (0.113) \\\\\n",
      "LR\\_ignore    &  0.29 (0.111) &  0.25 (0.136) \\\\\n",
      "LR\\_ohe       &  0.48 (0.151) &   0.41 (0.14) \\\\\n",
      "LR\\_ordinal   &   0.5 (0.077) &  0.51 (0.091) \\\\\n",
      "LR\\_target    &   0.53 (0.15) &   0.4 (0.027) \\\\\n",
      "XGB\\_catboost &  0.24 (0.053) &  0.16 (0.027) \\\\\n",
      "XGB\\_glmm     &  0.52 (0.088) &  0.25 (0.112) \\\\\n",
      "XGB\\_ignore   &  0.25 (0.094) &  0.14 (0.026) \\\\\n",
      "XGB\\_ohe      &  0.51 (0.082) &  0.21 (0.116) \\\\\n",
      "XGB\\_ordinal  &  0.51 (0.081) &  0.31 (0.102) \\\\\n",
      "XGB\\_target   &  0.58 (0.128) &  0.26 (0.105) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_df.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f3abd6",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f617d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_57889_row0_col2, #T_57889_row0_col3, #T_57889_row0_col4, #T_57889_row0_col6 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_57889\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_57889_level0_col0\" class=\"col_heading level0 col0\" >Baseline</th>\n",
       "      <th id=\"T_57889_level0_col1\" class=\"col_heading level0 col1\" >LR_ignore_tuned</th>\n",
       "      <th id=\"T_57889_level0_col2\" class=\"col_heading level0 col2\" >LR_ohe_tuned</th>\n",
       "      <th id=\"T_57889_level0_col3\" class=\"col_heading level0 col3\" >LR_target_tuned</th>\n",
       "      <th id=\"T_57889_level0_col4\" class=\"col_heading level0 col4\" >LR_ordinal_tuned</th>\n",
       "      <th id=\"T_57889_level0_col5\" class=\"col_heading level0 col5\" >LR_catboost_tuned</th>\n",
       "      <th id=\"T_57889_level0_col6\" class=\"col_heading level0 col6\" >LR_glmm_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_57889_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_57889_row0_col0\" class=\"data row0 col0\" >0.145 (0.026)</td>\n",
       "      <td id=\"T_57889_row0_col1\" class=\"data row0 col1\" >0.253 (0.136)</td>\n",
       "      <td id=\"T_57889_row0_col2\" class=\"data row0 col2\" >0.408 (0.14)</td>\n",
       "      <td id=\"T_57889_row0_col3\" class=\"data row0 col3\" >0.395 (0.027)</td>\n",
       "      <td id=\"T_57889_row0_col4\" class=\"data row0 col4\" >0.51 (0.091)</td>\n",
       "      <td id=\"T_57889_row0_col5\" class=\"data row0 col5\" >0.282 (0.032)</td>\n",
       "      <td id=\"T_57889_row0_col6\" class=\"data row0 col6\" >0.435 (0.113)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x298e75d54c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For LR\n",
    "models = [\"Baseline\"]+[i for i in results_encodings[0].keys() if (\"tuned\" in i and \"LR\" in i)]\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "use_df = pd.DataFrame([pd.DataFrame(results_encodings[fold_num]).loc[metric,models] for fold_num in results_encodings.keys()],index=results_encodings.keys())\n",
    "\n",
    "df_mean = pd.DataFrame((use_df).mean(axis=0).round(3).astype(str) + \" (\" + use_df.std(axis=0).round(3).astype(str) + \")\").transpose()\n",
    "model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "\n",
    "best_model = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "t_test_res = np.array([stats.ttest_rel(use_df[best_model].values, use_df[model].values)[1] for model in models]).round(3)\n",
    "t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    \n",
    "res_df_lr = pd.DataFrame([model_dict])\n",
    "\n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_res[i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "res_df_lr.style.apply(negative_bold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea7d97c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fce81_row0_col2, #T_fce81_row0_col3, #T_fce81_row0_col4, #T_fce81_row0_col6 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fce81\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fce81_level0_col0\" class=\"col_heading level0 col0\" >Baseline</th>\n",
       "      <th id=\"T_fce81_level0_col1\" class=\"col_heading level0 col1\" >XGB_ignore_tuned</th>\n",
       "      <th id=\"T_fce81_level0_col2\" class=\"col_heading level0 col2\" >XGB_ohe_tuned</th>\n",
       "      <th id=\"T_fce81_level0_col3\" class=\"col_heading level0 col3\" >XGB_target_tuned</th>\n",
       "      <th id=\"T_fce81_level0_col4\" class=\"col_heading level0 col4\" >XGB_ordinal_tuned</th>\n",
       "      <th id=\"T_fce81_level0_col5\" class=\"col_heading level0 col5\" >XGB_catboost_tuned</th>\n",
       "      <th id=\"T_fce81_level0_col6\" class=\"col_heading level0 col6\" >XGB_glmm_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fce81_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_fce81_row0_col0\" class=\"data row0 col0\" >0.145 (0.026)</td>\n",
       "      <td id=\"T_fce81_row0_col1\" class=\"data row0 col1\" >0.145 (0.026)</td>\n",
       "      <td id=\"T_fce81_row0_col2\" class=\"data row0 col2\" >0.21 (0.116)</td>\n",
       "      <td id=\"T_fce81_row0_col3\" class=\"data row0 col3\" >0.262 (0.105)</td>\n",
       "      <td id=\"T_fce81_row0_col4\" class=\"data row0 col4\" >0.312 (0.102)</td>\n",
       "      <td id=\"T_fce81_row0_col5\" class=\"data row0 col5\" >0.161 (0.027)</td>\n",
       "      <td id=\"T_fce81_row0_col6\" class=\"data row0 col6\" >0.254 (0.112)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x299c6a9a820>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For LR\n",
    "models = [\"Baseline\"]+[i for i in results_encodings[0].keys() if (\"tuned\" in i and \"XGB\" in i)]\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "use_df = pd.DataFrame([pd.DataFrame(results_encodings[fold_num]).loc[metric,models] for fold_num in results_encodings.keys()],index=results_encodings.keys())\n",
    "\n",
    "df_mean = pd.DataFrame((use_df).mean(axis=0).round(3).astype(str) + \" (\" + use_df.std(axis=0).round(3).astype(str) + \")\").transpose()\n",
    "model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "\n",
    "best_model = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "t_test_res = np.array([stats.ttest_rel(use_df[best_model].values, use_df[model].values)[1] for model in models]).round(3)\n",
    "t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    \n",
    "res_df_xgb = pd.DataFrame([model_dict])\n",
    "\n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_res[i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "res_df_xgb.style.apply(negative_bold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f5b6b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline</th>\n",
       "      <th>ignore</th>\n",
       "      <th>ohe</th>\n",
       "      <th>target</th>\n",
       "      <th>ordinal</th>\n",
       "      <th>catboost</th>\n",
       "      <th>glmm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.145 (0.026)</td>\n",
       "      <td>0.253 (0.136)</td>\n",
       "      <td>0.408 (0.14)</td>\n",
       "      <td>0.395 (0.027)</td>\n",
       "      <td>0.51 (0.091)</td>\n",
       "      <td>0.282 (0.032)</td>\n",
       "      <td>0.435 (0.113)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>0.145 (0.026)</td>\n",
       "      <td>0.145 (0.026)</td>\n",
       "      <td>0.21 (0.116)</td>\n",
       "      <td>0.262 (0.105)</td>\n",
       "      <td>0.312 (0.102)</td>\n",
       "      <td>0.161 (0.027)</td>\n",
       "      <td>0.254 (0.112)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Baseline         ignore           ohe         target        ordinal  \\\n",
       "LR   0.145 (0.026)  0.253 (0.136)  0.408 (0.14)  0.395 (0.027)   0.51 (0.091)   \n",
       "XGB  0.145 (0.026)  0.145 (0.026)  0.21 (0.116)  0.262 (0.105)  0.312 (0.102)   \n",
       "\n",
       "          catboost           glmm  \n",
       "LR   0.282 (0.032)  0.435 (0.113)  \n",
       "XGB  0.161 (0.027)  0.254 (0.112)  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df_lr.columns = [i.split(\"_\")[1] if i != \"Baseline\" else \"Baseline\" for i in res_df_lr.columns]    \n",
    "res_df_xgb.columns = [i.split(\"_\")[1] if i != \"Baseline\" else \"Baseline\" for i in res_df_xgb.columns]    \n",
    "\n",
    "latex_df_encodings = pd.concat([res_df_lr,res_df_xgb],axis=0)\n",
    "latex_df_encodings.index = [\"LR\", \"XGB\"]\n",
    "latex_df_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cbae6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "{} &       Baseline &         ignore &           ohe &         target &        ordinal &       catboost &           glmm \\\\\n",
      "\\midrule\n",
      "LR  &  0.145 (0.026) &  0.253 (0.136) &  0.408 (0.14) &  0.395 (0.027) &   0.51 (0.091) &  0.282 (0.032) &  0.435 (0.113) \\\\\n",
      "XGB &  0.145 (0.026) &  0.145 (0.026) &  0.21 (0.116) &  0.262 (0.105) &  0.312 (0.102) &  0.161 (0.027) &  0.254 (0.112) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_df_encodings.round(2).to_latex())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b3d5a4",
   "metadata": {},
   "source": [
    "## Data Subset Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b88fd232",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = {\"demo_only\": demographic_cols,\n",
    "           \"performance_only\": perf_cols,\n",
    "           \"activity_only\": activity_cols,\n",
    "           \"activity_and_demo\": activity_cols+demographic_cols,\n",
    "           \"performance_and_demo\": perf_cols+demographic_cols,\n",
    "           \"all\": list(df.columns)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73afc34e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing results for fold 0, subset=demo_only\n",
      "SCORE: 0.9153996771988113                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 19.60trial/s, best loss: 0.9153996771988113]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.752515516368836}\n",
      "Default performance on Test: 1.854904579092599\n",
      "SCORE: 1.2706343938526703                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.70trial/s, best loss: 1.2706343938526703]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.20423751643165355, 'n_estimators': 267.0}\n",
      "Test Performance after first tuning round: 2.06854902737716\n",
      "SCORE: 1.295591273318283                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.21trial/s, best loss: 1.295591273318283]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.20423751643165355, 'n_estimators': 267.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 2.0}\n",
      "Test Performance after second tuning round: 2.0649672368748955\n",
      "SCORE: 1.2041480683402823                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.21trial/s, best loss: 1.2041480683402823]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.20423751643165355, 'n_estimators': 267.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.7044205476292276, 'subsample': 0.570831904193142}\n",
      "Test Performance after third tuning round: 1.592873545564567\n",
      "SCORE: 1.3716585267750516                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.52trial/s, best loss: 1.3716585267750516]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.20423751643165355, 'n_estimators': 267.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.7044205476292276, 'subsample': 0.570831904193142, 'gamma': 4.4456054543324255, 'reg_alpha': 5.0, 'reg_lambda': 2.1615602016590882}\n",
      "Test Performance after last tuning round: 1.2934613137097994\n",
      "Preparing results for fold 0, subset=performance_only\n",
      "SCORE: 1.0677377106015156                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 29.40trial/s, best loss: 1.0677377106015156]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.8603363226520115}\n",
      "Default performance on Test: 1.2294477812285782\n",
      "SCORE: 1.01199218519944                                                                                                \n",
      "100%|████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.90trial/s, best loss: 1.01199218519944]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.23592077406173084, 'n_estimators': 177.0}\n",
      "Test Performance after first tuning round: 1.2654051881467225\n",
      "SCORE: 0.9995824456068657                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.88trial/s, best loss: 0.9995824456068657]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.23592077406173084, 'n_estimators': 177.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 1.163080752694872\n",
      "SCORE: 1.0249355251811418                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.32trial/s, best loss: 1.0249355251811418]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.23592077406173084, 'n_estimators': 177.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.7044546583756908, 'subsample': 0.5512754922466474}\n",
      "Test Performance after third tuning round: 1.0359549304866797\n",
      "SCORE: 1.3629880362364326                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.55trial/s, best loss: 1.3629880362364326]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.23592077406173084, 'n_estimators': 177.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.7044546583756908, 'subsample': 0.5512754922466474, 'gamma': 8.74552712704713, 'reg_alpha': 3.0, 'reg_lambda': 2.989992703379658}\n",
      "Test Performance after last tuning round: 1.2743604892992677\n",
      "Preparing results for fold 0, subset=activity_only\n",
      "SCORE: 1.0196609701499058                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 31.22trial/s, best loss: 1.0196609701499058]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.7273113715008113}\n",
      "Default performance on Test: 0.9013724433184825\n",
      "SCORE: 1.2655956189855642                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.67trial/s, best loss: 1.2655956189855642]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.20380549914847418, 'n_estimators': 186.0}\n",
      "Test Performance after first tuning round: 0.9020096923600263\n",
      "SCORE: 1.1501555137004476                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.02trial/s, best loss: 1.1501555137004476]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.20380549914847418, 'n_estimators': 186.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 9.0}\n",
      "Test Performance after second tuning round: 0.9986717497762917\n",
      "SCORE: 1.2123879443102148                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.94trial/s, best loss: 1.2123879443102148]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.20380549914847418, 'n_estimators': 186.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.9189102056161287, 'subsample': 0.6855380996425319}\n",
      "Test Performance after third tuning round: 1.0391908299419346\n",
      "SCORE: 1.361540045431211                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.93trial/s, best loss: 1.361540045431211]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.20380549914847418, 'n_estimators': 186.0, 'seed': 0, 'max_depth': 1.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.9189102056161287, 'subsample': 0.6855380996425319, 'gamma': 2.816005841360362, 'reg_alpha': 2.0, 'reg_lambda': 3.687906380594798}\n",
      "Test Performance after last tuning round: 1.2403773074204472\n",
      "Preparing results for fold 0, subset=activity_and_demo\n",
      "SCORE: 0.9492627067760389                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.51trial/s, best loss: 0.9492627067760389]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.8873201326150497}\n",
      "Default performance on Test: 1.2919006119384973\n",
      "SCORE: 1.2575989977278585                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.59trial/s, best loss: 1.2575989977278585]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3612664035823087, 'n_estimators': 88.0}\n",
      "Test Performance after first tuning round: 1.2416659876976597\n",
      "SCORE: 1.1081276782436131                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.36trial/s, best loss: 1.1081276782436131]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3612664035823087, 'n_estimators': 88.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 4.0}\n",
      "Test Performance after second tuning round: 1.0409916433547988\n",
      "SCORE: 1.0987592671820605                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.65trial/s, best loss: 1.0987592671820605]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3612664035823087, 'n_estimators': 88.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.88785368647772, 'subsample': 0.6311315095369214}\n",
      "Test Performance after third tuning round: 1.025262927672765\n",
      "SCORE: 1.3228139898213516                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.06trial/s, best loss: 1.3228139898213516]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3612664035823087, 'n_estimators': 88.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.88785368647772, 'subsample': 0.6311315095369214, 'gamma': 1.6449721291281132, 'reg_alpha': 4.0, 'reg_lambda': 2.087362484949363}\n",
      "Test Performance after last tuning round: 1.2424224016415684\n",
      "Preparing results for fold 0, subset=performance_and_demo\n",
      "SCORE: 0.8639127124071333                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.06trial/s, best loss: 0.8639127124071333]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.23870843914779144}\n",
      "Default performance on Test: 1.2380763252371318\n",
      "SCORE: 1.3111052125212748                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.94trial/s, best loss: 1.3111052125212748]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.014349859275383032, 'n_estimators': 148.0}\n",
      "Test Performance after first tuning round: 1.045285262615132\n",
      "SCORE: 1.3071444019489324                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.39trial/s, best loss: 1.3071444019489324]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.014349859275383032, 'n_estimators': 148.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 4.0}\n",
      "Test Performance after second tuning round: 1.0535681507048185\n",
      "SCORE: 1.3184480642399827                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.95trial/s, best loss: 1.3184480642399827]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.014349859275383032, 'n_estimators': 148.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.7197350503547848, 'subsample': 0.6371427478227649}\n",
      "Test Performance after third tuning round: 1.0425209385067702\n",
      "SCORE: 1.3844863470681752                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.46trial/s, best loss: 1.3844863470681752]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.014349859275383032, 'n_estimators': 148.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.7197350503547848, 'subsample': 0.6371427478227649, 'gamma': 7.419923455946045, 'reg_alpha': 2.0, 'reg_lambda': 3.6846619585120695}\n",
      "Test Performance after last tuning round: 1.2921746039355535\n",
      "Preparing results for fold 0, subset=all\n",
      "SCORE: 0.8756284461355912                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.38trial/s, best loss: 0.8756284461355912]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.8430588493199732}\n",
      "Default performance on Test: 0.8796817602377536\n",
      "SCORE: 1.3429732632738414                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.85trial/s, best loss: 1.3429732632738414]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.013662488085283388, 'n_estimators': 286.0}\n",
      "Test Performance after first tuning round: 0.9440692868807462\n",
      "SCORE: 1.2976821269914758                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.85trial/s, best loss: 1.2976821269914758]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.013662488085283388, 'n_estimators': 286.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 0.8959957633465352\n",
      "SCORE: 1.3002329078223007                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.03trial/s, best loss: 1.3002329078223007]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.013662488085283388, 'n_estimators': 286.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.8765773565902487, 'subsample': 0.5382377731440384}\n",
      "Test Performance after third tuning round: 0.9422068471932161\n",
      "SCORE: 1.3853696134198992                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.35trial/s, best loss: 1.3853696134198992]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.013662488085283388, 'n_estimators': 286.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.8765773565902487, 'subsample': 0.5382377731440384, 'gamma': 6.270007881881675, 'reg_alpha': 5.0, 'reg_lambda': 3.1272678469195725}\n",
      "Test Performance after last tuning round: 1.3179106040368187\n",
      "Preparing results for fold 1, subset=demo_only\n",
      "SCORE: 0.8804784500831582                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 20.00trial/s, best loss: 0.8804784500831582]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.8658346718237225}\n",
      "Default performance on Test: 2.1326380927072757\n",
      "SCORE: 1.229462814514712                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.08trial/s, best loss: 1.229462814514712]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.37835321399535604, 'n_estimators': 135.0}\n",
      "Test Performance after first tuning round: 2.325222763280091\n",
      "SCORE: 1.11327717880556                                                                                                \n",
      "100%|████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.43trial/s, best loss: 1.11327717880556]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.37835321399535604, 'n_estimators': 135.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 9.0}\n",
      "Test Performance after second tuning round: 1.7254895035622833\n",
      "SCORE: 1.083997892184359                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.61trial/s, best loss: 1.083997892184359]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.37835321399535604, 'n_estimators': 135.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.950401788304069, 'subsample': 0.9768179513646851}\n",
      "Test Performance after third tuning round: 1.7256710471972303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 1.3620049240405963                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.75trial/s, best loss: 1.3620049240405963]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.37835321399535604, 'n_estimators': 135.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 9.0, 'colsample_bytree': 0.950401788304069, 'subsample': 0.9768179513646851, 'gamma': 1.568581674379195, 'reg_alpha': 9.0, 'reg_lambda': 2.987315422198936}\n",
      "Test Performance after last tuning round: 1.3196863195580137\n",
      "Preparing results for fold 1, subset=performance_only\n",
      "SCORE: 1.1553035210403073                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.45trial/s, best loss: 1.1553035210403073]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.06539897457192942}\n",
      "Default performance on Test: 0.9558969310816731\n",
      "SCORE: 0.9510306683903                                                                                                 \n",
      "100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.52trial/s, best loss: 0.9510306683903]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.1741713264180112, 'n_estimators': 378.0}\n",
      "Test Performance after first tuning round: 0.9861326346355935\n",
      "SCORE: 1.1140364404978569                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.88trial/s, best loss: 1.1140364404978569]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.1741713264180112, 'n_estimators': 378.0, 'seed': 0, 'max_depth': 18.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 0.7544726713579214\n",
      "SCORE: 1.1298809108241816                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.80trial/s, best loss: 1.1298809108241816]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.1741713264180112, 'n_estimators': 378.0, 'seed': 0, 'max_depth': 18.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.6634606607345808, 'subsample': 0.8523099304104989}\n",
      "Test Performance after third tuning round: 0.8310915704853384\n",
      "SCORE: 1.339598264720016                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.87trial/s, best loss: 1.339598264720016]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.1741713264180112, 'n_estimators': 378.0, 'seed': 0, 'max_depth': 18.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.6634606607345808, 'subsample': 0.8523099304104989, 'gamma': 3.924764886767015, 'reg_alpha': 3.0, 'reg_lambda': 1.0808329816606594}\n",
      "Test Performance after last tuning round: 1.323828700398948\n",
      "Preparing results for fold 1, subset=activity_only\n",
      "SCORE: 1.0438909547936592                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 33.21trial/s, best loss: 1.0438909547936592]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.4508501830378313}\n",
      "Default performance on Test: 1.6297069794911663\n",
      "SCORE: 1.166037792514754                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.81trial/s, best loss: 1.166037792514754]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4078027520587052, 'n_estimators': 114.0}\n",
      "Test Performance after first tuning round: 1.7177813165142886\n",
      "SCORE: 1.0998738935488157                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.33trial/s, best loss: 1.0998738935488157]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4078027520587052, 'n_estimators': 114.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 1.5522692742203217\n",
      "SCORE: 1.123275182821097                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.09trial/s, best loss: 1.123275182821097]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4078027520587052, 'n_estimators': 114.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.5377174399903712, 'subsample': 0.7047798080948604}\n",
      "Test Performance after third tuning round: 1.298754759171404\n",
      "SCORE: 1.3378500514000955                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.69trial/s, best loss: 1.3378500514000955]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4078027520587052, 'n_estimators': 114.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.5377174399903712, 'subsample': 0.7047798080948604, 'gamma': 5.984262524115652, 'reg_alpha': 4.0, 'reg_lambda': 1.4406627521928606}\n",
      "Test Performance after last tuning round: 1.3896563263357333\n",
      "Preparing results for fold 1, subset=activity_and_demo\n",
      "SCORE: 0.8197810937867333                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 21.73trial/s, best loss: 0.8197810937867333]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.7279506816671062}\n",
      "Default performance on Test: 2.0269207383763854\n",
      "SCORE: 1.2354566712696482                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.59trial/s, best loss: 1.2354566712696482]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.2228641001154655, 'n_estimators': 119.0}\n",
      "Test Performance after first tuning round: 1.9398052054632626\n",
      "SCORE: 1.1624072790279258                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.40trial/s, best loss: 1.1624072790279258]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.2228641001154655, 'n_estimators': 119.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 1.7534834655772966\n",
      "SCORE: 1.1092230324623769                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.33trial/s, best loss: 1.1092230324623769]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.2228641001154655, 'n_estimators': 119.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.9001234438964152, 'subsample': 0.8784137248981951}\n",
      "Test Performance after third tuning round: 1.7794010542126926\n",
      "SCORE: 1.2929288234594918                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.25trial/s, best loss: 1.2929288234594918]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.2228641001154655, 'n_estimators': 119.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.9001234438964152, 'subsample': 0.8784137248981951, 'gamma': 4.486844289714159, 'reg_alpha': 0.0, 'reg_lambda': 3.271891516150606}\n",
      "Test Performance after last tuning round: 1.3410271097118351\n",
      "Preparing results for fold 1, subset=performance_and_demo\n",
      "SCORE: 0.8939727871634048                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 16.95trial/s, best loss: 0.8939727871634048]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.8877026496083877}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default performance on Test: 1.1075855774280081\n",
      "SCORE: 1.0977278099607708                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.72trial/s, best loss: 1.0977278099607708]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.23883918530327486, 'n_estimators': 173.0}\n",
      "Test Performance after first tuning round: 1.1118675036562555\n",
      "SCORE: 1.0723309246938224                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.72trial/s, best loss: 1.0723309246938224]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.23883918530327486, 'n_estimators': 173.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 1.0314023369805776\n",
      "SCORE: 0.9856608072042207                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.15trial/s, best loss: 0.9856608072042207]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.23883918530327486, 'n_estimators': 173.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.9036747050436529, 'subsample': 0.860903198483459}\n",
      "Test Performance after third tuning round: 1.1838784560905526\n",
      "SCORE: 1.3252798466399387                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.58trial/s, best loss: 1.3252798466399387]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.23883918530327486, 'n_estimators': 173.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.9036747050436529, 'subsample': 0.860903198483459, 'gamma': 4.365204769007925, 'reg_alpha': 3.0, 'reg_lambda': 1.9359906023836375}\n",
      "Test Performance after last tuning round: 1.3396066615026843\n",
      "Preparing results for fold 1, subset=all\n",
      "SCORE: 0.8412739708931876                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 19.19trial/s, best loss: 0.8412739708931876]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.40957659589991824}\n",
      "Default performance on Test: 1.3348602051794896\n",
      "SCORE: 1.0227653406241761                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.27trial/s, best loss: 1.0227653406241761]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.2826580526734724, 'n_estimators': 275.0}\n",
      "Test Performance after first tuning round: 1.5007508263127822\n",
      "SCORE: 1.005788592425545                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.07trial/s, best loss: 1.005788592425545]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.2826580526734724, 'n_estimators': 275.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 10.0}\n",
      "Test Performance after second tuning round: 1.297193223413068\n",
      "SCORE: 1.1104348743302621                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.83trial/s, best loss: 1.1104348743302621]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.2826580526734724, 'n_estimators': 275.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.6177589450976803, 'subsample': 0.5886498531675494}\n",
      "Test Performance after third tuning round: 1.5171899067786794\n",
      "SCORE: 1.379380353735165                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.57trial/s, best loss: 1.379380353735165]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.2826580526734724, 'n_estimators': 275.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.6177589450976803, 'subsample': 0.5886498531675494, 'gamma': 1.5228742523667134, 'reg_alpha': 9.0, 'reg_lambda': 3.118019845752298}\n",
      "Test Performance after last tuning round: 1.3351549370110907\n",
      "Preparing results for fold 2, subset=demo_only\n",
      "SCORE: 0.8914144122720303                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 27.77trial/s, best loss: 0.8914144122720303]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.2348714346833531}\n",
      "Default performance on Test: 1.756709147078612\n",
      "SCORE: 1.274046196542699                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.55trial/s, best loss: 1.274046196542699]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.49482336609987787, 'n_estimators': 205.0}\n",
      "Test Performance after first tuning round: 2.1021949416146173\n",
      "SCORE: 1.196351316087778                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.95trial/s, best loss: 1.196351316087778]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.49482336609987787, 'n_estimators': 205.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 1.8588359052745553\n",
      "SCORE: 1.1510614534354755                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.88trial/s, best loss: 1.1510614534354755]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.49482336609987787, 'n_estimators': 205.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.660466830851169, 'subsample': 0.6537250731788925}\n",
      "Test Performance after third tuning round: 1.4891419487905133\n",
      "SCORE: 1.2605903707100334                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.52trial/s, best loss: 1.2605903707100334]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.49482336609987787, 'n_estimators': 205.0, 'seed': 0, 'max_depth': 15.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.660466830851169, 'subsample': 0.6537250731788925, 'gamma': 1.8684902481298953, 'reg_alpha': 1.0, 'reg_lambda': 3.955060353290058}\n",
      "Test Performance after last tuning round: 1.0755782313748192\n",
      "Preparing results for fold 2, subset=performance_only\n",
      "SCORE: 1.0543160377118084                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 33.95trial/s, best loss: 1.0543160377118084]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.40476976905297224}\n",
      "Default performance on Test: 1.1960074571118484\n",
      "SCORE: 1.3491958790906184                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.33trial/s, best loss: 1.3491958790906184]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.013346486186740673, 'n_estimators': 448.0}\n",
      "Test Performance after first tuning round: 1.004014317408774\n",
      "SCORE: 1.3775863588939674                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.93trial/s, best loss: 1.3775863588939674]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.013346486186740673, 'n_estimators': 448.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 2.0}\n",
      "Test Performance after second tuning round: 0.9444617705996629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 1.3213906321473299                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.78trial/s, best loss: 1.3213906321473299]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.013346486186740673, 'n_estimators': 448.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.9061075416898151, 'subsample': 0.6855037946075293}\n",
      "Test Performance after third tuning round: 0.9314471495545913\n",
      "SCORE: 1.3860084470426597                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.85trial/s, best loss: 1.3860084470426597]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.013346486186740673, 'n_estimators': 448.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.9061075416898151, 'subsample': 0.6855037946075293, 'gamma': 8.45834124702508, 'reg_alpha': 8.0, 'reg_lambda': 1.3250894555106436}\n",
      "Test Performance after last tuning round: 1.2828114459646638\n",
      "Preparing results for fold 2, subset=activity_only\n",
      "SCORE: 1.0290072440129654                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.60trial/s, best loss: 1.0290072440129654]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.9572308772401007}\n",
      "Default performance on Test: 1.1146397511464707\n",
      "SCORE: 1.3404306700119217                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.17trial/s, best loss: 1.3404306700119217]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.03905228606463928, 'n_estimators': 417.0}\n",
      "Test Performance after first tuning round: 1.1003421196314134\n",
      "SCORE: 1.3467985973361918                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.32trial/s, best loss: 1.3467985973361918]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.03905228606463928, 'n_estimators': 417.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 4.0}\n",
      "Test Performance after second tuning round: 1.051139640403151\n",
      "SCORE: 1.327690562698658                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.85trial/s, best loss: 1.327690562698658]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.03905228606463928, 'n_estimators': 417.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.8860524882209633, 'subsample': 0.9085288921841691}\n",
      "Test Performance after third tuning round: 1.0383161161050363\n",
      "SCORE: 1.3835946632535823                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.33trial/s, best loss: 1.3835946632535823]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.03905228606463928, 'n_estimators': 417.0, 'seed': 0, 'max_depth': 4.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.8860524882209633, 'subsample': 0.9085288921841691, 'gamma': 1.6997139718343186, 'reg_alpha': 5.0, 'reg_lambda': 2.7936984202233295}\n",
      "Test Performance after last tuning round: 1.1951649321463425\n",
      "Preparing results for fold 2, subset=activity_and_demo\n",
      "SCORE: 0.8725259684101925                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.51trial/s, best loss: 0.8725259684101925]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.6965531644630256}\n",
      "Default performance on Test: 1.4305212835213477\n",
      "SCORE: 1.2849541652256893                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.74trial/s, best loss: 1.2849541652256893]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3365969819007312, 'n_estimators': 176.0}\n",
      "Test Performance after first tuning round: 1.5497772105751755\n",
      "SCORE: 1.1461551678126551                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.35trial/s, best loss: 1.1461551678126551]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3365969819007312, 'n_estimators': 176.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 4.0}\n",
      "Test Performance after second tuning round: 1.3969752326433205\n",
      "SCORE: 1.0743280483596074                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.21trial/s, best loss: 1.0743280483596074]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3365969819007312, 'n_estimators': 176.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.6241709359551835, 'subsample': 0.8076478058892811}\n",
      "Test Performance after third tuning round: 1.2771708347225317\n",
      "SCORE: 1.3664036483556365                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.30trial/s, best loss: 1.3664036483556365]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3365969819007312, 'n_estimators': 176.0, 'seed': 0, 'max_depth': 10.0, 'min_child_weight': 4.0, 'colsample_bytree': 0.6241709359551835, 'subsample': 0.8076478058892811, 'gamma': 5.085663026469713, 'reg_alpha': 5.0, 'reg_lambda': 3.7923148185355955}\n",
      "Test Performance after last tuning round: 1.166977449702866\n",
      "Preparing results for fold 2, subset=performance_and_demo\n",
      "SCORE: 0.837456654995765                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00, 21.23trial/s, best loss: 0.837456654995765]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.4101031831995533}\n",
      "Default performance on Test: 1.2649368103575855\n",
      "SCORE: 1.071229648729244                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.13trial/s, best loss: 1.071229648729244]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4780135488423315, 'n_estimators': 277.0}\n",
      "Test Performance after first tuning round: 1.5934972820518378\n",
      "SCORE: 1.0239358539298888                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.60trial/s, best loss: 1.0239358539298888]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4780135488423315, 'n_estimators': 277.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 1.6702465491606506\n",
      "SCORE: 0.9640140167447109                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.36trial/s, best loss: 0.9640140167447109]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4780135488423315, 'n_estimators': 277.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.7646107130398451, 'subsample': 0.7147817110915442}\n",
      "Test Performance after third tuning round: 1.5122604240831015\n",
      "SCORE: 1.3758897228637417                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.52trial/s, best loss: 1.3758897228637417]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4780135488423315, 'n_estimators': 277.0, 'seed': 0, 'max_depth': 5.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.7646107130398451, 'subsample': 0.7147817110915442, 'gamma': 7.390889554513301, 'reg_alpha': 8.0, 'reg_lambda': 3.6581229941320412}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance after last tuning round: 1.2151936686244715\n",
      "Preparing results for fold 2, subset=all\n",
      "SCORE: 0.8510820710720359                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 22.22trial/s, best loss: 0.8510820710720359]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.1644399714249691}\n",
      "Default performance on Test: 0.9692623948336883\n",
      "SCORE: 1.198768694516285                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.91trial/s, best loss: 1.198768694516285]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.05673378115449451, 'n_estimators': 374.0}\n",
      "Test Performance after first tuning round: 0.9650246930649521\n",
      "SCORE: 1.1025354957666278                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.62trial/s, best loss: 1.1025354957666278]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.05673378115449451, 'n_estimators': 374.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 0.9454465796847172\n",
      "SCORE: 1.2281540447575496                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.05trial/s, best loss: 1.2281540447575496]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.05673378115449451, 'n_estimators': 374.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.8006377262128599, 'subsample': 0.9020599732810667}\n",
      "Test Performance after third tuning round: 0.9587968568294865\n",
      "SCORE: 1.361058648180808                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.78trial/s, best loss: 1.361058648180808]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.05673378115449451, 'n_estimators': 374.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.8006377262128599, 'subsample': 0.9020599732810667, 'gamma': 7.233450794355435, 'reg_alpha': 3.0, 'reg_lambda': 1.268629055038807}\n",
      "Test Performance after last tuning round: 1.0927297609156952\n",
      "Preparing results for fold 3, subset=demo_only\n",
      "SCORE: 0.936352799204155                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00, 19.67trial/s, best loss: 0.936352799204155]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.8796816221273525}\n",
      "Default performance on Test: 1.962718365359898\n",
      "SCORE: 1.224959192519827                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.35trial/s, best loss: 1.224959192519827]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4294558565532756, 'n_estimators': 432.0}\n",
      "Test Performance after first tuning round: 2.1491423896101605\n",
      "SCORE: 1.195518438053757                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.25trial/s, best loss: 1.195518438053757]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4294558565532756, 'n_estimators': 432.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 1.7474857491884548\n",
      "SCORE: 1.1823051554945514                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.41trial/s, best loss: 1.1823051554945514]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4294558565532756, 'n_estimators': 432.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.8876534607303723, 'subsample': 0.7353579570126327}\n",
      "Test Performance after third tuning round: 1.403854451135364\n",
      "SCORE: 1.3775681338430332                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.93trial/s, best loss: 1.3775681338430332]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4294558565532756, 'n_estimators': 432.0, 'seed': 0, 'max_depth': 3.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.8876534607303723, 'subsample': 0.7353579570126327, 'gamma': 7.787959130245732, 'reg_alpha': 9.0, 'reg_lambda': 3.9431359337150185}\n",
      "Test Performance after last tuning round: 1.2639425063509409\n",
      "Preparing results for fold 3, subset=performance_only\n",
      "SCORE: 1.1221301062699378                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 39.99trial/s, best loss: 1.1221301062699378]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.1865933286732758}\n",
      "Default performance on Test: 1.152061821820626\n",
      "SCORE: 1.0171377098934247                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.99trial/s, best loss: 1.0171377098934247]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.17791405509580385, 'n_estimators': 423.0}\n",
      "Test Performance after first tuning round: 1.2539546545966733\n",
      "SCORE: 1.0811795339586585                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.62trial/s, best loss: 1.0811795339586585]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.17791405509580385, 'n_estimators': 423.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 8.0}\n",
      "Test Performance after second tuning round: 1.0042910225294222\n",
      "SCORE: 1.1706250143159356                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.66trial/s, best loss: 1.1706250143159356]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.17791405509580385, 'n_estimators': 423.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.8788164024236631, 'subsample': 0.8608534914484824}\n",
      "Test Performance after third tuning round: 1.046220823978747\n",
      "SCORE: 1.3412625909936196                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.40trial/s, best loss: 1.3412625909936196]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.17791405509580385, 'n_estimators': 423.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.8788164024236631, 'subsample': 0.8608534914484824, 'gamma': 0.8913313421641046, 'reg_alpha': 5.0, 'reg_lambda': 3.9402514932872585}\n",
      "Test Performance after last tuning round: 1.1340251052231762\n",
      "Preparing results for fold 3, subset=activity_only\n",
      "SCORE: 1.002745404150884                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00, 29.41trial/s, best loss: 1.002745404150884]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.706748557181119}\n",
      "Default performance on Test: 1.451064506181635\n",
      "SCORE: 1.3459484382618                                                                                                 \n",
      "100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.04trial/s, best loss: 1.3459484382618]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.07738025912171735, 'n_estimators': 115.0}\n",
      "Test Performance after first tuning round: 1.4376593769987447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 1.2954518427502504                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.71trial/s, best loss: 1.2954518427502504]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.07738025912171735, 'n_estimators': 115.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 0.0}\n",
      "Test Performance after second tuning round: 1.5090994040566947\n",
      "SCORE: 1.2709296021029577                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.53trial/s, best loss: 1.2709296021029577]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.07738025912171735, 'n_estimators': 115.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 0.0, 'colsample_bytree': 0.6203765151150111, 'subsample': 0.903117188348646}\n",
      "Test Performance after third tuning round: 1.2076811485461973\n",
      "SCORE: 1.3794504554225264                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.26trial/s, best loss: 1.3794504554225264]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.07738025912171735, 'n_estimators': 115.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 0.0, 'colsample_bytree': 0.6203765151150111, 'subsample': 0.903117188348646, 'gamma': 4.980865383932804, 'reg_alpha': 5.0, 'reg_lambda': 2.4870975453433504}\n",
      "Test Performance after last tuning round: 1.2407161090241123\n",
      "Preparing results for fold 3, subset=activity_and_demo\n",
      "SCORE: 0.8994385227063774                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.28trial/s, best loss: 0.8994385227063774]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.6321293608436241}\n",
      "Default performance on Test: 1.7538617215816024\n",
      "SCORE: 1.204350975788737                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.18trial/s, best loss: 1.204350975788737]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.16531249139485443, 'n_estimators': 427.0}\n",
      "Test Performance after first tuning round: 1.850161274268505\n",
      "SCORE: 1.1882720943153244                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.37trial/s, best loss: 1.1882720943153244]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.16531249139485443, 'n_estimators': 427.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 1.242874419499769\n",
      "SCORE: 1.145775872214592                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.21trial/s, best loss: 1.145775872214592]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.16531249139485443, 'n_estimators': 427.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.6972833491103423, 'subsample': 0.7074399300729974}\n",
      "Test Performance after third tuning round: 1.0824310813549614\n",
      "SCORE: 1.376756592819326                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.52trial/s, best loss: 1.376756592819326]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.16531249139485443, 'n_estimators': 427.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.6972833491103423, 'subsample': 0.7074399300729974, 'gamma': 3.680076270702347, 'reg_alpha': 6.0, 'reg_lambda': 1.9333621211208847}\n",
      "Test Performance after last tuning round: 1.223418421731726\n",
      "Preparing results for fold 3, subset=performance_and_demo\n",
      "SCORE: 0.8945739469058905                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.51trial/s, best loss: 0.8945739469058905]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.7659705977235252}\n",
      "Default performance on Test: 1.716247501892855\n",
      "SCORE: 1.0812126570074265                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.53trial/s, best loss: 1.0812126570074265]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.05139313275109914, 'n_estimators': 218.0}\n",
      "Test Performance after first tuning round: 1.460950915410539\n",
      "SCORE: 1.1167454774155399                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.44trial/s, best loss: 1.1167454774155399]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.05139313275109914, 'n_estimators': 218.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 3.0}\n",
      "Test Performance after second tuning round: 1.4127994411949591\n",
      "SCORE: 1.1840813237513463                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.95trial/s, best loss: 1.1840813237513463]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.05139313275109914, 'n_estimators': 218.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.8380625691923649, 'subsample': 0.9971249799292274}\n",
      "Test Performance after third tuning round: 1.361862853464804\n",
      "SCORE: 1.3721834558672767                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.80trial/s, best loss: 1.3721834558672767]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.05139313275109914, 'n_estimators': 218.0, 'seed': 0, 'max_depth': 13.0, 'min_child_weight': 3.0, 'colsample_bytree': 0.8380625691923649, 'subsample': 0.9971249799292274, 'gamma': 3.5113554644625022, 'reg_alpha': 6.0, 'reg_lambda': 1.8288406547789076}\n",
      "Test Performance after last tuning round: 1.198365792462663\n",
      "Preparing results for fold 3, subset=all\n",
      "SCORE: 0.8631999357010812                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.54trial/s, best loss: 0.8631999357010812]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.6426172369860012}\n",
      "Default performance on Test: 1.6401217408592987\n",
      "SCORE: 0.9856216493003297                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.99trial/s, best loss: 0.9856216493003297]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4847358406569528, 'n_estimators': 377.0}\n",
      "Test Performance after first tuning round: 1.689843807092856\n",
      "SCORE: 1.000645025207256                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.48trial/s, best loss: 1.000645025207256]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4847358406569528, 'n_estimators': 377.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 1.0}\n",
      "Test Performance after second tuning round: 1.7504789862571881\n",
      "SCORE: 0.903171074709703                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.90trial/s, best loss: 0.903171074709703]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4847358406569528, 'n_estimators': 377.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.563415136376966, 'subsample': 0.8804889117626642}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance after third tuning round: 1.5995122314133046\n",
      "SCORE: 1.2746216466830336                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.33trial/s, best loss: 1.2746216466830336]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4847358406569528, 'n_estimators': 377.0, 'seed': 0, 'max_depth': 11.0, 'min_child_weight': 1.0, 'colsample_bytree': 0.563415136376966, 'subsample': 0.8804889117626642, 'gamma': 5.097567911277314, 'reg_alpha': 5.0, 'reg_lambda': 2.597477666618161}\n",
      "Test Performance after last tuning round: 1.167883695877595\n",
      "Preparing results for fold 4, subset=demo_only\n",
      "SCORE: 0.942201292674761                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00, 20.83trial/s, best loss: 0.942201292674761]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.5202693939174737}\n",
      "Default performance on Test: 1.7992881142012678\n",
      "SCORE: 1.2792232848375875                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.80trial/s, best loss: 1.2792232848375875]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.3764111985080789, 'n_estimators': 267.0}\n",
      "Test Performance after first tuning round: 2.1170567386293\n",
      "SCORE: 1.2812126126541719                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.67trial/s, best loss: 1.2812126126541719]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.3764111985080789, 'n_estimators': 267.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 6.0}\n",
      "Test Performance after second tuning round: 1.7059982549824182\n",
      "SCORE: 1.22144930007959                                                                                                \n",
      "100%|████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.21trial/s, best loss: 1.22144930007959]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.3764111985080789, 'n_estimators': 267.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.8328519498561977, 'subsample': 0.7082957460992647}\n",
      "Test Performance after third tuning round: 1.6006987425283088\n",
      "SCORE: 1.3151602189428524                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.09trial/s, best loss: 1.3151602189428524]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.3764111985080789, 'n_estimators': 267.0, 'seed': 0, 'max_depth': 17.0, 'min_child_weight': 6.0, 'colsample_bytree': 0.8328519498561977, 'subsample': 0.7082957460992647, 'gamma': 5.68052113482567, 'reg_alpha': 0.0, 'reg_lambda': 2.7859955379084136}\n",
      "Test Performance after last tuning round: 1.2049588590301517\n",
      "Preparing results for fold 4, subset=performance_only\n",
      "SCORE: 1.1882428364614999                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 35.71trial/s, best loss: 1.1882428364614999]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.021669481653401636}\n",
      "Default performance on Test: 0.9348961585097864\n",
      "SCORE: 1.0873344861975647                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.33trial/s, best loss: 1.0873344861975647]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.293911257832007, 'n_estimators': 361.0}\n",
      "Test Performance after first tuning round: 1.0283328106663623\n",
      "SCORE: 1.0316542014393497                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.04trial/s, best loss: 1.0316542014393497]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.293911257832007, 'n_estimators': 361.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 8.0}\n",
      "Test Performance after second tuning round: 0.8994291108750203\n",
      "SCORE: 1.1073837826640074                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.19trial/s, best loss: 1.1073837826640074]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.293911257832007, 'n_estimators': 361.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.6153257285220513, 'subsample': 0.7857267245729153}\n",
      "Test Performance after third tuning round: 1.0529036743511218\n",
      "SCORE: 1.1279505902920228                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.92trial/s, best loss: 1.1279505902920228]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.293911257832007, 'n_estimators': 361.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 8.0, 'colsample_bytree': 0.6153257285220513, 'subsample': 0.7857267245729153, 'gamma': 1.1538057110807223, 'reg_alpha': 1.0, 'reg_lambda': 2.7465833067354897}\n",
      "Test Performance after last tuning round: 1.040804805974937\n",
      "Preparing results for fold 4, subset=activity_only\n",
      "SCORE: 1.0911734406145157                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 30.30trial/s, best loss: 1.0911734406145157]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.6598213058376443}\n",
      "Default performance on Test: 1.1131246238361991\n",
      "SCORE: 1.2887049284828316                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.81trial/s, best loss: 1.2887049284828316]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.4853669072718257, 'n_estimators': 148.0}\n",
      "Test Performance after first tuning round: 1.1224131505781794\n",
      "SCORE: 1.153924841372881                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.19trial/s, best loss: 1.153924841372881]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.4853669072718257, 'n_estimators': 148.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 10.0}\n",
      "Test Performance after second tuning round: 1.0835467700863983\n",
      "SCORE: 1.2071992975069812                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.10trial/s, best loss: 1.2071992975069812]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.4853669072718257, 'n_estimators': 148.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.5776709943459778, 'subsample': 0.6188040366322993}\n",
      "Test Performance after third tuning round: 1.098397006864744\n",
      "SCORE: 1.3577513295270978                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.77trial/s, best loss: 1.3577513295270978]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.4853669072718257, 'n_estimators': 148.0, 'seed': 0, 'max_depth': 16.0, 'min_child_weight': 10.0, 'colsample_bytree': 0.5776709943459778, 'subsample': 0.6188040366322993, 'gamma': 1.9716082183343095, 'reg_alpha': 6.0, 'reg_lambda': 1.6482999241667236}\n",
      "Test Performance after last tuning round: 1.2688278919199816\n",
      "Preparing results for fold 4, subset=activity_and_demo\n",
      "SCORE: 0.9349540192697582                                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 19.99trial/s, best loss: 0.9349540192697582]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.42784316717513793}\n",
      "Default performance on Test: 1.5145929673704077\n",
      "SCORE: 1.2207554047365963                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.61trial/s, best loss: 1.2207554047365963]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.2576104966589553, 'n_estimators': 312.0}\n",
      "Test Performance after first tuning round: 1.6682766734209067\n",
      "SCORE: 1.2474876524095522                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.76trial/s, best loss: 1.2474876524095522]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.2576104966589553, 'n_estimators': 312.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 2.0}\n",
      "Test Performance after second tuning round: 1.504972703096877\n",
      "SCORE: 1.2114079335517711                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.40trial/s, best loss: 1.2114079335517711]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.2576104966589553, 'n_estimators': 312.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.69489489158576, 'subsample': 0.9202676568879505}\n",
      "Test Performance after third tuning round: 1.5505297896490295\n",
      "SCORE: 1.3530149494388528                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.89trial/s, best loss: 1.3530149494388528]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.2576104966589553, 'n_estimators': 312.0, 'seed': 0, 'max_depth': 8.0, 'min_child_weight': 2.0, 'colsample_bytree': 0.69489489158576, 'subsample': 0.9202676568879505, 'gamma': 4.933554650596138, 'reg_alpha': 2.0, 'reg_lambda': 3.678642307571667}\n",
      "Test Performance after last tuning round: 1.2474720735971823\n",
      "Preparing results for fold 4, subset=performance_and_demo\n",
      "SCORE: 0.9197938435238511                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.51trial/s, best loss: 0.9197938435238511]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.7964077763570888}\n",
      "Default performance on Test: 1.2872434787360503\n",
      "SCORE: 1.0942475074242826                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.38trial/s, best loss: 1.0942475074242826]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.23641439909996315, 'n_estimators': 171.0}\n",
      "Test Performance after first tuning round: 1.3693493994912715\n",
      "SCORE: 1.184018604084874                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.62trial/s, best loss: 1.184018604084874]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.23641439909996315, 'n_estimators': 171.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 7.0}\n",
      "Test Performance after second tuning round: 1.215627218561289\n",
      "SCORE: 1.0896381513525515                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.88trial/s, best loss: 1.0896381513525515]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.23641439909996315, 'n_estimators': 171.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.9482578656426063, 'subsample': 0.6501391462836742}\n",
      "Test Performance after third tuning round: 1.0452305051401285\n",
      "SCORE: 1.2565227872452098                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.95trial/s, best loss: 1.2565227872452098]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.23641439909996315, 'n_estimators': 171.0, 'seed': 0, 'max_depth': 9.0, 'min_child_weight': 7.0, 'colsample_bytree': 0.9482578656426063, 'subsample': 0.6501391462836742, 'gamma': 1.9165618766274506, 'reg_alpha': 4.0, 'reg_lambda': 3.3645030675865972}\n",
      "Test Performance after last tuning round: 1.1437544070474748\n",
      "Preparing results for fold 4, subset=all\n",
      "SCORE: 0.8919470067750674                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.51trial/s, best loss: 0.8919470067750674]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'C': 0.21645081716628972}\n",
      "Default performance on Test: 1.1726159523360842\n",
      "SCORE: 1.1425848433738346                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.24trial/s, best loss: 1.1425848433738346]\n",
      "The best hyperparameters after step 1  are :  \n",
      "\n",
      "{'learning_rate': 0.07819596733259647, 'n_estimators': 155.0}\n",
      "Test Performance after first tuning round: 1.0555513647332035\n",
      "SCORE: 1.1830649490038947                                                                                              \n",
      "100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.47trial/s, best loss: 1.1830649490038947]\n",
      "The best hyperparameters after step 2 are :  \n",
      "\n",
      "{'learning_rate': 0.07819596733259647, 'n_estimators': 155.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 5.0}\n",
      "Test Performance after second tuning round: 1.017833780308362\n",
      "SCORE: 1.096349431176054                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.59trial/s, best loss: 1.096349431176054]\n",
      "The best hyperparameters after step 3 are :  \n",
      "\n",
      "{'learning_rate': 0.07819596733259647, 'n_estimators': 155.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.691233023917237, 'subsample': 0.8229512408165425}\n",
      "Test Performance after third tuning round: 0.9643526713519385\n",
      "SCORE: 1.364100304085818                                                                                               \n",
      "100%|███████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.67trial/s, best loss: 1.364100304085818]\n",
      "The best hyperparameters are :  \n",
      "\n",
      "{'learning_rate': 0.07819596733259647, 'n_estimators': 155.0, 'seed': 0, 'max_depth': 14.0, 'min_child_weight': 5.0, 'colsample_bytree': 0.691233023917237, 'subsample': 0.8229512408165425, 'gamma': 0.7513914501651303, 'reg_alpha': 8.0, 'reg_lambda': 2.0222264587560654}\n",
      "Test Performance after last tuning round: 1.220107719229674\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3c678_row0_col4, #T_3c678_row1_col4, #T_3c678_row3_col5, #T_3c678_row4_col5, #T_3c678_row6_col0, #T_3c678_row6_col1, #T_3c678_row6_col2, #T_3c678_row6_col3, #T_3c678_row7_col3, #T_3c678_row8_col3, #T_3c678_row9_col0, #T_3c678_row9_col1, #T_3c678_row9_col2, #T_3c678_row13_col0, #T_3c678_row13_col1, #T_3c678_row13_col2, #T_3c678_row17_col0, #T_3c678_row17_col1, #T_3c678_row17_col2 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3c678\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3c678_level0_col0\" class=\"col_heading level0 col0\" >Accuracy Train</th>\n",
       "      <th id=\"T_3c678_level0_col1\" class=\"col_heading level0 col1\" >F1 Train</th>\n",
       "      <th id=\"T_3c678_level0_col2\" class=\"col_heading level0 col2\" >AUROC Train</th>\n",
       "      <th id=\"T_3c678_level0_col3\" class=\"col_heading level0 col3\" >Accuracy Test</th>\n",
       "      <th id=\"T_3c678_level0_col4\" class=\"col_heading level0 col4\" >F1 Test</th>\n",
       "      <th id=\"T_3c678_level0_col5\" class=\"col_heading level0 col5\" >AUROC Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row0\" class=\"row_heading level0 row0\" >LR_all_tuned</th>\n",
       "      <td id=\"T_3c678_row0_col0\" class=\"data row0 col0\" >0.855800</td>\n",
       "      <td id=\"T_3c678_row0_col1\" class=\"data row0 col1\" >0.848700</td>\n",
       "      <td id=\"T_3c678_row0_col2\" class=\"data row0 col2\" >0.968700</td>\n",
       "      <td id=\"T_3c678_row0_col3\" class=\"data row0 col3\" >0.592600</td>\n",
       "      <td id=\"T_3c678_row0_col4\" class=\"data row0 col4\" >0.686700</td>\n",
       "      <td id=\"T_3c678_row0_col5\" class=\"data row0 col5\" >0.884800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row1\" class=\"row_heading level0 row1\" >LR_all</th>\n",
       "      <td id=\"T_3c678_row1_col0\" class=\"data row1 col0\" >0.865400</td>\n",
       "      <td id=\"T_3c678_row1_col1\" class=\"data row1 col1\" >0.855400</td>\n",
       "      <td id=\"T_3c678_row1_col2\" class=\"data row1 col2\" >0.971900</td>\n",
       "      <td id=\"T_3c678_row1_col3\" class=\"data row1 col3\" >0.592600</td>\n",
       "      <td id=\"T_3c678_row1_col4\" class=\"data row1 col4\" >0.686700</td>\n",
       "      <td id=\"T_3c678_row1_col5\" class=\"data row1 col5\" >0.878300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row2\" class=\"row_heading level0 row2\" >LR_performance_and_demo</th>\n",
       "      <td id=\"T_3c678_row2_col0\" class=\"data row2 col0\" >0.836500</td>\n",
       "      <td id=\"T_3c678_row2_col1\" class=\"data row2 col1\" >0.831800</td>\n",
       "      <td id=\"T_3c678_row2_col2\" class=\"data row2 col2\" >0.958900</td>\n",
       "      <td id=\"T_3c678_row2_col3\" class=\"data row2 col3\" >0.592600</td>\n",
       "      <td id=\"T_3c678_row2_col4\" class=\"data row2 col4\" >0.680600</td>\n",
       "      <td id=\"T_3c678_row2_col5\" class=\"data row2 col5\" >0.859600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row3\" class=\"row_heading level0 row3\" >LR_performance_only</th>\n",
       "      <td id=\"T_3c678_row3_col0\" class=\"data row3 col0\" >0.567300</td>\n",
       "      <td id=\"T_3c678_row3_col1\" class=\"data row3 col1\" >0.489700</td>\n",
       "      <td id=\"T_3c678_row3_col2\" class=\"data row3 col2\" >0.762400</td>\n",
       "      <td id=\"T_3c678_row3_col3\" class=\"data row3 col3\" >0.555600</td>\n",
       "      <td id=\"T_3c678_row3_col4\" class=\"data row3 col4\" >0.637300</td>\n",
       "      <td id=\"T_3c678_row3_col5\" class=\"data row3 col5\" >0.898900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row4\" class=\"row_heading level0 row4\" >LR_performance_only_tuned</th>\n",
       "      <td id=\"T_3c678_row4_col0\" class=\"data row4 col0\" >0.567300</td>\n",
       "      <td id=\"T_3c678_row4_col1\" class=\"data row4 col1\" >0.489700</td>\n",
       "      <td id=\"T_3c678_row4_col2\" class=\"data row4 col2\" >0.759200</td>\n",
       "      <td id=\"T_3c678_row4_col3\" class=\"data row4 col3\" >0.555600</td>\n",
       "      <td id=\"T_3c678_row4_col4\" class=\"data row4 col4\" >0.637300</td>\n",
       "      <td id=\"T_3c678_row4_col5\" class=\"data row4 col5\" >0.898900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row5\" class=\"row_heading level0 row5\" >XGB_performance_only</th>\n",
       "      <td id=\"T_3c678_row5_col0\" class=\"data row5 col0\" >0.769200</td>\n",
       "      <td id=\"T_3c678_row5_col1\" class=\"data row5 col1\" >0.759800</td>\n",
       "      <td id=\"T_3c678_row5_col2\" class=\"data row5 col2\" >0.928800</td>\n",
       "      <td id=\"T_3c678_row5_col3\" class=\"data row5 col3\" >0.518500</td>\n",
       "      <td id=\"T_3c678_row5_col4\" class=\"data row5 col4\" >0.610200</td>\n",
       "      <td id=\"T_3c678_row5_col5\" class=\"data row5 col5\" >0.827300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row6\" class=\"row_heading level0 row6\" >XGB_all</th>\n",
       "      <td id=\"T_3c678_row6_col0\" class=\"data row6 col0\" >1.000000</td>\n",
       "      <td id=\"T_3c678_row6_col1\" class=\"data row6 col1\" >1.000000</td>\n",
       "      <td id=\"T_3c678_row6_col2\" class=\"data row6 col2\" >1.000000</td>\n",
       "      <td id=\"T_3c678_row6_col3\" class=\"data row6 col3\" >0.629600</td>\n",
       "      <td id=\"T_3c678_row6_col4\" class=\"data row6 col4\" >0.486500</td>\n",
       "      <td id=\"T_3c678_row6_col5\" class=\"data row6 col5\" >0.897400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row7\" class=\"row_heading level0 row7\" >LR_activity_and_demo_tuned</th>\n",
       "      <td id=\"T_3c678_row7_col0\" class=\"data row7 col0\" >0.788500</td>\n",
       "      <td id=\"T_3c678_row7_col1\" class=\"data row7 col1\" >0.745400</td>\n",
       "      <td id=\"T_3c678_row7_col2\" class=\"data row7 col2\" >0.948800</td>\n",
       "      <td id=\"T_3c678_row7_col3\" class=\"data row7 col3\" >0.629600</td>\n",
       "      <td id=\"T_3c678_row7_col4\" class=\"data row7 col4\" >0.485500</td>\n",
       "      <td id=\"T_3c678_row7_col5\" class=\"data row7 col5\" >0.867200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row8\" class=\"row_heading level0 row8\" >LR_activity_and_demo</th>\n",
       "      <td id=\"T_3c678_row8_col0\" class=\"data row8 col0\" >0.817300</td>\n",
       "      <td id=\"T_3c678_row8_col1\" class=\"data row8 col1\" >0.793700</td>\n",
       "      <td id=\"T_3c678_row8_col2\" class=\"data row8 col2\" >0.951400</td>\n",
       "      <td id=\"T_3c678_row8_col3\" class=\"data row8 col3\" >0.629600</td>\n",
       "      <td id=\"T_3c678_row8_col4\" class=\"data row8 col4\" >0.485500</td>\n",
       "      <td id=\"T_3c678_row8_col5\" class=\"data row8 col5\" >0.867200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row9\" class=\"row_heading level0 row9\" >XGB_performance_and_demo</th>\n",
       "      <td id=\"T_3c678_row9_col0\" class=\"data row9 col0\" >1.000000</td>\n",
       "      <td id=\"T_3c678_row9_col1\" class=\"data row9 col1\" >1.000000</td>\n",
       "      <td id=\"T_3c678_row9_col2\" class=\"data row9 col2\" >1.000000</td>\n",
       "      <td id=\"T_3c678_row9_col3\" class=\"data row9 col3\" >0.592600</td>\n",
       "      <td id=\"T_3c678_row9_col4\" class=\"data row9 col4\" >0.454600</td>\n",
       "      <td id=\"T_3c678_row9_col5\" class=\"data row9 col5\" >0.846900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row10\" class=\"row_heading level0 row10\" >LR_demo_only_tuned</th>\n",
       "      <td id=\"T_3c678_row10_col0\" class=\"data row10 col0\" >0.740400</td>\n",
       "      <td id=\"T_3c678_row10_col1\" class=\"data row10 col1\" >0.660200</td>\n",
       "      <td id=\"T_3c678_row10_col2\" class=\"data row10 col2\" >0.931200</td>\n",
       "      <td id=\"T_3c678_row10_col3\" class=\"data row10 col3\" >0.555600</td>\n",
       "      <td id=\"T_3c678_row10_col4\" class=\"data row10 col4\" >0.406700</td>\n",
       "      <td id=\"T_3c678_row10_col5\" class=\"data row10 col5\" >0.812100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row11\" class=\"row_heading level0 row11\" >LR_demo_only</th>\n",
       "      <td id=\"T_3c678_row11_col0\" class=\"data row11 col0\" >0.778800</td>\n",
       "      <td id=\"T_3c678_row11_col1\" class=\"data row11 col1\" >0.735300</td>\n",
       "      <td id=\"T_3c678_row11_col2\" class=\"data row11 col2\" >0.937100</td>\n",
       "      <td id=\"T_3c678_row11_col3\" class=\"data row11 col3\" >0.555600</td>\n",
       "      <td id=\"T_3c678_row11_col4\" class=\"data row11 col4\" >0.406700</td>\n",
       "      <td id=\"T_3c678_row11_col5\" class=\"data row11 col5\" >0.799400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row12\" class=\"row_heading level0 row12\" >LR_performance_and_demo_tuned</th>\n",
       "      <td id=\"T_3c678_row12_col0\" class=\"data row12 col0\" >0.769200</td>\n",
       "      <td id=\"T_3c678_row12_col1\" class=\"data row12 col1\" >0.716400</td>\n",
       "      <td id=\"T_3c678_row12_col2\" class=\"data row12 col2\" >0.920500</td>\n",
       "      <td id=\"T_3c678_row12_col3\" class=\"data row12 col3\" >0.555600</td>\n",
       "      <td id=\"T_3c678_row12_col4\" class=\"data row12 col4\" >0.403700</td>\n",
       "      <td id=\"T_3c678_row12_col5\" class=\"data row12 col5\" >0.857900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row13\" class=\"row_heading level0 row13\" >XGB_activity_and_demo</th>\n",
       "      <td id=\"T_3c678_row13_col0\" class=\"data row13 col0\" >1.000000</td>\n",
       "      <td id=\"T_3c678_row13_col1\" class=\"data row13 col1\" >1.000000</td>\n",
       "      <td id=\"T_3c678_row13_col2\" class=\"data row13 col2\" >1.000000</td>\n",
       "      <td id=\"T_3c678_row13_col3\" class=\"data row13 col3\" >0.518500</td>\n",
       "      <td id=\"T_3c678_row13_col4\" class=\"data row13 col4\" >0.400300</td>\n",
       "      <td id=\"T_3c678_row13_col5\" class=\"data row13 col5\" >0.733000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row14\" class=\"row_heading level0 row14\" >XGB_activity_only</th>\n",
       "      <td id=\"T_3c678_row14_col0\" class=\"data row14 col0\" >0.548100</td>\n",
       "      <td id=\"T_3c678_row14_col1\" class=\"data row14 col1\" >0.405400</td>\n",
       "      <td id=\"T_3c678_row14_col2\" class=\"data row14 col2\" >0.813100</td>\n",
       "      <td id=\"T_3c678_row14_col3\" class=\"data row14 col3\" >0.481500</td>\n",
       "      <td id=\"T_3c678_row14_col4\" class=\"data row14 col4\" >0.380400</td>\n",
       "      <td id=\"T_3c678_row14_col5\" class=\"data row14 col5\" >0.805900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row15\" class=\"row_heading level0 row15\" >LR_activity_only_tuned</th>\n",
       "      <td id=\"T_3c678_row15_col0\" class=\"data row15 col0\" >0.596200</td>\n",
       "      <td id=\"T_3c678_row15_col1\" class=\"data row15 col1\" >0.440100</td>\n",
       "      <td id=\"T_3c678_row15_col2\" class=\"data row15 col2\" >0.824900</td>\n",
       "      <td id=\"T_3c678_row15_col3\" class=\"data row15 col3\" >0.555600</td>\n",
       "      <td id=\"T_3c678_row15_col4\" class=\"data row15 col4\" >0.377300</td>\n",
       "      <td id=\"T_3c678_row15_col5\" class=\"data row15 col5\" >0.843400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row16\" class=\"row_heading level0 row16\" >LR_activity_only</th>\n",
       "      <td id=\"T_3c678_row16_col0\" class=\"data row16 col0\" >0.605800</td>\n",
       "      <td id=\"T_3c678_row16_col1\" class=\"data row16 col1\" >0.445600</td>\n",
       "      <td id=\"T_3c678_row16_col2\" class=\"data row16 col2\" >0.828800</td>\n",
       "      <td id=\"T_3c678_row16_col3\" class=\"data row16 col3\" >0.555600</td>\n",
       "      <td id=\"T_3c678_row16_col4\" class=\"data row16 col4\" >0.377300</td>\n",
       "      <td id=\"T_3c678_row16_col5\" class=\"data row16 col5\" >0.843200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row17\" class=\"row_heading level0 row17\" >XGB_demo_only</th>\n",
       "      <td id=\"T_3c678_row17_col0\" class=\"data row17 col0\" >1.000000</td>\n",
       "      <td id=\"T_3c678_row17_col1\" class=\"data row17 col1\" >1.000000</td>\n",
       "      <td id=\"T_3c678_row17_col2\" class=\"data row17 col2\" >1.000000</td>\n",
       "      <td id=\"T_3c678_row17_col3\" class=\"data row17 col3\" >0.407400</td>\n",
       "      <td id=\"T_3c678_row17_col4\" class=\"data row17 col4\" >0.340900</td>\n",
       "      <td id=\"T_3c678_row17_col5\" class=\"data row17 col5\" >0.611800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row18\" class=\"row_heading level0 row18\" >XGB_performance_and_demo_tuned</th>\n",
       "      <td id=\"T_3c678_row18_col0\" class=\"data row18 col0\" >0.442300</td>\n",
       "      <td id=\"T_3c678_row18_col1\" class=\"data row18 col1\" >0.153300</td>\n",
       "      <td id=\"T_3c678_row18_col2\" class=\"data row18 col2\" >0.639500</td>\n",
       "      <td id=\"T_3c678_row18_col3\" class=\"data row18 col3\" >0.296300</td>\n",
       "      <td id=\"T_3c678_row18_col4\" class=\"data row18 col4\" >0.114300</td>\n",
       "      <td id=\"T_3c678_row18_col5\" class=\"data row18 col5\" >0.510700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row19\" class=\"row_heading level0 row19\" >Baseline</th>\n",
       "      <td id=\"T_3c678_row19_col0\" class=\"data row19 col0\" >0.442300</td>\n",
       "      <td id=\"T_3c678_row19_col1\" class=\"data row19 col1\" >0.153300</td>\n",
       "      <td id=\"T_3c678_row19_col2\" class=\"data row19 col2\" >0.500000</td>\n",
       "      <td id=\"T_3c678_row19_col3\" class=\"data row19 col3\" >0.296300</td>\n",
       "      <td id=\"T_3c678_row19_col4\" class=\"data row19 col4\" >0.114300</td>\n",
       "      <td id=\"T_3c678_row19_col5\" class=\"data row19 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row20\" class=\"row_heading level0 row20\" >XGB_activity_only_tuned</th>\n",
       "      <td id=\"T_3c678_row20_col0\" class=\"data row20 col0\" >0.442300</td>\n",
       "      <td id=\"T_3c678_row20_col1\" class=\"data row20 col1\" >0.153300</td>\n",
       "      <td id=\"T_3c678_row20_col2\" class=\"data row20 col2\" >0.500000</td>\n",
       "      <td id=\"T_3c678_row20_col3\" class=\"data row20 col3\" >0.296300</td>\n",
       "      <td id=\"T_3c678_row20_col4\" class=\"data row20 col4\" >0.114300</td>\n",
       "      <td id=\"T_3c678_row20_col5\" class=\"data row20 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row21\" class=\"row_heading level0 row21\" >XGB_activity_and_demo_tuned</th>\n",
       "      <td id=\"T_3c678_row21_col0\" class=\"data row21 col0\" >0.442300</td>\n",
       "      <td id=\"T_3c678_row21_col1\" class=\"data row21 col1\" >0.153300</td>\n",
       "      <td id=\"T_3c678_row21_col2\" class=\"data row21 col2\" >0.622600</td>\n",
       "      <td id=\"T_3c678_row21_col3\" class=\"data row21 col3\" >0.296300</td>\n",
       "      <td id=\"T_3c678_row21_col4\" class=\"data row21 col4\" >0.114300</td>\n",
       "      <td id=\"T_3c678_row21_col5\" class=\"data row21 col5\" >0.636600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row22\" class=\"row_heading level0 row22\" >XGB_performance_only_tuned</th>\n",
       "      <td id=\"T_3c678_row22_col0\" class=\"data row22 col0\" >0.442300</td>\n",
       "      <td id=\"T_3c678_row22_col1\" class=\"data row22 col1\" >0.153300</td>\n",
       "      <td id=\"T_3c678_row22_col2\" class=\"data row22 col2\" >0.500000</td>\n",
       "      <td id=\"T_3c678_row22_col3\" class=\"data row22 col3\" >0.296300</td>\n",
       "      <td id=\"T_3c678_row22_col4\" class=\"data row22 col4\" >0.114300</td>\n",
       "      <td id=\"T_3c678_row22_col5\" class=\"data row22 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row23\" class=\"row_heading level0 row23\" >XGB_demo_only_tuned</th>\n",
       "      <td id=\"T_3c678_row23_col0\" class=\"data row23 col0\" >0.442300</td>\n",
       "      <td id=\"T_3c678_row23_col1\" class=\"data row23 col1\" >0.153300</td>\n",
       "      <td id=\"T_3c678_row23_col2\" class=\"data row23 col2\" >0.500000</td>\n",
       "      <td id=\"T_3c678_row23_col3\" class=\"data row23 col3\" >0.296300</td>\n",
       "      <td id=\"T_3c678_row23_col4\" class=\"data row23 col4\" >0.114300</td>\n",
       "      <td id=\"T_3c678_row23_col5\" class=\"data row23 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3c678_level0_row24\" class=\"row_heading level0 row24\" >XGB_all_tuned</th>\n",
       "      <td id=\"T_3c678_row24_col0\" class=\"data row24 col0\" >0.442300</td>\n",
       "      <td id=\"T_3c678_row24_col1\" class=\"data row24 col1\" >0.153300</td>\n",
       "      <td id=\"T_3c678_row24_col2\" class=\"data row24 col2\" >0.500000</td>\n",
       "      <td id=\"T_3c678_row24_col3\" class=\"data row24 col3\" >0.296300</td>\n",
       "      <td id=\"T_3c678_row24_col4\" class=\"data row24 col4\" >0.114300</td>\n",
       "      <td id=\"T_3c678_row24_col5\" class=\"data row24 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x298e75d3c70>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(f\"../results/{dataset_name}/{experiment_name}/results_subsets.pickle\"):\n",
    "\n",
    "    results_subsets = {}\n",
    "    results_subsets_feature_importances = {}\n",
    "\n",
    "    for fold in range(folds):\n",
    "        results_subsets[fold] = {}\n",
    "        results_subsets_feature_importances[fold] = {}\n",
    "        # Create baseline\n",
    "        y_train = data_dict[f\"y_train_{fold}\"]\n",
    "        y_val = data_dict[f\"y_val_{fold}\"]\n",
    "        y_test = data_dict[f\"y_test_{fold}\"]\n",
    "        y_train_val = np.concatenate([y_train,y_val])\n",
    "\n",
    "        u,c = np.unique(y_train_val,return_counts=True)\n",
    "        nb_classes = len(u)\n",
    "        baseline = np.argmax(c)\n",
    "\n",
    "        y_train_val_pred_base = np.ones(y_train_val.shape[0])*baseline\n",
    "        y_test_pred_base = np.ones(y_test.shape[0])*baseline\n",
    "\n",
    "        results_subsets[fold][\"Baseline\"] = {}\n",
    "        eval_res_train = get_metrics(get_one_hot(y_train_val, nb_classes), get_one_hot(y_train_val_pred_base.astype(int), nb_classes), target=target)\n",
    "        for metric in eval_res_train.keys():\n",
    "            results_subsets[fold][\"Baseline\"][metric + \" Train\"] = eval_res_train[metric]\n",
    "        eval_res_test = get_metrics(get_one_hot(y_test, nb_classes), get_one_hot(y_test_pred_base.astype(int), nb_classes), target=target)\n",
    "        for metric in eval_res_test.keys():\n",
    "            results_subsets[fold][\"Baseline\"][metric + \" Test\"] = eval_res_test[metric]\n",
    "\n",
    "\n",
    "        for subset_key in subsets:\n",
    "            print(f\"Preparing results for fold {fold}, subset={subset_key}\")\n",
    "            # Retrieve data\n",
    "            z_cols = data_dict[\"z_cols\"]\n",
    "\n",
    "            X_train = data_dict[f\"X_train_{fold}\"]\n",
    "            y_train = data_dict[f\"y_train_{fold}\"]\n",
    "\n",
    "            X_val = data_dict[f\"X_val_{fold}\"]\n",
    "            y_val = data_dict[f\"y_val_{fold}\"]\n",
    "\n",
    "            X_test = data_dict[f\"X_test_{fold}\"]\n",
    "            y_test = data_dict[f\"y_test_{fold}\"]\n",
    "        \n",
    "            y_train_val = np.concatenate([y_train,y_val])\n",
    "\n",
    "            # Define data subset for LR\n",
    "            z_glmm_encoded_train = data_dict[f\"z_glmm_encoded_train_{fold}\"] \n",
    "            z_glmm_encoded_val = data_dict[f\"z_glmm_encoded_val_{fold}\"] \n",
    "            z_glmm_encoded_test = data_dict[f\"z_glmm_encoded_test_{fold}\"] \n",
    "            X_train_lr = pd.concat([X_train,z_glmm_encoded_train],axis=1)\n",
    "            X_val_lr = pd.concat([X_val,z_glmm_encoded_val],axis=1)\n",
    "            X_test_lr = pd.concat([X_test,z_glmm_encoded_test],axis=1)      \n",
    "            X_train_val_lr = pd.concat([X_train_lr,X_val_lr])\n",
    "\n",
    "            # Define data subset for XGB\n",
    "            z_ordinal_encoded_train = data_dict[f\"z_ordinal_encoded_train_{fold}\"] \n",
    "            z_ordinal_encoded_val = data_dict[f\"z_ordinal_encoded_val_{fold}\"] \n",
    "            z_ordinal_encoded_test = data_dict[f\"z_ordinal_encoded_test_{fold}\"] \n",
    "            X_train_xgb = pd.concat([X_train,z_ordinal_encoded_train],axis=1)\n",
    "            X_val_xgb = pd.concat([X_val,z_ordinal_encoded_val],axis=1)\n",
    "            X_test_xgb = pd.concat([X_test,z_ordinal_encoded_test],axis=1)\n",
    "            X_train_val_xgb = pd.concat([X_train_xgb,X_val_xgb])\n",
    "\n",
    "\n",
    "            # Define data subset for evaluation\n",
    "            X_train_val_lr = X_train_val_lr[[i for i in X_train_val_lr.columns if any([j in i for j in subsets[subset_key]])]]\n",
    "            X_test_lr = X_test_lr[[i for i in X_test_lr.columns if any([j in i for j in subsets[subset_key]])]]\n",
    "            X_train_val_xgb = X_train_val_xgb[[i for i in X_train_val_xgb.columns if any([j in i for j in subsets[subset_key]])]]\n",
    "            X_test_xgb = X_test_xgb[[i for i in X_test_xgb.columns if any([j in i for j in subsets[subset_key]])]]\n",
    "\n",
    "\n",
    "            # Train base models\n",
    "            res, feats = evaluate_logreg(X_train_val_lr, y_train_val, X_test_lr, y_test, target=target,tune=False, seed=RS)\n",
    "            results_subsets[fold][\"LR_\"+subset_key] = res\n",
    "            results_subsets_feature_importances[fold][\"LR_\"+subset_key] = feats\n",
    "\n",
    "            res, feats = evaluate_xgb(X_train_val_xgb, y_train_val, X_test_xgb, y_test, target, tune=False, max_evals=max_evals, early_stopping_rounds=early_stopping_rounds, seed=RS)\n",
    "            results_subsets[fold][\"XGB_\"+subset_key] = res\n",
    "            results_subsets_feature_importances[fold][\"XGB_\"+subset_key] = feats\n",
    "\n",
    "            # Train tuned models\n",
    "            res, feats = evaluate_logreg(X_train_val_lr, y_train_val, X_test_lr, y_test, target=target, max_evals=max_evals, tune=True, seed=RS)\n",
    "            results_subsets[fold][\"LR_\"+subset_key+\"_tuned\"] = res\n",
    "            results_subsets_feature_importances[fold][\"LR_\"+subset_key+\"_tuned\"] = feats\n",
    "\n",
    "            res, feats = evaluate_xgb(X_train_val_xgb, y_train_val, X_test_xgb, y_test, target, tune=True, max_evals=max_evals, early_stopping_rounds=early_stopping_rounds, seed=RS)\n",
    "            results_subsets[fold][\"XGB_\"+subset_key+\"_tuned\"] = res\n",
    "            results_subsets_feature_importances[fold][\"XGB_\"+subset_key+\"_tuned\"] = feats\n",
    "    \n",
    "    if not os.path.exists(f\"../results/{dataset_name}/{experiment_name}\"):\n",
    "        os.makedirs(f\"../results/{dataset_name}/{experiment_name}\")\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_subsets.pickle\", 'wb') as handle:\n",
    "        pickle.dump(results_subsets, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_subsets_feature_importances.pickle\", 'wb') as handle:\n",
    "        pickle.dump(results_subsets_feature_importances, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "else:\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_subsets.pickle\", 'rb') as handle:\n",
    "        results_subsets = pickle.load(handle)\n",
    "    with open(f\"../results/{dataset_name}/{experiment_name}/results_subsets_feature_importances.pickle\", 'rb') as handle:\n",
    "        results_subsets_feature_importances = pickle.load(handle)\n",
    "        \n",
    "        \n",
    "results_subsets_df = pd.DataFrame(results_subsets[0]).transpose().sort_values(\"F1 Test\",ascending=False).round(4)\n",
    "results_subsets_df[[\"Accuracy Train\", \"F1 Train\", \"AUROC Train\", \"Accuracy Test\", \"F1 Test\", \"AUROC Test\"]].style.highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab693122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d8479_row0_col4, #T_d8479_row1_col4, #T_d8479_row3_col5, #T_d8479_row4_col5, #T_d8479_row6_col0, #T_d8479_row6_col1, #T_d8479_row6_col2, #T_d8479_row6_col3, #T_d8479_row7_col3, #T_d8479_row8_col3, #T_d8479_row9_col0, #T_d8479_row9_col1, #T_d8479_row9_col2, #T_d8479_row13_col0, #T_d8479_row13_col1, #T_d8479_row13_col2, #T_d8479_row17_col0, #T_d8479_row17_col1, #T_d8479_row17_col2 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d8479\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d8479_level0_col0\" class=\"col_heading level0 col0\" >Accuracy Train</th>\n",
       "      <th id=\"T_d8479_level0_col1\" class=\"col_heading level0 col1\" >F1 Train</th>\n",
       "      <th id=\"T_d8479_level0_col2\" class=\"col_heading level0 col2\" >AUROC Train</th>\n",
       "      <th id=\"T_d8479_level0_col3\" class=\"col_heading level0 col3\" >Accuracy Test</th>\n",
       "      <th id=\"T_d8479_level0_col4\" class=\"col_heading level0 col4\" >F1 Test</th>\n",
       "      <th id=\"T_d8479_level0_col5\" class=\"col_heading level0 col5\" >AUROC Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row0\" class=\"row_heading level0 row0\" >LR_all_tuned</th>\n",
       "      <td id=\"T_d8479_row0_col0\" class=\"data row0 col0\" >0.855800</td>\n",
       "      <td id=\"T_d8479_row0_col1\" class=\"data row0 col1\" >0.848700</td>\n",
       "      <td id=\"T_d8479_row0_col2\" class=\"data row0 col2\" >0.968700</td>\n",
       "      <td id=\"T_d8479_row0_col3\" class=\"data row0 col3\" >0.592600</td>\n",
       "      <td id=\"T_d8479_row0_col4\" class=\"data row0 col4\" >0.686700</td>\n",
       "      <td id=\"T_d8479_row0_col5\" class=\"data row0 col5\" >0.884800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row1\" class=\"row_heading level0 row1\" >LR_all</th>\n",
       "      <td id=\"T_d8479_row1_col0\" class=\"data row1 col0\" >0.865400</td>\n",
       "      <td id=\"T_d8479_row1_col1\" class=\"data row1 col1\" >0.855400</td>\n",
       "      <td id=\"T_d8479_row1_col2\" class=\"data row1 col2\" >0.971900</td>\n",
       "      <td id=\"T_d8479_row1_col3\" class=\"data row1 col3\" >0.592600</td>\n",
       "      <td id=\"T_d8479_row1_col4\" class=\"data row1 col4\" >0.686700</td>\n",
       "      <td id=\"T_d8479_row1_col5\" class=\"data row1 col5\" >0.878300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row2\" class=\"row_heading level0 row2\" >LR_performance_and_demo</th>\n",
       "      <td id=\"T_d8479_row2_col0\" class=\"data row2 col0\" >0.836500</td>\n",
       "      <td id=\"T_d8479_row2_col1\" class=\"data row2 col1\" >0.831800</td>\n",
       "      <td id=\"T_d8479_row2_col2\" class=\"data row2 col2\" >0.958900</td>\n",
       "      <td id=\"T_d8479_row2_col3\" class=\"data row2 col3\" >0.592600</td>\n",
       "      <td id=\"T_d8479_row2_col4\" class=\"data row2 col4\" >0.680600</td>\n",
       "      <td id=\"T_d8479_row2_col5\" class=\"data row2 col5\" >0.859600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row3\" class=\"row_heading level0 row3\" >LR_performance_only</th>\n",
       "      <td id=\"T_d8479_row3_col0\" class=\"data row3 col0\" >0.567300</td>\n",
       "      <td id=\"T_d8479_row3_col1\" class=\"data row3 col1\" >0.489700</td>\n",
       "      <td id=\"T_d8479_row3_col2\" class=\"data row3 col2\" >0.762400</td>\n",
       "      <td id=\"T_d8479_row3_col3\" class=\"data row3 col3\" >0.555600</td>\n",
       "      <td id=\"T_d8479_row3_col4\" class=\"data row3 col4\" >0.637300</td>\n",
       "      <td id=\"T_d8479_row3_col5\" class=\"data row3 col5\" >0.898900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row4\" class=\"row_heading level0 row4\" >LR_performance_only_tuned</th>\n",
       "      <td id=\"T_d8479_row4_col0\" class=\"data row4 col0\" >0.567300</td>\n",
       "      <td id=\"T_d8479_row4_col1\" class=\"data row4 col1\" >0.489700</td>\n",
       "      <td id=\"T_d8479_row4_col2\" class=\"data row4 col2\" >0.759200</td>\n",
       "      <td id=\"T_d8479_row4_col3\" class=\"data row4 col3\" >0.555600</td>\n",
       "      <td id=\"T_d8479_row4_col4\" class=\"data row4 col4\" >0.637300</td>\n",
       "      <td id=\"T_d8479_row4_col5\" class=\"data row4 col5\" >0.898900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row5\" class=\"row_heading level0 row5\" >XGB_performance_only</th>\n",
       "      <td id=\"T_d8479_row5_col0\" class=\"data row5 col0\" >0.769200</td>\n",
       "      <td id=\"T_d8479_row5_col1\" class=\"data row5 col1\" >0.759800</td>\n",
       "      <td id=\"T_d8479_row5_col2\" class=\"data row5 col2\" >0.928800</td>\n",
       "      <td id=\"T_d8479_row5_col3\" class=\"data row5 col3\" >0.518500</td>\n",
       "      <td id=\"T_d8479_row5_col4\" class=\"data row5 col4\" >0.610200</td>\n",
       "      <td id=\"T_d8479_row5_col5\" class=\"data row5 col5\" >0.827300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row6\" class=\"row_heading level0 row6\" >XGB_all</th>\n",
       "      <td id=\"T_d8479_row6_col0\" class=\"data row6 col0\" >1.000000</td>\n",
       "      <td id=\"T_d8479_row6_col1\" class=\"data row6 col1\" >1.000000</td>\n",
       "      <td id=\"T_d8479_row6_col2\" class=\"data row6 col2\" >1.000000</td>\n",
       "      <td id=\"T_d8479_row6_col3\" class=\"data row6 col3\" >0.629600</td>\n",
       "      <td id=\"T_d8479_row6_col4\" class=\"data row6 col4\" >0.486500</td>\n",
       "      <td id=\"T_d8479_row6_col5\" class=\"data row6 col5\" >0.897400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row7\" class=\"row_heading level0 row7\" >LR_activity_and_demo_tuned</th>\n",
       "      <td id=\"T_d8479_row7_col0\" class=\"data row7 col0\" >0.788500</td>\n",
       "      <td id=\"T_d8479_row7_col1\" class=\"data row7 col1\" >0.745400</td>\n",
       "      <td id=\"T_d8479_row7_col2\" class=\"data row7 col2\" >0.948800</td>\n",
       "      <td id=\"T_d8479_row7_col3\" class=\"data row7 col3\" >0.629600</td>\n",
       "      <td id=\"T_d8479_row7_col4\" class=\"data row7 col4\" >0.485500</td>\n",
       "      <td id=\"T_d8479_row7_col5\" class=\"data row7 col5\" >0.867200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row8\" class=\"row_heading level0 row8\" >LR_activity_and_demo</th>\n",
       "      <td id=\"T_d8479_row8_col0\" class=\"data row8 col0\" >0.817300</td>\n",
       "      <td id=\"T_d8479_row8_col1\" class=\"data row8 col1\" >0.793700</td>\n",
       "      <td id=\"T_d8479_row8_col2\" class=\"data row8 col2\" >0.951400</td>\n",
       "      <td id=\"T_d8479_row8_col3\" class=\"data row8 col3\" >0.629600</td>\n",
       "      <td id=\"T_d8479_row8_col4\" class=\"data row8 col4\" >0.485500</td>\n",
       "      <td id=\"T_d8479_row8_col5\" class=\"data row8 col5\" >0.867200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row9\" class=\"row_heading level0 row9\" >XGB_performance_and_demo</th>\n",
       "      <td id=\"T_d8479_row9_col0\" class=\"data row9 col0\" >1.000000</td>\n",
       "      <td id=\"T_d8479_row9_col1\" class=\"data row9 col1\" >1.000000</td>\n",
       "      <td id=\"T_d8479_row9_col2\" class=\"data row9 col2\" >1.000000</td>\n",
       "      <td id=\"T_d8479_row9_col3\" class=\"data row9 col3\" >0.592600</td>\n",
       "      <td id=\"T_d8479_row9_col4\" class=\"data row9 col4\" >0.454600</td>\n",
       "      <td id=\"T_d8479_row9_col5\" class=\"data row9 col5\" >0.846900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row10\" class=\"row_heading level0 row10\" >LR_demo_only_tuned</th>\n",
       "      <td id=\"T_d8479_row10_col0\" class=\"data row10 col0\" >0.740400</td>\n",
       "      <td id=\"T_d8479_row10_col1\" class=\"data row10 col1\" >0.660200</td>\n",
       "      <td id=\"T_d8479_row10_col2\" class=\"data row10 col2\" >0.931200</td>\n",
       "      <td id=\"T_d8479_row10_col3\" class=\"data row10 col3\" >0.555600</td>\n",
       "      <td id=\"T_d8479_row10_col4\" class=\"data row10 col4\" >0.406700</td>\n",
       "      <td id=\"T_d8479_row10_col5\" class=\"data row10 col5\" >0.812100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row11\" class=\"row_heading level0 row11\" >LR_demo_only</th>\n",
       "      <td id=\"T_d8479_row11_col0\" class=\"data row11 col0\" >0.778800</td>\n",
       "      <td id=\"T_d8479_row11_col1\" class=\"data row11 col1\" >0.735300</td>\n",
       "      <td id=\"T_d8479_row11_col2\" class=\"data row11 col2\" >0.937100</td>\n",
       "      <td id=\"T_d8479_row11_col3\" class=\"data row11 col3\" >0.555600</td>\n",
       "      <td id=\"T_d8479_row11_col4\" class=\"data row11 col4\" >0.406700</td>\n",
       "      <td id=\"T_d8479_row11_col5\" class=\"data row11 col5\" >0.799400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row12\" class=\"row_heading level0 row12\" >LR_performance_and_demo_tuned</th>\n",
       "      <td id=\"T_d8479_row12_col0\" class=\"data row12 col0\" >0.769200</td>\n",
       "      <td id=\"T_d8479_row12_col1\" class=\"data row12 col1\" >0.716400</td>\n",
       "      <td id=\"T_d8479_row12_col2\" class=\"data row12 col2\" >0.920500</td>\n",
       "      <td id=\"T_d8479_row12_col3\" class=\"data row12 col3\" >0.555600</td>\n",
       "      <td id=\"T_d8479_row12_col4\" class=\"data row12 col4\" >0.403700</td>\n",
       "      <td id=\"T_d8479_row12_col5\" class=\"data row12 col5\" >0.857900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row13\" class=\"row_heading level0 row13\" >XGB_activity_and_demo</th>\n",
       "      <td id=\"T_d8479_row13_col0\" class=\"data row13 col0\" >1.000000</td>\n",
       "      <td id=\"T_d8479_row13_col1\" class=\"data row13 col1\" >1.000000</td>\n",
       "      <td id=\"T_d8479_row13_col2\" class=\"data row13 col2\" >1.000000</td>\n",
       "      <td id=\"T_d8479_row13_col3\" class=\"data row13 col3\" >0.518500</td>\n",
       "      <td id=\"T_d8479_row13_col4\" class=\"data row13 col4\" >0.400300</td>\n",
       "      <td id=\"T_d8479_row13_col5\" class=\"data row13 col5\" >0.733000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row14\" class=\"row_heading level0 row14\" >XGB_activity_only</th>\n",
       "      <td id=\"T_d8479_row14_col0\" class=\"data row14 col0\" >0.548100</td>\n",
       "      <td id=\"T_d8479_row14_col1\" class=\"data row14 col1\" >0.405400</td>\n",
       "      <td id=\"T_d8479_row14_col2\" class=\"data row14 col2\" >0.813100</td>\n",
       "      <td id=\"T_d8479_row14_col3\" class=\"data row14 col3\" >0.481500</td>\n",
       "      <td id=\"T_d8479_row14_col4\" class=\"data row14 col4\" >0.380400</td>\n",
       "      <td id=\"T_d8479_row14_col5\" class=\"data row14 col5\" >0.805900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row15\" class=\"row_heading level0 row15\" >LR_activity_only_tuned</th>\n",
       "      <td id=\"T_d8479_row15_col0\" class=\"data row15 col0\" >0.596200</td>\n",
       "      <td id=\"T_d8479_row15_col1\" class=\"data row15 col1\" >0.440100</td>\n",
       "      <td id=\"T_d8479_row15_col2\" class=\"data row15 col2\" >0.824900</td>\n",
       "      <td id=\"T_d8479_row15_col3\" class=\"data row15 col3\" >0.555600</td>\n",
       "      <td id=\"T_d8479_row15_col4\" class=\"data row15 col4\" >0.377300</td>\n",
       "      <td id=\"T_d8479_row15_col5\" class=\"data row15 col5\" >0.843400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row16\" class=\"row_heading level0 row16\" >LR_activity_only</th>\n",
       "      <td id=\"T_d8479_row16_col0\" class=\"data row16 col0\" >0.605800</td>\n",
       "      <td id=\"T_d8479_row16_col1\" class=\"data row16 col1\" >0.445600</td>\n",
       "      <td id=\"T_d8479_row16_col2\" class=\"data row16 col2\" >0.828800</td>\n",
       "      <td id=\"T_d8479_row16_col3\" class=\"data row16 col3\" >0.555600</td>\n",
       "      <td id=\"T_d8479_row16_col4\" class=\"data row16 col4\" >0.377300</td>\n",
       "      <td id=\"T_d8479_row16_col5\" class=\"data row16 col5\" >0.843200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row17\" class=\"row_heading level0 row17\" >XGB_demo_only</th>\n",
       "      <td id=\"T_d8479_row17_col0\" class=\"data row17 col0\" >1.000000</td>\n",
       "      <td id=\"T_d8479_row17_col1\" class=\"data row17 col1\" >1.000000</td>\n",
       "      <td id=\"T_d8479_row17_col2\" class=\"data row17 col2\" >1.000000</td>\n",
       "      <td id=\"T_d8479_row17_col3\" class=\"data row17 col3\" >0.407400</td>\n",
       "      <td id=\"T_d8479_row17_col4\" class=\"data row17 col4\" >0.340900</td>\n",
       "      <td id=\"T_d8479_row17_col5\" class=\"data row17 col5\" >0.611800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row18\" class=\"row_heading level0 row18\" >XGB_performance_and_demo_tuned</th>\n",
       "      <td id=\"T_d8479_row18_col0\" class=\"data row18 col0\" >0.442300</td>\n",
       "      <td id=\"T_d8479_row18_col1\" class=\"data row18 col1\" >0.153300</td>\n",
       "      <td id=\"T_d8479_row18_col2\" class=\"data row18 col2\" >0.639500</td>\n",
       "      <td id=\"T_d8479_row18_col3\" class=\"data row18 col3\" >0.296300</td>\n",
       "      <td id=\"T_d8479_row18_col4\" class=\"data row18 col4\" >0.114300</td>\n",
       "      <td id=\"T_d8479_row18_col5\" class=\"data row18 col5\" >0.510700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row19\" class=\"row_heading level0 row19\" >Baseline</th>\n",
       "      <td id=\"T_d8479_row19_col0\" class=\"data row19 col0\" >0.442300</td>\n",
       "      <td id=\"T_d8479_row19_col1\" class=\"data row19 col1\" >0.153300</td>\n",
       "      <td id=\"T_d8479_row19_col2\" class=\"data row19 col2\" >0.500000</td>\n",
       "      <td id=\"T_d8479_row19_col3\" class=\"data row19 col3\" >0.296300</td>\n",
       "      <td id=\"T_d8479_row19_col4\" class=\"data row19 col4\" >0.114300</td>\n",
       "      <td id=\"T_d8479_row19_col5\" class=\"data row19 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row20\" class=\"row_heading level0 row20\" >XGB_activity_only_tuned</th>\n",
       "      <td id=\"T_d8479_row20_col0\" class=\"data row20 col0\" >0.442300</td>\n",
       "      <td id=\"T_d8479_row20_col1\" class=\"data row20 col1\" >0.153300</td>\n",
       "      <td id=\"T_d8479_row20_col2\" class=\"data row20 col2\" >0.500000</td>\n",
       "      <td id=\"T_d8479_row20_col3\" class=\"data row20 col3\" >0.296300</td>\n",
       "      <td id=\"T_d8479_row20_col4\" class=\"data row20 col4\" >0.114300</td>\n",
       "      <td id=\"T_d8479_row20_col5\" class=\"data row20 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row21\" class=\"row_heading level0 row21\" >XGB_activity_and_demo_tuned</th>\n",
       "      <td id=\"T_d8479_row21_col0\" class=\"data row21 col0\" >0.442300</td>\n",
       "      <td id=\"T_d8479_row21_col1\" class=\"data row21 col1\" >0.153300</td>\n",
       "      <td id=\"T_d8479_row21_col2\" class=\"data row21 col2\" >0.622600</td>\n",
       "      <td id=\"T_d8479_row21_col3\" class=\"data row21 col3\" >0.296300</td>\n",
       "      <td id=\"T_d8479_row21_col4\" class=\"data row21 col4\" >0.114300</td>\n",
       "      <td id=\"T_d8479_row21_col5\" class=\"data row21 col5\" >0.636600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row22\" class=\"row_heading level0 row22\" >XGB_performance_only_tuned</th>\n",
       "      <td id=\"T_d8479_row22_col0\" class=\"data row22 col0\" >0.442300</td>\n",
       "      <td id=\"T_d8479_row22_col1\" class=\"data row22 col1\" >0.153300</td>\n",
       "      <td id=\"T_d8479_row22_col2\" class=\"data row22 col2\" >0.500000</td>\n",
       "      <td id=\"T_d8479_row22_col3\" class=\"data row22 col3\" >0.296300</td>\n",
       "      <td id=\"T_d8479_row22_col4\" class=\"data row22 col4\" >0.114300</td>\n",
       "      <td id=\"T_d8479_row22_col5\" class=\"data row22 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row23\" class=\"row_heading level0 row23\" >XGB_demo_only_tuned</th>\n",
       "      <td id=\"T_d8479_row23_col0\" class=\"data row23 col0\" >0.442300</td>\n",
       "      <td id=\"T_d8479_row23_col1\" class=\"data row23 col1\" >0.153300</td>\n",
       "      <td id=\"T_d8479_row23_col2\" class=\"data row23 col2\" >0.500000</td>\n",
       "      <td id=\"T_d8479_row23_col3\" class=\"data row23 col3\" >0.296300</td>\n",
       "      <td id=\"T_d8479_row23_col4\" class=\"data row23 col4\" >0.114300</td>\n",
       "      <td id=\"T_d8479_row23_col5\" class=\"data row23 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8479_level0_row24\" class=\"row_heading level0 row24\" >XGB_all_tuned</th>\n",
       "      <td id=\"T_d8479_row24_col0\" class=\"data row24 col0\" >0.442300</td>\n",
       "      <td id=\"T_d8479_row24_col1\" class=\"data row24 col1\" >0.153300</td>\n",
       "      <td id=\"T_d8479_row24_col2\" class=\"data row24 col2\" >0.500000</td>\n",
       "      <td id=\"T_d8479_row24_col3\" class=\"data row24 col3\" >0.296300</td>\n",
       "      <td id=\"T_d8479_row24_col4\" class=\"data row24 col4\" >0.114300</td>\n",
       "      <td id=\"T_d8479_row24_col5\" class=\"data row24 col5\" >0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x299c6ae7d60>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_subsets_df = pd.DataFrame(results_subsets[0]).transpose().sort_values(\"F1 Test\",ascending=False).round(4)\n",
    "results_subsets_df[[\"Accuracy Train\", \"F1 Train\", \"AUROC Train\", \"Accuracy Test\", \"F1 Test\", \"AUROC Test\"]].style.highlight_max(color = 'lightgreen', axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff73930",
   "metadata": {},
   "source": [
    "### Effectiveness of Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "513536a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e59b8_row0_col0, #T_e59b8_row0_col1, #T_e59b8_row1_col0, #T_e59b8_row2_col0, #T_e59b8_row3_col0, #T_e59b8_row3_col1, #T_e59b8_row4_col1, #T_e59b8_row5_col0, #T_e59b8_row6_col0, #T_e59b8_row7_col0, #T_e59b8_row8_col0, #T_e59b8_row9_col0, #T_e59b8_row10_col0, #T_e59b8_row11_col0, #T_e59b8_row12_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e59b8\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e59b8_level0_col0\" class=\"col_heading level0 col0\" >Untuned</th>\n",
       "      <th id=\"T_e59b8_level0_col1\" class=\"col_heading level0 col1\" >Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e59b8_level0_row0\" class=\"row_heading level0 row0\" >Baseline</th>\n",
       "      <td id=\"T_e59b8_row0_col0\" class=\"data row0 col0\" >0.140000</td>\n",
       "      <td id=\"T_e59b8_row0_col1\" class=\"data row0 col1\" >0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e59b8_level0_row1\" class=\"row_heading level0 row1\" >LR_activity_and_demo</th>\n",
       "      <td id=\"T_e59b8_row1_col0\" class=\"data row1 col0\" >0.440000</td>\n",
       "      <td id=\"T_e59b8_row1_col1\" class=\"data row1 col1\" >0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e59b8_level0_row2\" class=\"row_heading level0 row2\" >LR_activity_only</th>\n",
       "      <td id=\"T_e59b8_row2_col0\" class=\"data row2 col0\" >0.400000</td>\n",
       "      <td id=\"T_e59b8_row2_col1\" class=\"data row2 col1\" >0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e59b8_level0_row3\" class=\"row_heading level0 row3\" >LR_all</th>\n",
       "      <td id=\"T_e59b8_row3_col0\" class=\"data row3 col0\" >0.520000</td>\n",
       "      <td id=\"T_e59b8_row3_col1\" class=\"data row3 col1\" >0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e59b8_level0_row4\" class=\"row_heading level0 row4\" >LR_demo_only</th>\n",
       "      <td id=\"T_e59b8_row4_col0\" class=\"data row4 col0\" >0.420000</td>\n",
       "      <td id=\"T_e59b8_row4_col1\" class=\"data row4 col1\" >0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e59b8_level0_row5\" class=\"row_heading level0 row5\" >LR_performance_and_demo</th>\n",
       "      <td id=\"T_e59b8_row5_col0\" class=\"data row5 col0\" >0.500000</td>\n",
       "      <td id=\"T_e59b8_row5_col1\" class=\"data row5 col1\" >0.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e59b8_level0_row6\" class=\"row_heading level0 row6\" >LR_performance_only</th>\n",
       "      <td id=\"T_e59b8_row6_col0\" class=\"data row6 col0\" >0.430000</td>\n",
       "      <td id=\"T_e59b8_row6_col1\" class=\"data row6 col1\" >0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e59b8_level0_row7\" class=\"row_heading level0 row7\" >XGB_activity_and_demo</th>\n",
       "      <td id=\"T_e59b8_row7_col0\" class=\"data row7 col0\" >0.370000</td>\n",
       "      <td id=\"T_e59b8_row7_col1\" class=\"data row7 col1\" >0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e59b8_level0_row8\" class=\"row_heading level0 row8\" >XGB_activity_only</th>\n",
       "      <td id=\"T_e59b8_row8_col0\" class=\"data row8 col0\" >0.290000</td>\n",
       "      <td id=\"T_e59b8_row8_col1\" class=\"data row8 col1\" >0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e59b8_level0_row9\" class=\"row_heading level0 row9\" >XGB_all</th>\n",
       "      <td id=\"T_e59b8_row9_col0\" class=\"data row9 col0\" >0.530000</td>\n",
       "      <td id=\"T_e59b8_row9_col1\" class=\"data row9 col1\" >0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e59b8_level0_row10\" class=\"row_heading level0 row10\" >XGB_demo_only</th>\n",
       "      <td id=\"T_e59b8_row10_col0\" class=\"data row10 col0\" >0.300000</td>\n",
       "      <td id=\"T_e59b8_row10_col1\" class=\"data row10 col1\" >0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e59b8_level0_row11\" class=\"row_heading level0 row11\" >XGB_performance_and_demo</th>\n",
       "      <td id=\"T_e59b8_row11_col0\" class=\"data row11 col0\" >0.490000</td>\n",
       "      <td id=\"T_e59b8_row11_col1\" class=\"data row11 col1\" >0.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e59b8_level0_row12\" class=\"row_heading level0 row12\" >XGB_performance_only</th>\n",
       "      <td id=\"T_e59b8_row12_col0\" class=\"data row12 col0\" >0.580000</td>\n",
       "      <td id=\"T_e59b8_row12_col1\" class=\"data row12 col1\" >0.260000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x298e75d9970>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = results_subsets[0].keys()\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "subsets_folds_df = pd.DataFrame([pd.DataFrame(results_subsets[fold_num]).loc[metric,models] for fold_num in results_subsets.keys()],index=results_subsets.keys())\n",
    "subsets_mean_df = subsets_folds_df.mean(axis=0)\n",
    "subsets_std_df = subsets_folds_df.std(axis=0)\n",
    "\n",
    "methods = sorted(list(subsets_mean_df.index))[1:]\n",
    "not_tuned = [\"Baseline\"]+methods[::2]\n",
    "tuned = [\"Baseline\"]+methods[1::2]\n",
    "\n",
    "res_df_tune_comp_mean = pd.DataFrame([subsets_mean_df.loc[not_tuned].values,subsets_mean_df.loc[tuned]],index=[\"Untuned\",\"Tuned\"],columns=not_tuned).transpose()\n",
    "res_df_tune_comp_std = pd.DataFrame([subsets_std_df.loc[not_tuned].values,subsets_std_df.loc[tuned]],index=[\"Untuned\",\"Tuned\"],columns=not_tuned).transpose()\n",
    "\n",
    "res_df_tune_comp_mean.round(2).style.highlight_max(color = 'lightgreen', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8370957b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Untuned</th>\n",
       "      <th>Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.14 (0.026)</td>\n",
       "      <td>0.14 (0.026)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_activity_and_demo</th>\n",
       "      <td>0.44 (0.095)</td>\n",
       "      <td>0.43 (0.084)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_activity_only</th>\n",
       "      <td>0.4 (0.105)</td>\n",
       "      <td>0.39 (0.099)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_all</th>\n",
       "      <td>0.52 (0.111)</td>\n",
       "      <td>0.52 (0.112)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_demo_only</th>\n",
       "      <td>0.42 (0.106)</td>\n",
       "      <td>0.44 (0.108)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_performance_and_demo</th>\n",
       "      <td>0.5 (0.163)</td>\n",
       "      <td>0.46 (0.125)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_performance_only</th>\n",
       "      <td>0.43 (0.125)</td>\n",
       "      <td>0.38 (0.169)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_activity_and_demo</th>\n",
       "      <td>0.37 (0.082)</td>\n",
       "      <td>0.15 (0.025)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_activity_only</th>\n",
       "      <td>0.29 (0.055)</td>\n",
       "      <td>0.14 (0.026)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_all</th>\n",
       "      <td>0.53 (0.07)</td>\n",
       "      <td>0.22 (0.103)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_demo_only</th>\n",
       "      <td>0.3 (0.049)</td>\n",
       "      <td>0.17 (0.074)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_performance_and_demo</th>\n",
       "      <td>0.49 (0.058)</td>\n",
       "      <td>0.26 (0.114)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_performance_only</th>\n",
       "      <td>0.58 (0.069)</td>\n",
       "      <td>0.26 (0.111)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Untuned         Tuned\n",
       "Baseline                  0.14 (0.026)  0.14 (0.026)\n",
       "LR_activity_and_demo      0.44 (0.095)  0.43 (0.084)\n",
       "LR_activity_only           0.4 (0.105)  0.39 (0.099)\n",
       "LR_all                    0.52 (0.111)  0.52 (0.112)\n",
       "LR_demo_only              0.42 (0.106)  0.44 (0.108)\n",
       "LR_performance_and_demo    0.5 (0.163)  0.46 (0.125)\n",
       "LR_performance_only       0.43 (0.125)  0.38 (0.169)\n",
       "XGB_activity_and_demo     0.37 (0.082)  0.15 (0.025)\n",
       "XGB_activity_only         0.29 (0.055)  0.14 (0.026)\n",
       "XGB_all                    0.53 (0.07)  0.22 (0.103)\n",
       "XGB_demo_only              0.3 (0.049)  0.17 (0.074)\n",
       "XGB_performance_and_demo  0.49 (0.058)  0.26 (0.114)\n",
       "XGB_performance_only      0.58 (0.069)  0.26 (0.111)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_df = res_df_tune_comp_mean.round(2).astype(str) + \" (\" +  res_df_tune_comp_std.round(3).astype(str) + \")\"\n",
    "latex_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ba86b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_activity_and_demo</th>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_activity_only</th>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_all</th>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_demo_only</th>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_performance_and_demo</th>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_performance_only</th>\n",
       "      <td>-0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_activity_and_demo</th>\n",
       "      <td>-0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_activity_only</th>\n",
       "      <td>-0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_all</th>\n",
       "      <td>-0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_demo_only</th>\n",
       "      <td>-0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_performance_and_demo</th>\n",
       "      <td>-0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_performance_only</th>\n",
       "      <td>-0.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Tuned\n",
       "Baseline                   0.00\n",
       "LR_activity_and_demo      -0.00\n",
       "LR_activity_only          -0.01\n",
       "LR_all                    -0.00\n",
       "LR_demo_only               0.03\n",
       "LR_performance_and_demo   -0.05\n",
       "LR_performance_only       -0.05\n",
       "XGB_activity_and_demo     -0.23\n",
       "XGB_activity_only         -0.15\n",
       "XGB_all                   -0.32\n",
       "XGB_demo_only             -0.13\n",
       "XGB_performance_and_demo  -0.23\n",
       "XGB_performance_only      -0.31"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df_tune_comp_diff = res_df_tune_comp_mean[[\"Tuned\"]]-res_df_tune_comp_mean[[\"Untuned\"]].values\n",
    "res_df_tune_comp_diff.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b7f5278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_and_demo</th>\n",
       "      <td>-0.228252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_only</th>\n",
       "      <td>-0.146236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>-0.316066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>demo_only</th>\n",
       "      <td>-0.129091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>performance_and_demo</th>\n",
       "      <td>-0.226843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>performance_only</th>\n",
       "      <td>-0.312727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Tuned\n",
       "Baseline              0.000000\n",
       "activity_and_demo    -0.228252\n",
       "activity_only        -0.146236\n",
       "all                  -0.316066\n",
       "demo_only            -0.129091\n",
       "performance_and_demo -0.226843\n",
       "performance_only     -0.312727"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df_tune_comp_diff_lr = res_df_tune_comp_diff.loc[[i for i in res_df_tune_comp_diff.index if (i == \"Baseline\" or \"LR\" in i)]]\n",
    "res_df_tune_comp_diff_xgb = res_df_tune_comp_diff.loc[[i for i in res_df_tune_comp_diff.index if (i == \"Baseline\" or \"XGB\" in i)]]\n",
    "\n",
    "res_df_tune_comp_diff_lr.index = [i[3:] if i!=\"Baseline\" else \"Baseline\" for i in res_df_tune_comp_diff_lr.index]\n",
    "res_df_tune_comp_diff_xgb.index = [i[4:] if i!=\"Baseline\" else \"Baseline\" for i in res_df_tune_comp_diff_xgb.index]\n",
    "res_df_tune_comp_diff_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c17cb615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR</th>\n",
       "      <th>XGB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_and_demo</th>\n",
       "      <td>-0.002600</td>\n",
       "      <td>-0.228252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activity_only</th>\n",
       "      <td>-0.005416</td>\n",
       "      <td>-0.146236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>-0.002858</td>\n",
       "      <td>-0.316066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>demo_only</th>\n",
       "      <td>0.028533</td>\n",
       "      <td>-0.129091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>performance_and_demo</th>\n",
       "      <td>-0.045445</td>\n",
       "      <td>-0.226843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>performance_only</th>\n",
       "      <td>-0.045952</td>\n",
       "      <td>-0.312727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            LR       XGB\n",
       "Baseline              0.000000  0.000000\n",
       "activity_and_demo    -0.002600 -0.228252\n",
       "activity_only        -0.005416 -0.146236\n",
       "all                  -0.002858 -0.316066\n",
       "demo_only             0.028533 -0.129091\n",
       "performance_and_demo -0.045445 -0.226843\n",
       "performance_only     -0.045952 -0.312727"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_df_diff = pd.concat([res_df_tune_comp_diff_lr,res_df_tune_comp_diff_xgb],axis=1)\n",
    "latex_df_diff.columns = [\"LR\", \"XGB\"]\n",
    "latex_df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e778fca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "{} &    LR &   XGB \\\\\n",
      "\\midrule\n",
      "Baseline             &  0.00 &  0.00 \\\\\n",
      "activity\\_and\\_demo    & -0.00 & -0.23 \\\\\n",
      "activity\\_only        & -0.01 & -0.15 \\\\\n",
      "all                  & -0.00 & -0.32 \\\\\n",
      "demo\\_only            &  0.03 & -0.13 \\\\\n",
      "performance\\_and\\_demo & -0.05 & -0.23 \\\\\n",
      "performance\\_only     & -0.05 & -0.31 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_df_diff.round(2).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa5602b",
   "metadata": {},
   "source": [
    "### Performance Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e0c7008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e40a7_row0_col1, #T_e40a7_row0_col2, #T_e40a7_row0_col3, #T_e40a7_row0_col4, #T_e40a7_row0_col5, #T_e40a7_row0_col6 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e40a7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e40a7_level0_col0\" class=\"col_heading level0 col0\" >Baseline</th>\n",
       "      <th id=\"T_e40a7_level0_col1\" class=\"col_heading level0 col1\" >LR_demo_only_tuned</th>\n",
       "      <th id=\"T_e40a7_level0_col2\" class=\"col_heading level0 col2\" >LR_performance_only_tuned</th>\n",
       "      <th id=\"T_e40a7_level0_col3\" class=\"col_heading level0 col3\" >LR_activity_only_tuned</th>\n",
       "      <th id=\"T_e40a7_level0_col4\" class=\"col_heading level0 col4\" >LR_activity_and_demo_tuned</th>\n",
       "      <th id=\"T_e40a7_level0_col5\" class=\"col_heading level0 col5\" >LR_performance_and_demo_tuned</th>\n",
       "      <th id=\"T_e40a7_level0_col6\" class=\"col_heading level0 col6\" >LR_all_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e40a7_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e40a7_row0_col0\" class=\"data row0 col0\" >0.145 (0.026)</td>\n",
       "      <td id=\"T_e40a7_row0_col1\" class=\"data row0 col1\" >0.445 (0.108)</td>\n",
       "      <td id=\"T_e40a7_row0_col2\" class=\"data row0 col2\" >0.379 (0.169)</td>\n",
       "      <td id=\"T_e40a7_row0_col3\" class=\"data row0 col3\" >0.39 (0.099)</td>\n",
       "      <td id=\"T_e40a7_row0_col4\" class=\"data row0 col4\" >0.433 (0.084)</td>\n",
       "      <td id=\"T_e40a7_row0_col5\" class=\"data row0 col5\" >0.455 (0.125)</td>\n",
       "      <td id=\"T_e40a7_row0_col6\" class=\"data row0 col6\" >0.52 (0.112)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x298847f6700>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For LR\n",
    "models = [\"Baseline\"]+[i for i in results_subsets[0].keys() if (\"tuned\" in i and \"LR\" in i)]\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "use_df = pd.DataFrame([pd.DataFrame(results_subsets[fold_num]).loc[metric,models] for fold_num in results_subsets.keys()],index=results_subsets.keys())\n",
    "\n",
    "df_mean = pd.DataFrame((use_df).mean(axis=0).round(3).astype(str) + \" (\" + use_df.std(axis=0).round(3).astype(str) + \")\").transpose()\n",
    "model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "\n",
    "best_model = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "t_test_res = np.array([stats.ttest_rel(use_df[best_model].values, use_df[model].values)[1] for model in models]).round(3)\n",
    "t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    \n",
    "res_df_lr = pd.DataFrame([model_dict])\n",
    "\n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_res[i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "res_df_lr.style.apply(negative_bold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4be5597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f2fea_row0_col0, #T_f2fea_row0_col1, #T_f2fea_row0_col2, #T_f2fea_row0_col3, #T_f2fea_row0_col4, #T_f2fea_row0_col5, #T_f2fea_row0_col6 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f2fea\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f2fea_level0_col0\" class=\"col_heading level0 col0\" >Baseline</th>\n",
       "      <th id=\"T_f2fea_level0_col1\" class=\"col_heading level0 col1\" >XGB_demo_only_tuned</th>\n",
       "      <th id=\"T_f2fea_level0_col2\" class=\"col_heading level0 col2\" >XGB_performance_only_tuned</th>\n",
       "      <th id=\"T_f2fea_level0_col3\" class=\"col_heading level0 col3\" >XGB_activity_only_tuned</th>\n",
       "      <th id=\"T_f2fea_level0_col4\" class=\"col_heading level0 col4\" >XGB_activity_and_demo_tuned</th>\n",
       "      <th id=\"T_f2fea_level0_col5\" class=\"col_heading level0 col5\" >XGB_performance_and_demo_tuned</th>\n",
       "      <th id=\"T_f2fea_level0_col6\" class=\"col_heading level0 col6\" >XGB_all_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f2fea_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f2fea_row0_col0\" class=\"data row0 col0\" >0.145 (0.026)</td>\n",
       "      <td id=\"T_f2fea_row0_col1\" class=\"data row0 col1\" >0.169 (0.074)</td>\n",
       "      <td id=\"T_f2fea_row0_col2\" class=\"data row0 col2\" >0.262 (0.111)</td>\n",
       "      <td id=\"T_f2fea_row0_col3\" class=\"data row0 col3\" >0.145 (0.026)</td>\n",
       "      <td id=\"T_f2fea_row0_col4\" class=\"data row0 col4\" >0.145 (0.025)</td>\n",
       "      <td id=\"T_f2fea_row0_col5\" class=\"data row0 col5\" >0.264 (0.114)</td>\n",
       "      <td id=\"T_f2fea_row0_col6\" class=\"data row0 col6\" >0.216 (0.103)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x299c6ae8550>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For XGB\n",
    "models = [\"Baseline\"]+[i for i in results_subsets[0].keys() if (\"tuned\" in i and \"XGB\" in i)]\n",
    "metric = \"F1 Test\"\n",
    "\n",
    "#####\n",
    "dataset_res_dict = {}\n",
    "best_models = {}\n",
    "t_test_results = {}\n",
    "\n",
    "use_df = pd.DataFrame([pd.DataFrame(results_subsets[fold_num]).loc[metric,models] for fold_num in results_subsets.keys()],index=results_subsets.keys())\n",
    "\n",
    "df_mean = pd.DataFrame((use_df).mean(axis=0).round(3).astype(str) + \" (\" + use_df.std(axis=0).round(3).astype(str) + \")\").transpose()\n",
    "model_dict = {i: df_mean[i].values[0] for i in df_mean.columns}\n",
    "\n",
    "best_model = use_df.columns[use_df.mean(axis=0).argmax()]\n",
    "\n",
    "t_test_res = np.array([stats.ttest_rel(use_df[best_model].values, use_df[model].values)[1] for model in models]).round(3)\n",
    "t_test_res[np.isnan(t_test_res)] = 1.\n",
    "    \n",
    "res_df_xgb = pd.DataFrame([model_dict])\n",
    "\n",
    "def negative_bold(val):\n",
    "    i = np.where(val.name==np.array(models))[0][0]\n",
    "    return [\"font-weight: bold\"  if t_test_res[i]>=0.05 else \"\" for dataset_name in val.keys()]\n",
    "    # Case without transpose:\n",
    "#     return [\"font-weight: bold\"  if t_test_results[val.name][i]>=0.05 else \"\" for i in range(len(val))]\n",
    "\n",
    "res_df_xgb.style.apply(negative_bold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a01b512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Baseline</th>\n",
       "      <th>demo_only</th>\n",
       "      <th>performance_only</th>\n",
       "      <th>activity_only</th>\n",
       "      <th>activity_and_demo</th>\n",
       "      <th>performance_and_demo</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.145 (0.026)</td>\n",
       "      <td>0.445 (0.108)</td>\n",
       "      <td>0.379 (0.169)</td>\n",
       "      <td>0.39 (0.099)</td>\n",
       "      <td>0.433 (0.084)</td>\n",
       "      <td>0.455 (0.125)</td>\n",
       "      <td>0.52 (0.112)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>0.145 (0.026)</td>\n",
       "      <td>0.169 (0.074)</td>\n",
       "      <td>0.262 (0.111)</td>\n",
       "      <td>0.145 (0.026)</td>\n",
       "      <td>0.145 (0.025)</td>\n",
       "      <td>0.264 (0.114)</td>\n",
       "      <td>0.216 (0.103)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Baseline      demo_only performance_only  activity_only  \\\n",
       "LR   0.145 (0.026)  0.445 (0.108)    0.379 (0.169)   0.39 (0.099)   \n",
       "XGB  0.145 (0.026)  0.169 (0.074)    0.262 (0.111)  0.145 (0.026)   \n",
       "\n",
       "    activity_and_demo performance_and_demo            all  \n",
       "LR      0.433 (0.084)        0.455 (0.125)   0.52 (0.112)  \n",
       "XGB     0.145 (0.025)        0.264 (0.114)  0.216 (0.103)  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df_lr.columns = [i[3:-6] if i != \"Baseline\" else \"Baseline\" for i in res_df_lr.columns]    \n",
    "res_df_xgb.columns = [i[4:-6] if i != \"Baseline\" else \"Baseline\" for i in res_df_xgb.columns]    \n",
    "\n",
    "latex_df_subsets = pd.concat([res_df_lr,res_df_xgb],axis=0)\n",
    "latex_df_subsets.index = [\"LR\", \"XGB\"]\n",
    "latex_df_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0778e499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lll}\n",
      "\\toprule\n",
      "{} &             LR &            XGB \\\\\n",
      "\\midrule\n",
      "Baseline             &  0.145 (0.026) &  0.145 (0.026) \\\\\n",
      "demo\\_only            &  0.445 (0.108) &  0.169 (0.074) \\\\\n",
      "performance\\_only     &  0.379 (0.169) &  0.262 (0.111) \\\\\n",
      "activity\\_only        &   0.39 (0.099) &  0.145 (0.026) \\\\\n",
      "activity\\_and\\_demo    &  0.433 (0.084) &  0.145 (0.025) \\\\\n",
      "performance\\_and\\_demo &  0.455 (0.125) &  0.264 (0.114) \\\\\n",
      "all                  &   0.52 (0.112) &  0.216 (0.103) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_df_subsets.round(2).transpose().to_latex())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16aa8e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ge</th>\n",
       "      <th>arr</th>\n",
       "      <th>ls</th>\n",
       "      <th>as</th>\n",
       "      <th>ss</th>\n",
       "      <th>fq_c0</th>\n",
       "      <th>fq_c1</th>\n",
       "      <th>fq_c2</th>\n",
       "      <th>fq_c3</th>\n",
       "      <th>twp_c0</th>\n",
       "      <th>...</th>\n",
       "      <th>tnp_c2</th>\n",
       "      <th>tnp_c3</th>\n",
       "      <th>fmi_c0</th>\n",
       "      <th>fmi_c1</th>\n",
       "      <th>fmi_c2</th>\n",
       "      <th>fmi_c3</th>\n",
       "      <th>atd_c0</th>\n",
       "      <th>atd_c1</th>\n",
       "      <th>atd_c2</th>\n",
       "      <th>atd_c3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.291281</td>\n",
       "      <td>-0.145776</td>\n",
       "      <td>0.121496</td>\n",
       "      <td>0.057080</td>\n",
       "      <td>-0.775396</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003979</td>\n",
       "      <td>0.038619</td>\n",
       "      <td>0.005802</td>\n",
       "      <td>-0.036608</td>\n",
       "      <td>-0.000893</td>\n",
       "      <td>-0.400812</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>-0.119226</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>1.229653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.103676</td>\n",
       "      <td>-0.257282</td>\n",
       "      <td>-0.033518</td>\n",
       "      <td>-0.000324</td>\n",
       "      <td>-0.592144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.065040</td>\n",
       "      <td>0.010272</td>\n",
       "      <td>-0.016779</td>\n",
       "      <td>-0.000914</td>\n",
       "      <td>-0.394815</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>-0.158153</td>\n",
       "      <td>-0.001676</td>\n",
       "      <td>-0.296369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.502198</td>\n",
       "      <td>1.098808</td>\n",
       "      <td>-0.127057</td>\n",
       "      <td>0.005813</td>\n",
       "      <td>0.356272</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007294</td>\n",
       "      <td>0.038619</td>\n",
       "      <td>0.039193</td>\n",
       "      <td>-0.005874</td>\n",
       "      <td>-0.001082</td>\n",
       "      <td>-0.456516</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>-0.114217</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>-0.323181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.101849</td>\n",
       "      <td>-0.325024</td>\n",
       "      <td>0.013729</td>\n",
       "      <td>0.009153</td>\n",
       "      <td>-0.135188</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001678</td>\n",
       "      <td>0.683465</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>-0.001564</td>\n",
       "      <td>-0.327751</td>\n",
       "      <td>0.007415</td>\n",
       "      <td>0.004118</td>\n",
       "      <td>0.080787</td>\n",
       "      <td>0.020920</td>\n",
       "      <td>0.004837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.087583</td>\n",
       "      <td>1.240984</td>\n",
       "      <td>0.013729</td>\n",
       "      <td>-0.005026</td>\n",
       "      <td>0.773648</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001678</td>\n",
       "      <td>0.683465</td>\n",
       "      <td>-0.002158</td>\n",
       "      <td>-0.002342</td>\n",
       "      <td>-0.327751</td>\n",
       "      <td>-0.006180</td>\n",
       "      <td>-0.002679</td>\n",
       "      <td>-0.015779</td>\n",
       "      <td>-0.004249</td>\n",
       "      <td>-0.002889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.368145</td>\n",
       "      <td>1.334509</td>\n",
       "      <td>-0.159979</td>\n",
       "      <td>-0.003733</td>\n",
       "      <td>-0.692770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.042433</td>\n",
       "      <td>-0.015118</td>\n",
       "      <td>-0.019396</td>\n",
       "      <td>-0.000779</td>\n",
       "      <td>1.202608</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>-0.118191</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>-0.244963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.368145</td>\n",
       "      <td>-0.057942</td>\n",
       "      <td>0.210878</td>\n",
       "      <td>0.013544</td>\n",
       "      <td>-0.692770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004281</td>\n",
       "      <td>0.042433</td>\n",
       "      <td>-0.015118</td>\n",
       "      <td>0.083121</td>\n",
       "      <td>-0.000779</td>\n",
       "      <td>-0.386979</td>\n",
       "      <td>-0.001871</td>\n",
       "      <td>0.242136</td>\n",
       "      <td>-0.003034</td>\n",
       "      <td>-0.244963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.368145</td>\n",
       "      <td>-0.057942</td>\n",
       "      <td>0.210878</td>\n",
       "      <td>0.013544</td>\n",
       "      <td>0.229023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.042433</td>\n",
       "      <td>0.007322</td>\n",
       "      <td>-0.036489</td>\n",
       "      <td>-0.000779</td>\n",
       "      <td>1.202608</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.242136</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>1.254629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.368145</td>\n",
       "      <td>-0.057942</td>\n",
       "      <td>-0.159979</td>\n",
       "      <td>-0.001074</td>\n",
       "      <td>-0.692770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002659</td>\n",
       "      <td>0.042433</td>\n",
       "      <td>-0.015118</td>\n",
       "      <td>0.083121</td>\n",
       "      <td>-0.000779</td>\n",
       "      <td>1.202608</td>\n",
       "      <td>-0.001871</td>\n",
       "      <td>-0.130966</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>-0.298006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.204644</td>\n",
       "      <td>-0.057942</td>\n",
       "      <td>0.210878</td>\n",
       "      <td>0.013544</td>\n",
       "      <td>0.229023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002659</td>\n",
       "      <td>-0.037920</td>\n",
       "      <td>0.007322</td>\n",
       "      <td>0.083121</td>\n",
       "      <td>-0.000779</td>\n",
       "      <td>1.202608</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>-0.118191</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>1.254629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ge  arr   ls   as   ss     fq_c0     fq_c1     fq_c2     fq_c3  \\\n",
       "102  1.0  1.0  1.0  0.0  0.0  0.291281 -0.145776  0.121496  0.057080   \n",
       "117  1.0  0.0  1.0  1.0  0.0 -0.103676 -0.257282 -0.033518 -0.000324   \n",
       "103  1.0  1.0  1.0  0.0  0.0  0.502198  1.098808 -0.127057  0.005813   \n",
       "44   1.0  0.0  1.0  1.0  1.0 -0.101849 -0.325024  0.013729  0.009153   \n",
       "114  0.0  1.0  0.0  0.0  0.0 -0.087583  1.240984  0.013729 -0.005026   \n",
       "..   ...  ...  ...  ...  ...       ...       ...       ...       ...   \n",
       "67   1.0  1.0  0.0  1.0  1.0  0.368145  1.334509 -0.159979 -0.003733   \n",
       "111  1.0  1.0  1.0  0.0  0.0  0.368145 -0.057942  0.210878  0.013544   \n",
       "42   1.0  1.0  1.0  0.0  1.0  0.368145 -0.057942  0.210878  0.013544   \n",
       "97   0.0  1.0  1.0  1.0  0.0  0.368145 -0.057942 -0.159979 -0.001074   \n",
       "125  1.0  0.0  0.0  0.0  0.0 -0.204644 -0.057942  0.210878  0.013544   \n",
       "\n",
       "       twp_c0  ...    tnp_c2    tnp_c3    fmi_c0    fmi_c1    fmi_c2  \\\n",
       "102 -0.775396  ... -0.003979  0.038619  0.005802 -0.036608 -0.000893   \n",
       "117 -0.592144  ...  0.000353  0.065040  0.010272 -0.016779 -0.000914   \n",
       "103  0.356272  ... -0.007294  0.038619  0.039193 -0.005874 -0.001082   \n",
       "44  -0.135188  ... -0.001678  0.683465  0.003701 -0.001564 -0.327751   \n",
       "114  0.773648  ... -0.001678  0.683465 -0.002158 -0.002342 -0.327751   \n",
       "..        ...  ...       ...       ...       ...       ...       ...   \n",
       "67  -0.692770  ...  0.000166  0.042433 -0.015118 -0.019396 -0.000779   \n",
       "111 -0.692770  ... -0.004281  0.042433 -0.015118  0.083121 -0.000779   \n",
       "42   0.229023  ...  0.000166  0.042433  0.007322 -0.036489 -0.000779   \n",
       "97  -0.692770  ...  0.002659  0.042433 -0.015118  0.083121 -0.000779   \n",
       "125  0.229023  ...  0.002659 -0.037920  0.007322  0.083121 -0.000779   \n",
       "\n",
       "       fmi_c3    atd_c0    atd_c1    atd_c2    atd_c3  \n",
       "102 -0.400812  0.001315 -0.119226  0.002267  1.229653  \n",
       "117 -0.394815  0.000751 -0.158153 -0.001676 -0.296369  \n",
       "103 -0.456516  0.000835 -0.114217  0.002101 -0.323181  \n",
       "44   0.007415  0.004118  0.080787  0.020920  0.004837  \n",
       "114 -0.006180 -0.002679 -0.015779 -0.004249 -0.002889  \n",
       "..        ...       ...       ...       ...       ...  \n",
       "67   1.202608  0.002223 -0.118191  0.002729 -0.244963  \n",
       "111 -0.386979 -0.001871  0.242136 -0.003034 -0.244963  \n",
       "42   1.202608  0.000819  0.242136  0.002729  1.254629  \n",
       "97   1.202608 -0.001871 -0.130966  0.002729 -0.298006  \n",
       "125  1.202608  0.002223 -0.118191  0.002729  1.254629  \n",
       "\n",
       "[105 rows x 65 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_val_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f69c9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18dc7f47",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "daf01f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_10_importances = {}\n",
    "\n",
    "# for model in list(results_subsets_feature_importances[fold].keys()):\n",
    "#     imp_df = pd.concat([results_subsets_feature_importances[fold][model] for fold in range(folds)],axis=1)\n",
    "\n",
    "#     if \"LR\" in model:\n",
    "#         direction = imp_df.apply(lambda x: np.sign(x))\n",
    "#         imp_df = imp_df.abs()\n",
    "\n",
    "#     imp_df = imp_df/imp_df.sum(axis=0)\n",
    "\n",
    "#     mean_imp_df = imp_df.mean(axis=1)\n",
    "#     std_imp_df = imp_df.std(axis=1)\n",
    "\n",
    "#     mean_imp_df = mean_imp_df.sort_values(ascending=False)\n",
    "#     std_imp_df = std_imp_df.loc[mean_imp_df.index]\n",
    "#     final_imps = mean_imp_df[:10]\n",
    "#     final_imps[\"Rest\"] = sum(mean_imp_df[10:])\n",
    "#     top_5_importances[model] = np.array([final_imps.index.values, final_imps.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9aa66e3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demo_importances = {}\n",
    "demo_importances_stds = {}\n",
    "\n",
    "for model in list(results_subsets_feature_importances[fold].keys()):\n",
    "    if \"demo\" in model or \"all\" in model:\n",
    "        imp_df_all = pd.concat([results_subsets_feature_importances[fold][model] for fold in range(folds)],axis=1)\n",
    "        \n",
    "        if \"LR\" in model:\n",
    "            direction = imp_df_all.apply(lambda x: np.sign(x))\n",
    "            imp_df_all = imp_df_all.abs()\n",
    "        if imp_df_all.sum().sum()!=0:\n",
    "            imp_df = imp_df_all/imp_df_all.sum(axis=0)\n",
    "        imp_df = imp_df.fillna(1/imp_df.shape[0])\n",
    "#         imp_df = imp_df.loc[demographic_cols]\n",
    "\n",
    "#         mean_imp_df = imp_df.mean(axis=1)\n",
    "#         std_imp_df = imp_df.std(axis=1)\n",
    "\n",
    "#         mean_imp_df = mean_imp_df.sort_values(ascending=False)\n",
    "#         std_imp_df = std_imp_df.loc[mean_imp_df.index]\n",
    "#         final_imps = mean_imp_df#[:10]\n",
    "#         final_imps[\"Rest\"] = sum(mean_imp_df[10:])\n",
    "#         final_imps[\"Total\"] = sum(mean_imp_df)\n",
    "        demo_importances[model] = np.round(np.mean(imp_df.loc[[i for i in imp_df.index if any([j in i for j in demographic_cols])]].sum(axis=0)),2)#final_imps.values\n",
    "        demo_importances_stds[model] = np.round(np.std(imp_df.loc[[i for i in imp_df.index if any([j in i for j in demographic_cols])]].sum(axis=0)),2)#final_imps.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f6c4448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demo_only</th>\n",
       "      <th>activity_and_demo</th>\n",
       "      <th>performance_and_demo</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>1.0 (0.0)</td>\n",
       "      <td>0.81 (0.04)</td>\n",
       "      <td>0.78 (0.04)</td>\n",
       "      <td>0.63 (0.03)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>1.0 (0.0)</td>\n",
       "      <td>0.71 (0.12)</td>\n",
       "      <td>0.23 (0.32)</td>\n",
       "      <td>0.26 (0.32)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     demo_only activity_and_demo performance_and_demo          all\n",
       "LR   1.0 (0.0)       0.81 (0.04)          0.78 (0.04)  0.63 (0.03)\n",
       "XGB  1.0 (0.0)       0.71 (0.12)          0.23 (0.32)  0.26 (0.32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_demo_imp = pd.Series({i: demo_importances[i] for i in demo_importances if \"LR\" in i and \"tuned\" in i})\n",
    "xgb_demo_imp = pd.Series({i: demo_importances[i] for i in demo_importances if \"XGB\" in i and \"tuned\" in i})\n",
    "lr_demo_imp.index = [i[3:-6] for i in lr_demo_imp.index]    \n",
    "xgb_demo_imp.index = [i[4:-6] for i in xgb_demo_imp.index]    \n",
    "\n",
    "lr_demo_imp_stds = pd.Series({i: demo_importances_stds[i] for i in demo_importances_stds if \"LR\" in i and \"tuned\" in i})\n",
    "xgb_demo_imp_stds = pd.Series({i: demo_importances_stds[i] for i in demo_importances_stds if \"XGB\" in i and \"tuned\" in i})\n",
    "lr_demo_imp_stds.index = [i[3:-6] for i in lr_demo_imp_stds.index]    \n",
    "xgb_demo_imp_stds.index = [i[4:-6] for i in xgb_demo_imp_stds.index]    \n",
    "\n",
    "\n",
    "latex_df_imp = pd.DataFrame([lr_demo_imp.astype(str) + \" (\" + lr_demo_imp_stds.astype(str) + \")\",\n",
    "                             xgb_demo_imp.astype(str) + \" (\" + xgb_demo_imp_stds.astype(str) + \")\"])\n",
    "latex_df_imp.index = [\"LR\", \"XGB\"]\n",
    "latex_df_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5e9930b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "{} &       Baseline &      demo\\_only & performance\\_only &  activity\\_only & activity\\_and\\_demo & performance\\_and\\_demo &            all \\\\\n",
      "\\midrule\n",
      "LR  &  0.145 (0.026) &  0.445 (0.108) &    0.379 (0.169) &   0.39 (0.099) &     0.433 (0.084) &        0.455 (0.125) &   0.52 (0.112) \\\\\n",
      "XGB &  0.145 (0.026) &  0.169 (0.074) &    0.262 (0.111) &  0.145 (0.026) &     0.145 (0.025) &        0.264 (0.114) &  0.216 (0.103) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_df_subsets.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d0fa1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07bf14f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc07112e7ae1e8e28a0232207620ff002934c05692de8df42430404c766a0a8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
